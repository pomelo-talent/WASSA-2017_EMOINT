{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Reading the Training, the Development and the Testing Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger-ratings-0to1.train.txt',\n",
       " 'fear-ratings-0to1.train.txt',\n",
       " 'joy-ratings-0to1.train.txt',\n",
       " 'sadness-ratings-0to1.train.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory1 = 'data/train'\n",
    "paths1 = listdir(directory1)\n",
    "paths1.sort()\n",
    "paths1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the path of training dataset\n",
    "path1 = paths1[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sadness'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you want to restore x_train vectors, y_train_vectors, x_test_vectors, y_test_vectors: you can change the 'emotion' here \n",
    "# and then restore those vectors\n",
    "\n",
    "emotion = path1.split(\"-\")[0]\n",
    "emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40000</td>\n",
       "      <td>Depression sucks! #depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40001</td>\n",
       "      <td>Feeling worthless as always #depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40002</td>\n",
       "      <td>Feeling worthless as always</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40003</td>\n",
       "      <td>My #Fibromyalgia has been really bad lately wh...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40004</td>\n",
       "      <td>Im think ima lay in bed all day and sulk. Life...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                                                  1        2      3\n",
       "0  40000                      Depression sucks! #depression  sadness  0.958\n",
       "1  40001            Feeling worthless as always #depression  sadness  0.958\n",
       "2  40002                       Feeling worthless as always   sadness  0.958\n",
       "3  40003  My #Fibromyalgia has been really bad lately wh...  sadness  0.946\n",
       "4  40004  Im think ima lay in bed all day and sulk. Life...  sadness  0.934"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('%s/%s' %(directory1,path1), delimiter='\\t',header=None)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40000</td>\n",
       "      <td>Depression sucks! #depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40001</td>\n",
       "      <td>Feeling worthless as always #depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40002</td>\n",
       "      <td>Feeling worthless as always</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40003</td>\n",
       "      <td>My #Fibromyalgia has been really bad lately wh...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40004</td>\n",
       "      <td>Im think ima lay in bed all day and sulk. Life...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40005</td>\n",
       "      <td>So when I try I fail... and when I don't try.....</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40006</td>\n",
       "      <td>Extreme sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40007</td>\n",
       "      <td>my life in one word is depressing</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40008</td>\n",
       "      <td>Panic attacks are the worst. Feeling really si...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40009</td>\n",
       "      <td>Feel so grim + ugly atm</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SentID                                              Tweet  Emotion  Rating\n",
       "0   40000                      Depression sucks! #depression  sadness   0.958\n",
       "1   40001            Feeling worthless as always #depression  sadness   0.958\n",
       "2   40002                       Feeling worthless as always   sadness   0.958\n",
       "3   40003  My #Fibromyalgia has been really bad lately wh...  sadness   0.946\n",
       "4   40004  Im think ima lay in bed all day and sulk. Life...  sadness   0.934\n",
       "5   40005  So when I try I fail... and when I don't try.....  sadness   0.917\n",
       "6   40006                                    Extreme sadness  sadness   0.917\n",
       "7   40007                  my life in one word is depressing  sadness   0.917\n",
       "8   40008  Panic attacks are the worst. Feeling really si...  sadness   0.917\n",
       "9   40009                            Feel so grim + ugly atm  sadness   0.896"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns = ['SentID', 'Tweet', 'Emotion', 'Rating']\n",
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any duplicates(subset = all of the columns)\n",
    "\n",
    "assert train.duplicated().sum() == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 786 entries, 0 to 785\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   SentID   786 non-null    int64  \n",
      " 1   Tweet    786 non-null    object \n",
      " 2   Emotion  786 non-null    object \n",
      " 3   Rating   786 non-null    float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 24.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# checking missing data & len(train),len(test)\n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             SentID      Rating\n",
      "count    786.000000  786.000000\n",
      "mean   40392.500000    0.495957\n",
      "std      227.042947    0.190841\n",
      "min    40000.000000    0.083000\n",
      "25%    40196.250000    0.351750\n",
      "50%    40392.500000    0.479000\n",
      "75%    40588.750000    0.646000\n",
      "max    40785.000000    0.958000\n"
     ]
    }
   ],
   "source": [
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger-ratings-0to1.dev.gold.txt',\n",
       " 'fear-ratings-0to1.dev.gold.txt',\n",
       " 'joy-ratings-0to1.dev.gold.txt',\n",
       " 'sadness-ratings-0to1.dev.gold.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory2 = 'data/dev'\n",
    "paths2 = listdir(directory2)\n",
    "paths2.sort()\n",
    "paths2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the path of training dataset\n",
    "path2 = emotion + \"-ratings-0to1.dev.gold.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40786</td>\n",
       "      <td>@1johndes ball watching &amp;amp; Rojo'd header wa...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40787</td>\n",
       "      <td>A pessimist is someone who, when opportunity k...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40788</td>\n",
       "      <td>A .500 season is all I'm looking for at this p...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40789</td>\n",
       "      <td>Stars, when you shine,\\nYou know how I feel.\\n...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40790</td>\n",
       "      <td>All I want to do is watch some netflix but I a...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40791</td>\n",
       "      <td>Buddha doesn't possess enough power to deliver...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40792</td>\n",
       "      <td>Donating to Trump puts a damper on a very exci...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40793</td>\n",
       "      <td>Hello my dear friends, I will be back online t...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40794</td>\n",
       "      <td>@ccrago It was dreadful, even after he met the...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40795</td>\n",
       "      <td>watching this uni reveal is so depressing i mi...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SentID                                              Tweet  Emotion  Rating\n",
       "0   40786  @1johndes ball watching &amp; Rojo'd header wa...  sadness   0.583\n",
       "1   40787  A pessimist is someone who, when opportunity k...  sadness   0.188\n",
       "2   40788  A .500 season is all I'm looking for at this p...  sadness   0.688\n",
       "3   40789  Stars, when you shine,\\nYou know how I feel.\\n...  sadness   0.292\n",
       "4   40790  All I want to do is watch some netflix but I a...  sadness   0.667\n",
       "5   40791  Buddha doesn't possess enough power to deliver...  sadness   0.542\n",
       "6   40792  Donating to Trump puts a damper on a very exci...  sadness   0.438\n",
       "7   40793  Hello my dear friends, I will be back online t...  sadness   0.417\n",
       "8   40794  @ccrago It was dreadful, even after he met the...  sadness   0.542\n",
       "9   40795  watching this uni reveal is so depressing i mi...  sadness   0.667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev = pd.read_csv('%s/%s' %(directory2,path2), delimiter='\\t',header=None)\n",
    "dev.columns = train.columns\n",
    "dev[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any duplicates(subset = all of the columns)\n",
    "\n",
    "assert dev.duplicated().sum() == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74 entries, 0 to 73\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   SentID   74 non-null     int64  \n",
      " 1   Tweet    74 non-null     object \n",
      " 2   Emotion  74 non-null     object \n",
      " 3   Rating   74 non-null     float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 2.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# checking missing data & len(train),len(test)\n",
    "print(dev.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             SentID     Rating\n",
      "count     74.000000  74.000000\n",
      "mean   40822.500000   0.475743\n",
      "std       21.505813   0.178436\n",
      "min    40786.000000   0.125000\n",
      "25%    40804.250000   0.340750\n",
      "50%    40822.500000   0.458000\n",
      "75%    40840.750000   0.625000\n",
      "max    40859.000000   0.875000\n"
     ]
    }
   ],
   "source": [
    "print(dev.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger-ratings-0to1.test.gold.txt',\n",
       " 'fear-ratings-0to1.test.gold.txt',\n",
       " 'joy-ratings-0to1.test.gold.txt',\n",
       " 'sadness-ratings-0to1.test.gold.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory3 = 'data/test'\n",
    "paths3 = listdir(directory3)\n",
    "paths3.sort()\n",
    "paths3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the path of training dataset\n",
    "path3 = emotion + \"-ratings-0to1.test.gold.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40860</td>\n",
       "      <td>My 2 teens sons just left in the car to get ha...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40861</td>\n",
       "      <td>My 2 teens sons just left in the car to get ha...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40862</td>\n",
       "      <td>HartRamsey'sUPLIFT If you're still discouraged...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40863</td>\n",
       "      <td>@AmontanaW I nearly dropped my phone into the ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40864</td>\n",
       "      <td>Whenever I'm feeling sad I will listen to mons...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40865</td>\n",
       "      <td>@spamvicious I've just found out it's Candice ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40866</td>\n",
       "      <td>@British_Airways In your Concorde Lounge in Te...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40867</td>\n",
       "      <td>#Facebook is #depressing without even being th...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40868</td>\n",
       "      <td>BTW, offended policy wonks, H only had one rea...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40869</td>\n",
       "      <td>I believe the work I do is meaningful; my clie...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SentID                                              Tweet  Emotion  Rating\n",
       "0   40860  My 2 teens sons just left in the car to get ha...  sadness   0.667\n",
       "1   40861  My 2 teens sons just left in the car to get ha...  sadness   0.458\n",
       "2   40862  HartRamsey'sUPLIFT If you're still discouraged...  sadness   0.396\n",
       "3   40863  @AmontanaW I nearly dropped my phone into the ...  sadness   0.271\n",
       "4   40864  Whenever I'm feeling sad I will listen to mons...  sadness   0.604\n",
       "5   40865  @spamvicious I've just found out it's Candice ...  sadness   0.271\n",
       "6   40866  @British_Airways In your Concorde Lounge in Te...  sadness   0.542\n",
       "7   40867  #Facebook is #depressing without even being th...  sadness   0.708\n",
       "8   40868  BTW, offended policy wonks, H only had one rea...  sadness   0.542\n",
       "9   40869  I believe the work I do is meaningful; my clie...  sadness   0.625"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('%s/%s' %(directory3,path3), delimiter='\\t',header=None)\n",
    "test.columns = train.columns\n",
    "\n",
    "test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there are any duplicates(subset = all of the columns)\n",
    "\n",
    "assert test.duplicated().sum() == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 673 entries, 0 to 672\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   SentID   673 non-null    int64  \n",
      " 1   Tweet    673 non-null    object \n",
      " 2   Emotion  673 non-null    object \n",
      " 3   Rating   673 non-null    float64\n",
      "dtypes: float64(1), int64(1), object(2)\n",
      "memory usage: 21.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# checking missing data & len(train),len(test)\n",
    "print(test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            SentID      Rating\n",
      "count    673.00000  673.000000\n",
      "mean   41196.00000    0.511272\n",
      "std      194.42265    0.202737\n",
      "min    40860.00000    0.083000\n",
      "25%    41028.00000    0.354000\n",
      "50%    41196.00000    0.500000\n",
      "75%    41364.00000    0.667000\n",
      "max    41532.00000    1.000000\n"
     ]
    }
   ],
   "source": [
    "print(test.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40000</td>\n",
       "      <td>Depression sucks! #depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40001</td>\n",
       "      <td>Feeling worthless as always #depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40002</td>\n",
       "      <td>Feeling worthless as always</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40003</td>\n",
       "      <td>My #Fibromyalgia has been really bad lately wh...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40004</td>\n",
       "      <td>Im think ima lay in bed all day and sulk. Life...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>40855</td>\n",
       "      <td>Common app just randomly logged me out as I wa...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>40856</td>\n",
       "      <td>I'd rather laugh with the rarest genius, in be...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>40857</td>\n",
       "      <td>If you #invest in my new #film I will stop ask...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>858</th>\n",
       "      <td>40858</td>\n",
       "      <td>Just watched Django Unchained, Other people ma...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>40859</td>\n",
       "      <td>@KeithOlbermann depressing how despicable Trum...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>860 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     SentID                                              Tweet  Emotion  \\\n",
       "0     40000                      Depression sucks! #depression  sadness   \n",
       "1     40001            Feeling worthless as always #depression  sadness   \n",
       "2     40002                       Feeling worthless as always   sadness   \n",
       "3     40003  My #Fibromyalgia has been really bad lately wh...  sadness   \n",
       "4     40004  Im think ima lay in bed all day and sulk. Life...  sadness   \n",
       "..      ...                                                ...      ...   \n",
       "855   40855  Common app just randomly logged me out as I wa...  sadness   \n",
       "856   40856  I'd rather laugh with the rarest genius, in be...  sadness   \n",
       "857   40857  If you #invest in my new #film I will stop ask...  sadness   \n",
       "858   40858  Just watched Django Unchained, Other people ma...  sadness   \n",
       "859   40859  @KeithOlbermann depressing how despicable Trum...  sadness   \n",
       "\n",
       "     Rating  \n",
       "0     0.958  \n",
       "1     0.958  \n",
       "2     0.958  \n",
       "3     0.946  \n",
       "4     0.934  \n",
       "..      ...  \n",
       "855   0.833  \n",
       "856   0.688  \n",
       "857   0.458  \n",
       "858   0.333  \n",
       "859   0.708  \n",
       "\n",
       "[860 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plan to train models on the combined training and development sets\n",
    "train = pd.concat([train, dev],axis=0,ignore_index=True)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Define Text Data Cleaning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "\n",
    "import wordsegment as ws # $ pip install wordsegment    \n",
    "ws.load()     \n",
    "\n",
    "import emoji  # $ pip install emoji\n",
    "\n",
    "# As the glove model contains many words made with grammatical role, tense ,or derivational morphology,\n",
    "# we do not need WordNetLemmatizer or SnowballStemmer here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \n",
    "    # replace emoji to word\n",
    "    # text = emoji.demojize(text)\n",
    "    \n",
    "    # remove characters outside the ascii code 128\n",
    "    # text = ''.join([w if ord(w)<128 else ' ' for w in text])\n",
    "    \n",
    "    # replace '--' with a space\n",
    "    text = text.replace('--',' ')\n",
    "    \n",
    "    # remove any newline characters\n",
    "    text = text.replace('\\n',' ')\n",
    "    \n",
    "    # tweets mentions user using '@' followed by username. Replace all those with <user> to be usable for Glove\n",
    "    text = re.sub('@[^ ]+','<user>',text)\n",
    "    \n",
    "    # Replace all URLs with <url> to be usable for Glove\n",
    "    text = re.sub(r'\\d+','<number>',text)\n",
    "   \n",
    "    # Replace all numbers with <number> to be usable for Glove\n",
    "    text = re.sub(r'http\\S+','<url>',text)\n",
    "    \n",
    "    # turn some abbreviations into a whold word\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"fu\\*k\", \" fuck\", text)\n",
    "    text = re.sub(r\"f\\*c+\", \"fuck\", text)\n",
    "    text = text.replace(\"wtf\", \"what the fuck\")\n",
    "    \n",
    "    # prepare spaces between punctuation and words\n",
    "    text1 = text.split('...')\n",
    "    for i in range(len(text1)):\n",
    "        text1[i] = text1[i].replace('/',' / ').replace('\\\\',' \\ ').replace(',',' , ').replace('.',' . ').replace('?',' ? ').replace('!',' ! ').replace(\"'\",\" ' \").replace(':',' : ').replace(';',' ; ').replace('-',' - ').replace('(',' ( ').replace(')',' ) ').replace('[',' [ ').replace(']',' ] ').replace('&',' & ').replace('*',' * ').replace('{',' { ').replace('}',' } ').replace('-',' - ').replace('`',' ` ').replace('\"',' \" ').replace('>','> ').replace('<',' <')\n",
    "    text1 = ' '.join(text1)\n",
    "    \n",
    "    # split into tokens by white space\n",
    "    tokens = text1.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def clean_text(text):\n",
    "   \n",
    "    tokens = normalize_text(text)\n",
    "    \n",
    "    new_tokens1 = []\n",
    "    for w in tokens:\n",
    "        if w == \"<user>\":\n",
    "            w = w\n",
    "        elif w == \"<url>\":\n",
    "            w = w\n",
    "        elif w == \"<number>\":\n",
    "            w = w\n",
    "        elif w in emoji.UNICODE_EMOJI:\n",
    "            w = w\n",
    "        else:\n",
    "            # prepare regex for char filtering: !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "            re_punc = re.compile('[%s]' %re.escape(string.punctuation))\n",
    "            # remove punctuation from each word\n",
    "            w = re_punc.sub('', w)\n",
    "    \n",
    "            # remove remaining tokens that are not alphabetic\n",
    "            if w.isalpha():\n",
    "                w = w\n",
    "        new_tokens1.append(w) \n",
    "        \n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in new_tokens1 if not w in stop_words]\n",
    "    \n",
    "    new_tokens2 = []\n",
    "    for w in tokens:\n",
    "        if w == \"<user>\":\n",
    "            w = w\n",
    "        elif w == \"<url>\":\n",
    "            w = w\n",
    "        elif w == \"<number>\":\n",
    "            w = w\n",
    "        elif w in emoji.UNICODE_EMOJI:\n",
    "            w = w\n",
    "        else:\n",
    "            # word segment will convert the hashtag based joined words, for example, it will segment #iamthebest to ['i','am','the','best']\n",
    "            w = ' '.join(ws.segment(w)) \n",
    "        new_tokens2.append(w)\n",
    "        \n",
    "    \n",
    "    # make lower case\n",
    "    tokens = [word.lower() for word in new_tokens2]\n",
    "    \n",
    "    \n",
    "    clean_text = ' '.join(tokens)\n",
    "    tokens = clean_text.split()\n",
    "    \n",
    "    new_tokens3 = []   \n",
    "    # filter out short tokens\n",
    "    for w in tokens:\n",
    "        if w in emoji.UNICODE_EMOJI:\n",
    "            w = w\n",
    "        else:\n",
    "            if len(w) > 1:\n",
    "                w =w\n",
    "        new_tokens3.append(w)\n",
    "    \n",
    "    return ' '.join(new_tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<user> i left dad deal üòÇ my work done soon felt wrath slipper üò∑'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do a sanity check\n",
    "text = \"@laura221b I've left it for my dad to deal with üòÇ My work is done as soon as it's felt the wrath of my slipper üò∑\"\n",
    "clean_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for clean Hashtag Emotion Intensity Lexicons...\n",
    "\n",
    "def remove_stopwords(string):\n",
    "    split_string = \\\n",
    "        [word for word in string.split()\n",
    "         if word not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join(split_string)\n",
    "\n",
    "def clean_str(string):  \n",
    "    string = string.replace(\"\\\\n\", \" \")\n",
    "    string = string.replace(\"_NEG\", \"\")\n",
    "    string = string.replace(\"_NEGFIRST\", \"\")\n",
    "    string = re.sub(r\"@[A-Za-z0-9_s(),!?\\'\\`]+\", \"\", string) # removing any twitter handle mentions\n",
    "\n",
    "    string = re.sub(r\"#\", \"\", string)\n",
    "    string = re.sub(r\"\\*\", \"\", string)\n",
    "    string = re.sub(r\"\\'s\", \"\", string)\n",
    "    string = re.sub(r\"\\'m\", \" am\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" have\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" not\", string)\n",
    "    string = re.sub(r\"\\'re\", \" are\", string)\n",
    "    string = re.sub(r\"\\'d\", \" would\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" will\", string)\n",
    "    string = re.sub(r\",\", \"\", string)\n",
    "    string = re.sub(r\"!\", \" !\", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \" ?\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "\n",
    "    return remove_stopwords(string.strip().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3) Clean Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Tweet'] = train['Tweet'].apply(clean_text)\n",
    "\n",
    "test['Tweet'] = test['Tweet'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40000</td>\n",
       "      <td>depression sucks depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40001</td>\n",
       "      <td>feeling worthless always depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40002</td>\n",
       "      <td>feeling worthless always</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40003</td>\n",
       "      <td>my fibromyalgia really bad lately good mental ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40004</td>\n",
       "      <td>im think ima lay bed day sulk life hitting har...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40005</td>\n",
       "      <td>so i try i fail i try i still fail confused lost</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40006</td>\n",
       "      <td>extreme sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40007</td>\n",
       "      <td>life one word depressing</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40008</td>\n",
       "      <td>panic attacks worst feeling really sick still ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40009</td>\n",
       "      <td>feel grim ugly atm</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>40010</td>\n",
       "      <td>honestly depression kicking ass lately üòî</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>40011</td>\n",
       "      <td>at age i see gray is gray bad eyes perspective...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>40012</td>\n",
       "      <td>after &lt;number&gt; idk i start feeling depress sad...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>40013</td>\n",
       "      <td>i sulk much good</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>40014</td>\n",
       "      <td>a night depression winning depression fml help</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>40015</td>\n",
       "      <td>going home depressing</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>40016</td>\n",
       "      <td>we even grieve one black body another one pops...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>40017</td>\n",
       "      <td>it gloomy ass day</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>40018</td>\n",
       "      <td>&lt;user&gt; wow i really sadden terrible</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40019</td>\n",
       "      <td>my heads still ibiza body sat desk work depres...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>40020</td>\n",
       "      <td>&lt;user&gt; leave sad</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>40021</td>\n",
       "      <td>depression sucks</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>40022</td>\n",
       "      <td>it week awful connectivity &lt;user&gt; service &lt;num...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>40023</td>\n",
       "      <td>this depressing shit ever</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>40024</td>\n",
       "      <td>sometimes the worst place you can be is in you...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>40025</td>\n",
       "      <td>&lt;user&gt; service cleveland oh what going unhappy</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>40026</td>\n",
       "      <td>what sad evening clearing harvey cage belongin...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>40027</td>\n",
       "      <td>when &lt;number&gt; doe run opposite side üôÅ depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40028</td>\n",
       "      <td>the moment day start plaster smile face depres...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>40029</td>\n",
       "      <td>i hate gloomy outside always gets depressing mood</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>40030</td>\n",
       "      <td>people stealing things work quite damper day i...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>40031</td>\n",
       "      <td>&lt;user&gt; ring ring depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>40032</td>\n",
       "      <td>it september still battling situation said han...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>40033</td>\n",
       "      <td>&lt;user&gt; yeah i sure depressing talk parents pho...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>40034</td>\n",
       "      <td>at home sick the blues cure i need ideas sore ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>40035</td>\n",
       "      <td>i would wish anxiety depression even worst peo...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>40036</td>\n",
       "      <td>sorry main twitter im depress</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>40037</td>\n",
       "      <td>&lt;user&gt; i surrounded trump voters you right fuc...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40038</td>\n",
       "      <td>come funeral tomorrow &lt;number&gt; mourn death gpa</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40039</td>\n",
       "      <td>&lt;user&gt; jesus made think vols v fl games i seen...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40040</td>\n",
       "      <td>after spending &lt;number&gt; &lt;user&gt; i offered &lt;numb...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>40041</td>\n",
       "      <td>thought i pretty solid gpa kin major i look av...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>40042</td>\n",
       "      <td>depress üò≠</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>40043</td>\n",
       "      <td>lament n saddened heart n so far amp n yet nea...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>40044</td>\n",
       "      <td>summer officially ends today sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>40045</td>\n",
       "      <td>well evaluation came back minimally effective ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>40046</td>\n",
       "      <td>bad news fam life still hard awful depression ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>40047</td>\n",
       "      <td>oh i get depression happy do think i tried tho...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>40048</td>\n",
       "      <td>im really constipated this depressing</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>40049</td>\n",
       "      <td>when health insurance cover tms let know cover...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>40050</td>\n",
       "      <td>somewhere hope despair day meeting my abuser i...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>40051</td>\n",
       "      <td>&lt;user&gt; &lt;user&gt; i despondent</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>40052</td>\n",
       "      <td>&lt;number&gt; applications dbs still waiting been y...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>40053</td>\n",
       "      <td>i miss social media place get laughs jump dms ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>40054</td>\n",
       "      <td>&lt;user&gt; the three r depress</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SentID                                              Tweet  Emotion  Rating\n",
       "0    40000                        depression sucks depression  sadness   0.958\n",
       "1    40001                feeling worthless always depression  sadness   0.958\n",
       "2    40002                           feeling worthless always  sadness   0.958\n",
       "3    40003  my fibromyalgia really bad lately good mental ...  sadness   0.946\n",
       "4    40004  im think ima lay bed day sulk life hitting har...  sadness   0.934\n",
       "5    40005   so i try i fail i try i still fail confused lost  sadness   0.917\n",
       "6    40006                                    extreme sadness  sadness   0.917\n",
       "7    40007                           life one word depressing  sadness   0.917\n",
       "8    40008  panic attacks worst feeling really sick still ...  sadness   0.917\n",
       "9    40009                                 feel grim ugly atm  sadness   0.896\n",
       "10   40010           honestly depression kicking ass lately üòî  sadness   0.896\n",
       "11   40011  at age i see gray is gray bad eyes perspective...  sadness   0.896\n",
       "12   40012  after <number> idk i start feeling depress sad...  sadness   0.896\n",
       "13   40013                                   i sulk much good  sadness   0.896\n",
       "14   40014     a night depression winning depression fml help  sadness   0.896\n",
       "15   40015                              going home depressing  sadness   0.896\n",
       "16   40016  we even grieve one black body another one pops...  sadness   0.892\n",
       "17   40017                                  it gloomy ass day  sadness   0.888\n",
       "18   40018                <user> wow i really sadden terrible  sadness   0.875\n",
       "19   40019  my heads still ibiza body sat desk work depres...  sadness   0.875\n",
       "20   40020                                   <user> leave sad  sadness   0.875\n",
       "21   40021                                   depression sucks  sadness   0.875\n",
       "22   40022  it week awful connectivity <user> service <num...  sadness   0.870\n",
       "23   40023                          this depressing shit ever  sadness   0.861\n",
       "24   40024  sometimes the worst place you can be is in you...  sadness   0.854\n",
       "25   40025     <user> service cleveland oh what going unhappy  sadness   0.854\n",
       "26   40026  what sad evening clearing harvey cage belongin...  sadness   0.854\n",
       "27   40027   when <number> doe run opposite side üôÅ depression  sadness   0.854\n",
       "28   40028  the moment day start plaster smile face depres...  sadness   0.854\n",
       "29   40029  i hate gloomy outside always gets depressing mood  sadness   0.839\n",
       "30   40030  people stealing things work quite damper day i...  sadness   0.833\n",
       "31   40031                        <user> ring ring depression  sadness   0.833\n",
       "32   40032  it september still battling situation said han...  sadness   0.833\n",
       "33   40033  <user> yeah i sure depressing talk parents pho...  sadness   0.833\n",
       "34   40034  at home sick the blues cure i need ideas sore ...  sadness   0.833\n",
       "35   40035  i would wish anxiety depression even worst peo...  sadness   0.833\n",
       "36   40036                      sorry main twitter im depress  sadness   0.833\n",
       "37   40037  <user> i surrounded trump voters you right fuc...  sadness   0.833\n",
       "38   40038     come funeral tomorrow <number> mourn death gpa  sadness   0.833\n",
       "39   40039  <user> jesus made think vols v fl games i seen...  sadness   0.833\n",
       "40   40040  after spending <number> <user> i offered <numb...  sadness   0.814\n",
       "41   40041  thought i pretty solid gpa kin major i look av...  sadness   0.812\n",
       "42   40042                                          depress üò≠  sadness   0.812\n",
       "43   40043  lament n saddened heart n so far amp n yet nea...  sadness   0.812\n",
       "44   40044               summer officially ends today sadness  sadness   0.812\n",
       "45   40045  well evaluation came back minimally effective ...  sadness   0.812\n",
       "46   40046  bad news fam life still hard awful depression ...  sadness   0.812\n",
       "47   40047  oh i get depression happy do think i tried tho...  sadness   0.812\n",
       "48   40048              im really constipated this depressing  sadness   0.812\n",
       "49   40049  when health insurance cover tms let know cover...  sadness   0.811\n",
       "50   40050  somewhere hope despair day meeting my abuser i...  sadness   0.810\n",
       "51   40051                         <user> <user> i despondent  sadness   0.806\n",
       "52   40052  <number> applications dbs still waiting been y...  sadness   0.792\n",
       "53   40053  i miss social media place get laughs jump dms ...  sadness   0.792\n",
       "54   40054                         <user> the three r depress  sadness   0.792"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do a sanity check\n",
    "train[0:55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40000</td>\n",
       "      <td>depression sucks depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40001</td>\n",
       "      <td>feeling worthless always depression</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40002</td>\n",
       "      <td>feeling worthless always</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40003</td>\n",
       "      <td>my fibromyalgia really bad lately good mental ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40004</td>\n",
       "      <td>im think ima lay bed day sulk life hitting har...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>40005</td>\n",
       "      <td>so i try i fail i try i still fail confused lost</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>40006</td>\n",
       "      <td>extreme sadness</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>40007</td>\n",
       "      <td>life one word depressing</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>40008</td>\n",
       "      <td>panic attacks worst feeling really sick still ...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>40009</td>\n",
       "      <td>feel grim ugly atm</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SentID                                              Tweet  Emotion  Rating\n",
       "0   40000                        depression sucks depression  sadness   0.958\n",
       "1   40001                feeling worthless always depression  sadness   0.958\n",
       "2   40002                           feeling worthless always  sadness   0.958\n",
       "3   40003  my fibromyalgia really bad lately good mental ...  sadness   0.946\n",
       "4   40004  im think ima lay bed day sulk life hitting har...  sadness   0.934\n",
       "5   40005   so i try i fail i try i still fail confused lost  sadness   0.917\n",
       "6   40006                                    extreme sadness  sadness   0.917\n",
       "7   40007                           life one word depressing  sadness   0.917\n",
       "8   40008  panic attacks worst feeling really sick still ...  sadness   0.917\n",
       "9   40009                                 feel grim ugly atm  sadness   0.896"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do a sanity check\n",
    "train[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the distribution of text length (from below: there is no need to truncate any of texts)\n",
    "def show_text_len(train):\n",
    "    train[\"text_len\"] = train['Tweet'].map(lambda x: len(x.split()))\n",
    "    return train[\"text_len\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    860.000000\n",
       "mean      11.393023\n",
       "std        4.984628\n",
       "min        1.000000\n",
       "25%        7.000000\n",
       "50%       12.000000\n",
       "75%       15.000000\n",
       "max       32.000000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_text_len(train).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    673.000000\n",
       "mean      11.450223\n",
       "std        4.664740\n",
       "min        2.000000\n",
       "25%        8.000000\n",
       "50%       12.000000\n",
       "75%       15.000000\n",
       "max       27.000000\n",
       "Name: text_len, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_text_len(test).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not any reviews' length = 0 after text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = list(train['Tweet'])\n",
    "train_intensities = list(train['Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['depression sucks depression',\n",
       " 'feeling worthless always depression',\n",
       " 'feeling worthless always',\n",
       " 'my fibromyalgia really bad lately good mental state i feel overwhelmed anxiety bipolar depression',\n",
       " 'im think ima lay bed day sulk life hitting hard rn',\n",
       " 'so i try i fail i try i still fail confused lost',\n",
       " 'extreme sadness',\n",
       " 'life one word depressing',\n",
       " 'panic attacks worst feeling really sick still shaking i sleep anxiety depression',\n",
       " 'feel grim ugly atm',\n",
       " 'honestly depression kicking ass lately üòî',\n",
       " 'at age i see gray is gray bad eyes perspective depression healing justice',\n",
       " 'after <number> idk i start feeling depress sad lonely',\n",
       " 'i sulk much good',\n",
       " 'a night depression winning depression fml help',\n",
       " 'going home depressing',\n",
       " 'we even grieve one black body another one pops due pig brutality',\n",
       " 'it gloomy ass day',\n",
       " '<user> wow i really sadden terrible',\n",
       " 'my heads still ibiza body sat desk work depressing',\n",
       " '<user> leave sad',\n",
       " 'depression sucks',\n",
       " 'it week awful connectivity <user> service <number> g im paying unhappy poor service',\n",
       " 'this depressing shit ever',\n",
       " 'sometimes the worst place you can be is in your own head n n quotes worst enemy depression think too much',\n",
       " '<user> service cleveland oh what going unhappy',\n",
       " 'what sad evening clearing harvey cage belongings now final goodbye little man depressing',\n",
       " 'when <number> doe run opposite side üôÅ depression',\n",
       " 'the moment day start plaster smile face depression',\n",
       " 'i hate gloomy outside always gets depressing mood',\n",
       " 'people stealing things work quite damper day i going wear pajamas day',\n",
       " '<user> ring ring depression',\n",
       " 'it september still battling situation said handled march year <user> unacceptable unhappy',\n",
       " '<user> yeah i sure depressing talk parents phone instead talking downstairs',\n",
       " 'at home sick the blues cure i need ideas sore throat sick blues music fall weather carleton university ottawa',\n",
       " 'i would wish anxiety depression even worst people it fun anxiety depression',\n",
       " 'sorry main twitter im depress',\n",
       " '<user> i surrounded trump voters you right fucking terrifying redstate despair',\n",
       " 'come funeral tomorrow <number> mourn death gpa',\n",
       " '<user> jesus made think vols v fl games i seen i never seen win depression',\n",
       " 'after spending <number> <user> i offered <number> voucher i asked i could spend weekend was told valid today unhappy',\n",
       " 'thought i pretty solid gpa kin major i look average dpt programs i feel even discouraged üò™',\n",
       " 'depress üò≠',\n",
       " 'lament n saddened heart n so far amp n yet near n the years n tough amp scarred n this lonesome n road n still feared depression n n poetry poem',\n",
       " 'summer officially ends today sadness',\n",
       " 'well evaluation came back minimally effective student test scores parc c sunk eval time quit teaching',\n",
       " 'bad news fam life still hard awful depression anxiety at least i have buffy',\n",
       " 'oh i get depression happy do think i tried thousands times already you helping',\n",
       " 'im really constipated this depressing',\n",
       " 'when health insurance cover tms let know cover ect mental health psychology depression tms ect',\n",
       " 'somewhere hope despair day meeting my abuser in court ndv survivor',\n",
       " '<user> <user> i despondent',\n",
       " '<number> applications dbs still waiting been year going loose job üëçüèø <user> thanks unhappy crb',\n",
       " 'i miss social media place get laughs jump dms lol shit depressing üòû',\n",
       " '<user> the three r depress',\n",
       " 'too many gloomy days',\n",
       " 'the weather sure matches mood state today gloomy',\n",
       " 'dirty self loathing attitude mope talking',\n",
       " 'wearing black tomorrow i continue mourn lives recent victims police brutality blackout wu',\n",
       " 'am i person dislikes fall first day off all leaves things die depressing cold no flip flops',\n",
       " 'my soul weary fighting battles world black in america we are not safe',\n",
       " 'my prayers family friends amp members <user> mourn loss engineer ryan osler l odd rip',\n",
       " 'just wish i appreciated i when turn taken care i want break tired',\n",
       " '<user> though lately bad depression feel like body like taking little get',\n",
       " 'we left maine sadness',\n",
       " 'fucking sad person love try fighting happiness would anything',\n",
       " '<user> charlotte protest u wait <number> facts video u hate ask questions later sad protest police shootings suggestions',\n",
       " 'it interesting photo mono lisa crying well whole scene depressing ele <number>',\n",
       " 'the news disheartening everything going result lack understanding misinformation media sadness',\n",
       " 'never ever unhappy life lmao',\n",
       " '<user> worst possible decision i could made arriving uk shocking service poor coverage unhappy',\n",
       " 'condolences jc georges family sad',\n",
       " 'no one wants win wild card play cubs road sadness',\n",
       " 'groom gloom',\n",
       " 'i think i go work tomorrow since val left gb bo i need day mourn',\n",
       " 'good morning lovely people not gonna lie i woken feeling pretty glum',\n",
       " '<user> i help feel melancholic',\n",
       " '<user> <user> horrid disease my maternal grandmother sisters suffered affliction it hard',\n",
       " 'if anybody needs i drowning blues sea whiskey üçª',\n",
       " '<user> <user> god <user> full shilling seriously need major rethink lab gove lifetime sad',\n",
       " 'health think depression feel like',\n",
       " '<user> <user> <user> even hard facts seem sinking i despair üò©',\n",
       " 'so depressing darker much earlier',\n",
       " '<user> <user> to good hearts i lost job i responsible <number> families my information profile even dollar cant re tweet',\n",
       " 'currently un following anything relating disney world florida holiday blues depressing wan to go back',\n",
       " 'it begun ladies gents the war racism begun sadness segregation continues smh üòí',\n",
       " 'just wish i appreciated i when turn taken care i want break tired lost',\n",
       " 'so mary berry mel sue gone principles <user> gone fame fortune gb bo depressing',\n",
       " '<user> super sad especially talking real person bot makes feel real',\n",
       " '<user> call i laying bed moping like i intend next <number> months',\n",
       " 'baa ariss shhhh h sad song prefect night feeling alone',\n",
       " 'im gloomy today',\n",
       " 'honestly i come place i keep getting discouraged gotta tell keep pushing',\n",
       " '<user> <user> official got touch issue addressed unhappy pathetic services',\n",
       " 'back cardiff amazing <number> days away üò≠ depressing',\n",
       " 'lost faded',\n",
       " 'sometimes the worst place you can be is in your own head n n quotes worst enemy think too much',\n",
       " '<user> came ba <number> <user> least <number> passengers broken bags this new suitcase unhappy customer',\n",
       " 'tell i supposed feel broken hateful guilty love sadness',\n",
       " 'tomorrow last episode despair arc much shit gonna happen emotionally prepared']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "train_tweets[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.958,\n",
       " 0.958,\n",
       " 0.958,\n",
       " 0.946,\n",
       " 0.934,\n",
       " 0.917,\n",
       " 0.917,\n",
       " 0.917,\n",
       " 0.917,\n",
       " 0.8959999999999999]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "train_intensities[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = list(test['Tweet'])\n",
    "test_intensities = list(test['Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max text length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       max text length\n",
       "train               32\n",
       "test                27"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show Max Text Length of All Datasets for comparsion\n",
    "\n",
    "all_tweets_max_len = pd.DataFrame(np.array([max(show_text_len(train)), max(show_text_len(test))]))\n",
    "\n",
    "all_tweets_max_len.index = ['train', 'test']\n",
    "all_tweets_max_len.columns = ['max text length']\n",
    "\n",
    "all_tweets_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "all_tweets_max_len.plot(kind='bar')\n",
    "plt.title('Max Text Length of All Datasets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "we set the length to which each tweet vector will be zero padded to.\n",
    "this is based on the maximum length we got on the training set - we do not want to remove\n",
    "any words as the maximun length of the training set is not very big.\n",
    "'''\n",
    "\n",
    "max_len = max(show_text_len(train))\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Data PreparationÔºàFeature Extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Load Pre-trained Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_path = \"files/wv_models/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r', encoding = 'UTF-8')\n",
    "    model = {}\n",
    "    num = 1\n",
    "    for line in f:\n",
    "        try:\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            coefs = np.asarray(splitLine[1:], dtype = 'float32')\n",
    "            model[word] = coefs\n",
    "            num += 1\n",
    "        except Exception as e:\n",
    "            print(\"Failed at line \" + str(num))\n",
    "    print(\"Done. Found %s word vectors.\" %len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "Done. Found 1193514 word vectors.  words loaded!\n"
     ]
    }
   ],
   "source": [
    "# To download the pretrained glove model (2B tweets, 27B tokens) - [https://nlp.stanford.edu/projects/glove/   glove.twitter.27B.zip]\n",
    "# choose glove.twitter.27B.200d.txt from glove.twitter.27B.zip. [200-dimension vectors]\n",
    "\n",
    "wv_model_path1 = word_vector_path + \"glove.twitter.27B.200d.txt\"\n",
    " \n",
    "wv_model_g = loadGloveModel(wv_model_path1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download the pretrained word2vec model  - [https://github.com/FredericGodin/TwitterEmbeddings]\n",
    "\n",
    "wv_model_path2 = word_vector_path + \"word2vec_twitter_tokens.bin\"\n",
    "wv_model_w = gensim.models.KeyedVectors.load_word2vec_format(wv_model_path2, binary=True, unicode_errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vectors: 3039345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "words = list(wv_model_w.wv.vocab)\n",
    "print('Word Vectors: %d' % len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Define Averaged Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dimensions_g = len(wv_model_g['word'])\n",
    "w2v_dimensions_w = len(wv_model_w['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 400\n"
     ]
    }
   ],
   "source": [
    "print(w2v_dimensions_g,w2v_dimensions_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_averaged_embeddings(tweet, model, dimensions):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    vector_list = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            vector_list.append(model[token])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    if len(vector_list) == 0:\n",
    "        uni_vec_rep = np.zeros(dimensions).tolist()\n",
    "    else:\n",
    "        uni_vec_rep = sum(vector_list) / float(len(vector_list))\n",
    "    return uni_vec_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (2) Load Lexicon Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "lexicons_path = \"files/lexicons/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.Emoji_Vectors',\n",
       " '1.NRC-Emotion-Intensity-Lexicon',\n",
       " '3.NRC-Emotion-Lexicon',\n",
       " '4.NRC-Hashtag-Emotion-Lexicon',\n",
       " '5.NRC-Emoticon-Lexicon',\n",
       " '6.NRC-Emoticon-AffLexNegLex',\n",
       " '7.NRC-Hashtag-Sentiment-AffLexNegLex',\n",
       " '8.NRC-Hashtag-Sentiment-Lexicon',\n",
       " '9.DepecheMood_V1.0']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths2 = listdir(lexicons_path)\n",
    "paths2.sort()\n",
    "paths2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Emoji Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(('%s%s/' %(lexicons_path, paths2[0])) + listdir('%s%s/' %(lexicons_path, paths2[0]))[0], encoding = 'UTF-8') \\\n",
    "as emoji_file:\n",
    "    emoji_list = json.load(emoji_file)\n",
    "    \n",
    "emoji_dict = dict()\n",
    "for emoji in emoji_list:\n",
    "    emoji_dict[emoji[\"emoji\"]] = (emoji[\"name\"], emoji[\"polarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('joy', 3)\n"
     ]
    }
   ],
   "source": [
    "# do a sanity check\n",
    "print(emoji_dict[\"üòÇ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_emoji_intensity = PolynomialFeatures(5)\n",
    "\n",
    "def get_emoji_intensity(tweet):\n",
    "    score = 0.0\n",
    "    for emoji in emoji_dict.keys():\n",
    "        count = tweet.count(emoji)\n",
    "        score += count * emoji_dict[emoji][1]\n",
    "        \n",
    "    return normalize(poly_emoji_intensity.fit_transform(np.array([score]).reshape(1, -1))[0].reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00387988, 0.01163963, 0.03491889, 0.10475666, 0.31426998,\n",
       "       0.94280993])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do a sanity check\n",
    "get_emoji_intensity(\"üòÇ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Emotion Intensity Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_intensity_file_path = ('%s%s/' %(lexicons_path, paths2[1])) + listdir('%s%s/' %(lexicons_path, paths2[1]))[0]\n",
    "\n",
    "def get_word_affect_intensity_dict(emotion):\n",
    "    word_intensities = dict()\n",
    "\n",
    "    with open(affect_intensity_file_path) as affect_intensity_file:\n",
    "        for line in affect_intensity_file:\n",
    "            word_int_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_int_array[1] == emotion):\n",
    "                word_intensities[word_int_array[0]] = float(word_int_array[2])\n",
    "\n",
    "    return word_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'heartbreaking': 0.969,\n",
       " 'mourning': 0.969,\n",
       " 'tragic': 0.961,\n",
       " 'holocaust': 0.953,\n",
       " 'suicidal': 0.941,\n",
       " 'misery': 0.938,\n",
       " 'massacre': 0.931,\n",
       " 'euthanasia': 0.927,\n",
       " 'depression': 0.925,\n",
       " 'bereavement': 0.922,\n",
       " 'fatal': 0.922,\n",
       " 'grieving': 0.922,\n",
       " 'bereaved': 0.92,\n",
       " 'devastation': 0.917,\n",
       " 'death': 0.915,\n",
       " 'suicide': 0.912,\n",
       " 'devastated': 0.912,\n",
       " 'catastrophe': 0.911,\n",
       " 'horrifying': 0.907,\n",
       " 'died': 0.906,\n",
       " 'depressing': 0.906,\n",
       " 'tragedy': 0.906,\n",
       " 'anguish': 0.902,\n",
       " 'agony': 0.9,\n",
       " 'deadly': 0.898,\n",
       " 'weeping': 0.896,\n",
       " 'stillbirth': 0.891,\n",
       " 'deceased': 0.891,\n",
       " 'murderer': 0.877,\n",
       " 'cancer': 0.875,\n",
       " 'rape': 0.875,\n",
       " 'devastating': 0.875,\n",
       " 'dying': 0.875,\n",
       " 'morbidity': 0.864,\n",
       " 'sadness': 0.864,\n",
       " 'perish': 0.859,\n",
       " 'grief': 0.859,\n",
       " 'abandonment': 0.859,\n",
       " 'traumatic': 0.859,\n",
       " 'execution': 0.859,\n",
       " 'atrocity': 0.859,\n",
       " 'depressed': 0.859,\n",
       " 'crucifixion': 0.859,\n",
       " 'cadaver': 0.853,\n",
       " 'betrayed': 0.848,\n",
       " 'treachery': 0.848,\n",
       " 'destroyed': 0.844,\n",
       " 'sorrow': 0.844,\n",
       " 'funeral': 0.844,\n",
       " 'persecution': 0.844,\n",
       " 'miserable': 0.844,\n",
       " 'murderous': 0.844,\n",
       " 'slaughtering': 0.844,\n",
       " 'hopelessness': 0.844,\n",
       " 'horrific': 0.844,\n",
       " 'grieve': 0.844,\n",
       " 'sad': 0.844,\n",
       " 'suffering': 0.844,\n",
       " 'homicide': 0.844,\n",
       " 'unhappiness': 0.839,\n",
       " 'crippled': 0.836,\n",
       " 'bloodshed': 0.836,\n",
       " 'pained': 0.833,\n",
       " 'manslaughter': 0.833,\n",
       " 'carnage': 0.833,\n",
       " 'stillborn': 0.83,\n",
       " 'unbearable': 0.83,\n",
       " 'enslaved': 0.828,\n",
       " 'annihilated': 0.828,\n",
       " 'horrors': 0.828,\n",
       " 'mutilation': 0.828,\n",
       " 'murder': 0.828,\n",
       " 'annihilation': 0.828,\n",
       " 'morbid': 0.828,\n",
       " 'abandoned': 0.828,\n",
       " 'torment': 0.828,\n",
       " 'slavery': 0.828,\n",
       " 'mourn': 0.828,\n",
       " 'helplessness': 0.828,\n",
       " 'casualty': 0.828,\n",
       " 'sickness': 0.828,\n",
       " 'miscarriage': 0.824,\n",
       " 'starvation': 0.819,\n",
       " 'cruelty': 0.812,\n",
       " 'oppression': 0.812,\n",
       " 'terrorism': 0.812,\n",
       " 'killing': 0.812,\n",
       " 'burial': 0.812,\n",
       " 'sadly': 0.812,\n",
       " 'despair': 0.812,\n",
       " 'disgrace': 0.812,\n",
       " 'childloss': 0.812,\n",
       " 'distraught': 0.812,\n",
       " 'saddens': 0.812,\n",
       " 'failure': 0.812,\n",
       " 'mournful': 0.812,\n",
       " 'famine': 0.812,\n",
       " 'heartache': 0.812,\n",
       " 'bloody': 0.806,\n",
       " 'perishing': 0.804,\n",
       " 'inhumanity': 0.804,\n",
       " 'malignancy': 0.803,\n",
       " 'mortification': 0.802,\n",
       " 'lifeless': 0.797,\n",
       " 'dreadful': 0.797,\n",
       " 'gallows': 0.797,\n",
       " 'slave': 0.797,\n",
       " 'perished': 0.797,\n",
       " 'sarcoma': 0.797,\n",
       " 'dreadfully': 0.797,\n",
       " 'lethal': 0.797,\n",
       " 'assassination': 0.797,\n",
       " 'kill': 0.797,\n",
       " 'desolation': 0.797,\n",
       " 'leukemia': 0.797,\n",
       " 'devastate': 0.797,\n",
       " 'mortuary': 0.797,\n",
       " 'brokenheart': 0.792,\n",
       " 'banishment': 0.79,\n",
       " 'afflict': 0.789,\n",
       " 'disheartened': 0.788,\n",
       " 'tumour': 0.781,\n",
       " 'heartbreak': 0.781,\n",
       " 'terrorize': 0.781,\n",
       " 'crying': 0.781,\n",
       " 'bury': 0.781,\n",
       " 'demoralized': 0.781,\n",
       " 'desecration': 0.781,\n",
       " 'die': 0.773,\n",
       " 'lynch': 0.773,\n",
       " 'sufferer': 0.77,\n",
       " 'fearful': 0.766,\n",
       " 'loneliness': 0.766,\n",
       " 'destitute': 0.766,\n",
       " 'doomed': 0.766,\n",
       " 'dismemberment': 0.766,\n",
       " 'fatality': 0.766,\n",
       " 'slayer': 0.766,\n",
       " 'diseased': 0.766,\n",
       " 'torture': 0.766,\n",
       " 'cemetery': 0.766,\n",
       " 'abortion': 0.766,\n",
       " 'condolence': 0.766,\n",
       " 'disaster': 0.758,\n",
       " 'painfully': 0.758,\n",
       " 'moribund': 0.758,\n",
       " 'depress': 0.755,\n",
       " 'condemnation': 0.754,\n",
       " 'damnation': 0.75,\n",
       " 'guilt': 0.75,\n",
       " 'defeated': 0.75,\n",
       " 'terrorist': 0.75,\n",
       " 'depressive': 0.75,\n",
       " 'hemorrhage': 0.75,\n",
       " 'sickening': 0.75,\n",
       " 'incest': 0.75,\n",
       " 'obliteration': 0.75,\n",
       " 'sorrowful': 0.75,\n",
       " 'unhappy': 0.75,\n",
       " 'pandemic': 0.75,\n",
       " 'abduction': 0.75,\n",
       " 'doomsday': 0.75,\n",
       " 'regretful': 0.75,\n",
       " 'desperation': 0.75,\n",
       " 'corpse': 0.75,\n",
       " 'victimized': 0.75,\n",
       " 'painful': 0.75,\n",
       " 'deplorable': 0.75,\n",
       " 'cry': 0.75,\n",
       " 'unfairness': 0.745,\n",
       " 'molestation': 0.744,\n",
       " 'exile': 0.742,\n",
       " 'abysmal': 0.742,\n",
       " 'hellish': 0.738,\n",
       " 'exterminate': 0.736,\n",
       " 'assassin': 0.734,\n",
       " 'warfare': 0.734,\n",
       " 'horrid': 0.734,\n",
       " 'morgue': 0.734,\n",
       " 'homeless': 0.734,\n",
       " 'earthquake': 0.734,\n",
       " 'lonesome': 0.734,\n",
       " 'betrayal': 0.734,\n",
       " 'destroying': 0.734,\n",
       " 'orphan': 0.734,\n",
       " 'miserably': 0.734,\n",
       " 'slaughter': 0.734,\n",
       " 'disgraced': 0.734,\n",
       " 'battered': 0.734,\n",
       " 'disastrous': 0.734,\n",
       " 'listless': 0.729,\n",
       " 'alienated': 0.727,\n",
       " 'emptiness': 0.727,\n",
       " 'grave': 0.727,\n",
       " 'unfortunately': 0.727,\n",
       " 'fraught': 0.722,\n",
       " 'paralysis': 0.719,\n",
       " 'ashamed': 0.719,\n",
       " 'forsaken': 0.719,\n",
       " 'cried': 0.719,\n",
       " 'disheartening': 0.719,\n",
       " 'danger': 0.719,\n",
       " 'emaciated': 0.719,\n",
       " 'horror': 0.719,\n",
       " 'atrophy': 0.719,\n",
       " 'violently': 0.719,\n",
       " 'woe': 0.719,\n",
       " 'leprosy': 0.719,\n",
       " 'heartless': 0.719,\n",
       " 'cripple': 0.719,\n",
       " 'missing': 0.719,\n",
       " 'pain': 0.719,\n",
       " 'malicious': 0.719,\n",
       " 'demise': 0.717,\n",
       " 'violence': 0.712,\n",
       " 'disgruntled': 0.712,\n",
       " 'sickly': 0.712,\n",
       " 'rejected': 0.712,\n",
       " 'torn': 0.71,\n",
       " 'calamity': 0.709,\n",
       " 'grim': 0.708,\n",
       " 'grievous': 0.704,\n",
       " 'obit': 0.703,\n",
       " 'extinct': 0.703,\n",
       " 'isolate': 0.703,\n",
       " 'slaughterhouse': 0.703,\n",
       " 'paralyzed': 0.703,\n",
       " 'buried': 0.703,\n",
       " 'humiliate': 0.703,\n",
       " 'disfigured': 0.703,\n",
       " 'incurable': 0.703,\n",
       " 'punishing': 0.703,\n",
       " 'meltdown': 0.703,\n",
       " 'tearful': 0.703,\n",
       " 'abandon': 0.703,\n",
       " 'strangle': 0.703,\n",
       " 'suffocating': 0.703,\n",
       " 'decomposition': 0.703,\n",
       " 'inhuman': 0.703,\n",
       " 'crushed': 0.703,\n",
       " 'deformed': 0.703,\n",
       " 'carcinoma': 0.703,\n",
       " 'isolation': 0.703,\n",
       " 'hearse': 0.703,\n",
       " 'victim': 0.703,\n",
       " 'deformity': 0.703,\n",
       " 'oppressor': 0.703,\n",
       " 'hell': 0.7,\n",
       " 'lifesucks': 0.7,\n",
       " 'ruinous': 0.698,\n",
       " 'widow': 0.697,\n",
       " 'banish': 0.697,\n",
       " 'accursed': 0.697,\n",
       " 'ruined': 0.697,\n",
       " 'vanished': 0.695,\n",
       " 'displaced': 0.691,\n",
       " 'poverty': 0.69,\n",
       " 'nohope': 0.688,\n",
       " 'hurt': 0.688,\n",
       " 'epidemic': 0.688,\n",
       " 'infidelity': 0.688,\n",
       " 'loss': 0.688,\n",
       " 'dementia': 0.688,\n",
       " 'shooting': 0.688,\n",
       " 'sob': 0.688,\n",
       " 'neglected': 0.688,\n",
       " 'imprisoned': 0.688,\n",
       " 'hospice': 0.688,\n",
       " 'widower': 0.688,\n",
       " 'depraved': 0.688,\n",
       " 'illness': 0.688,\n",
       " 'travesty': 0.688,\n",
       " 'hopeless': 0.688,\n",
       " 'stab': 0.688,\n",
       " 'hurtful': 0.688,\n",
       " 'foreveralone': 0.688,\n",
       " 'teary': 0.688,\n",
       " 'insanity': 0.688,\n",
       " 'regretting': 0.688,\n",
       " 'pathetic': 0.688,\n",
       " 'terminal': 0.688,\n",
       " 'deserted': 0.688,\n",
       " 'banished': 0.688,\n",
       " 'dismissal': 0.686,\n",
       " 'hardship': 0.685,\n",
       " 'alienation': 0.685,\n",
       " 'choke': 0.682,\n",
       " 'kidnap': 0.682,\n",
       " 'bleeding': 0.673,\n",
       " 'coffin': 0.672,\n",
       " 'poison': 0.672,\n",
       " 'missingyou': 0.672,\n",
       " 'obituary': 0.672,\n",
       " 'gory': 0.672,\n",
       " 'malevolent': 0.672,\n",
       " 'evil': 0.672,\n",
       " 'despairing': 0.672,\n",
       " 'woefully': 0.672,\n",
       " 'frightful': 0.672,\n",
       " 'disabled': 0.672,\n",
       " 'imprisonment': 0.672,\n",
       " 'punished': 0.672,\n",
       " 'wretched': 0.672,\n",
       " 'outcast': 0.672,\n",
       " 'feelingdown': 0.672,\n",
       " 'decomposed': 0.672,\n",
       " 'wail': 0.672,\n",
       " 'disparage': 0.672,\n",
       " 'abortive': 0.672,\n",
       " 'wretch': 0.672,\n",
       " 'deprivation': 0.672,\n",
       " 'belittle': 0.672,\n",
       " 'barren': 0.67,\n",
       " 'poisoned': 0.667,\n",
       " 'executioner': 0.667,\n",
       " 'disease': 0.665,\n",
       " 'oppress': 0.664,\n",
       " 'disembodied': 0.66,\n",
       " 'weep': 0.656,\n",
       " 'steal': 0.656,\n",
       " 'blighted': 0.656,\n",
       " 'casket': 0.656,\n",
       " 'hate': 0.656,\n",
       " 'polio': 0.656,\n",
       " 'tear': 0.656,\n",
       " 'dreary': 0.656,\n",
       " 'demonic': 0.656,\n",
       " 'shitty': 0.656,\n",
       " 'bleak': 0.656,\n",
       " 'ruin': 0.656,\n",
       " 'ailing': 0.656,\n",
       " 'mangle': 0.656,\n",
       " 'jail': 0.656,\n",
       " 'lonely': 0.656,\n",
       " 'peril': 0.656,\n",
       " 'lamenting': 0.656,\n",
       " 'carcass': 0.653,\n",
       " 'cowardice': 0.652,\n",
       " 'regretted': 0.652,\n",
       " 'beating': 0.652,\n",
       " 'disability': 0.648,\n",
       " 'affliction': 0.645,\n",
       " 'scourge': 0.641,\n",
       " 'infertility': 0.641,\n",
       " 'shroud': 0.641,\n",
       " 'inequality': 0.641,\n",
       " 'sinful': 0.641,\n",
       " 'perilous': 0.641,\n",
       " 'emergency': 0.641,\n",
       " 'terribly': 0.641,\n",
       " 'worry': 0.641,\n",
       " 'powerless': 0.641,\n",
       " 'incarceration': 0.641,\n",
       " 'poisonous': 0.641,\n",
       " 'exorcism': 0.641,\n",
       " 'hatred': 0.641,\n",
       " 'termination': 0.641,\n",
       " 'woeful': 0.641,\n",
       " 'stricken': 0.641,\n",
       " 'awful': 0.641,\n",
       " 'drown': 0.641,\n",
       " 'failing': 0.641,\n",
       " 'psychosis': 0.638,\n",
       " 'dismay': 0.636,\n",
       " 'lament': 0.636,\n",
       " 'disappointed': 0.636,\n",
       " 'demolish': 0.636,\n",
       " 'burdensome': 0.634,\n",
       " 'mausoleum': 0.63,\n",
       " 'shattered': 0.63,\n",
       " 'impotence': 0.625,\n",
       " 'terminate': 0.625,\n",
       " 'lost': 0.625,\n",
       " 'posthumous': 0.625,\n",
       " 'wound': 0.625,\n",
       " 'regret': 0.625,\n",
       " 'palsy': 0.625,\n",
       " 'demolished': 0.625,\n",
       " 'gloom': 0.625,\n",
       " 'oppressive': 0.625,\n",
       " 'duress': 0.625,\n",
       " 'disappointing': 0.625,\n",
       " 'undesired': 0.625,\n",
       " 'dishonor': 0.625,\n",
       " 'hurting': 0.625,\n",
       " 'cursed': 0.625,\n",
       " 'abuse': 0.625,\n",
       " 'bitterly': 0.625,\n",
       " 'deteriorate': 0.625,\n",
       " 'insurmountable': 0.625,\n",
       " 'schizophrenia': 0.625,\n",
       " 'tyrant': 0.625,\n",
       " 'wrecked': 0.625,\n",
       " 'forlorn': 0.625,\n",
       " 'soulless': 0.623,\n",
       " 'divorce': 0.623,\n",
       " 'cremation': 0.621,\n",
       " 'worried': 0.621,\n",
       " 'forsake': 0.621,\n",
       " 'melancholy': 0.621,\n",
       " 'plight': 0.621,\n",
       " 'bomb': 0.621,\n",
       " 'unforgiving': 0.612,\n",
       " 'sepsis': 0.611,\n",
       " 'sacrifices': 0.609,\n",
       " 'irreparable': 0.609,\n",
       " 'languishing': 0.609,\n",
       " 'faithless': 0.609,\n",
       " 'dire': 0.609,\n",
       " 'gore': 0.609,\n",
       " 'subjugation': 0.609,\n",
       " 'worthless': 0.609,\n",
       " 'derogatory': 0.609,\n",
       " 'crushing': 0.609,\n",
       " 'excluded': 0.609,\n",
       " 'dilapidated': 0.609,\n",
       " 'breakup': 0.609,\n",
       " 'shameful': 0.609,\n",
       " 'bankrupt': 0.609,\n",
       " 'guilty': 0.609,\n",
       " 'ugliness': 0.609,\n",
       " 'injure': 0.609,\n",
       " 'shackle': 0.609,\n",
       " 'sinner': 0.609,\n",
       " 'shatter': 0.609,\n",
       " 'cruel': 0.609,\n",
       " 'disappoint': 0.609,\n",
       " 'injured': 0.609,\n",
       " 'debacle': 0.609,\n",
       " 'alcoholism': 0.609,\n",
       " 'overwhelmed': 0.609,\n",
       " 'fearfully': 0.609,\n",
       " 'degrading': 0.609,\n",
       " 'disparaging': 0.609,\n",
       " 'curse': 0.608,\n",
       " 'anthrax': 0.6,\n",
       " 'alone': 0.6,\n",
       " 'robbery': 0.6,\n",
       " 'angst': 0.598,\n",
       " 'obliterate': 0.594,\n",
       " 'nothingness': 0.594,\n",
       " 'remorse': 0.594,\n",
       " 'offender': 0.594,\n",
       " 'prison': 0.594,\n",
       " 'martyrdom': 0.594,\n",
       " 'losing': 0.594,\n",
       " 'irreconcilable': 0.594,\n",
       " 'deportation': 0.594,\n",
       " 'bawl': 0.594,\n",
       " 'poaching': 0.594,\n",
       " 'downfall': 0.594,\n",
       " 'petloss': 0.594,\n",
       " 'violation': 0.594,\n",
       " 'broken': 0.594,\n",
       " 'crumbling': 0.594,\n",
       " 'condolences': 0.594,\n",
       " 'chaos': 0.594,\n",
       " 'captivity': 0.594,\n",
       " 'ill': 0.594,\n",
       " 'shame': 0.594,\n",
       " 'deprived': 0.594,\n",
       " 'eviction': 0.594,\n",
       " 'crypt': 0.594,\n",
       " 'disappointment': 0.594,\n",
       " 'somber': 0.594,\n",
       " 'longing': 0.594,\n",
       " 'demon': 0.594,\n",
       " 'contaminated': 0.594,\n",
       " 'helpless': 0.594,\n",
       " 'betray': 0.594,\n",
       " 'vendetta': 0.594,\n",
       " 'abyss': 0.594,\n",
       " 'distress': 0.594,\n",
       " 'dysentery': 0.593,\n",
       " 'blight': 0.591,\n",
       " 'melancholic': 0.591,\n",
       " 'traitor': 0.588,\n",
       " 'rupture': 0.588,\n",
       " 'haggard': 0.587,\n",
       " 'lie': 0.585,\n",
       " 'cholera': 0.583,\n",
       " 'bitterness': 0.578,\n",
       " 'bully': 0.578,\n",
       " 'sordid': 0.578,\n",
       " 'interment': 0.578,\n",
       " 'damage': 0.578,\n",
       " 'frighten': 0.578,\n",
       " 'immoral': 0.578,\n",
       " 'undesirable': 0.578,\n",
       " 'gloomy': 0.578,\n",
       " 'unwell': 0.578,\n",
       " 'degeneracy': 0.578,\n",
       " 'terrible': 0.578,\n",
       " 'turmoil': 0.578,\n",
       " 'discrimination': 0.578,\n",
       " 'humiliation': 0.578,\n",
       " 'whine': 0.578,\n",
       " 'harmful': 0.578,\n",
       " 'denied': 0.578,\n",
       " 'captive': 0.576,\n",
       " 'devil': 0.576,\n",
       " 'delirium': 0.576,\n",
       " 'whimper': 0.576,\n",
       " 'disliked': 0.576,\n",
       " 'deplore': 0.576,\n",
       " 'pessimism': 0.576,\n",
       " 'damages': 0.576,\n",
       " 'hateful': 0.575,\n",
       " 'bigoted': 0.574,\n",
       " 'perdition': 0.569,\n",
       " 'adultery': 0.566,\n",
       " 'corrupting': 0.565,\n",
       " 'rheumatism': 0.562,\n",
       " 'wallow': 0.562,\n",
       " 'punitive': 0.562,\n",
       " 'dismal': 0.562,\n",
       " 'anathema': 0.562,\n",
       " 'comatose': 0.562,\n",
       " 'shipwreck': 0.562,\n",
       " 'deceive': 0.562,\n",
       " 'rejection': 0.562,\n",
       " 'infliction': 0.562,\n",
       " 'pauper': 0.562,\n",
       " 'injury': 0.562,\n",
       " 'resentment': 0.562,\n",
       " 'worsening': 0.562,\n",
       " 'autopsy': 0.562,\n",
       " 'deceit': 0.562,\n",
       " 'tyranny': 0.562,\n",
       " 'pessimist': 0.562,\n",
       " 'endocarditis': 0.562,\n",
       " 'sadday': 0.562,\n",
       " 'runaway': 0.562,\n",
       " 'flog': 0.562,\n",
       " 'urn': 0.562,\n",
       " 'worrying': 0.562,\n",
       " 'tomb': 0.562,\n",
       " 'deceitful': 0.562,\n",
       " 'upset': 0.562,\n",
       " 'expire': 0.562,\n",
       " 'departed': 0.558,\n",
       " 'martyr': 0.556,\n",
       " 'smite': 0.555,\n",
       " 'pity': 0.547,\n",
       " 'imissyou': 0.547,\n",
       " 'unkind': 0.547,\n",
       " 'chagrin': 0.547,\n",
       " 'discourage': 0.547,\n",
       " 'bomber': 0.547,\n",
       " 'surrendering': 0.547,\n",
       " 'unfulfilled': 0.547,\n",
       " 'hanging': 0.547,\n",
       " 'memorial': 0.547,\n",
       " 'plague': 0.547,\n",
       " 'malaria': 0.547,\n",
       " 'malaise': 0.547,\n",
       " 'hydrocephalus': 0.547,\n",
       " 'disillusionment': 0.547,\n",
       " 'shun': 0.547,\n",
       " 'resignation': 0.547,\n",
       " 'isolated': 0.547,\n",
       " 'glum': 0.547,\n",
       " 'shot': 0.547,\n",
       " 'bummed': 0.547,\n",
       " 'reject': 0.547,\n",
       " 'absence': 0.547,\n",
       " 'nefarious': 0.546,\n",
       " 'groan': 0.545,\n",
       " 'incrimination': 0.545,\n",
       " 'dark': 0.545,\n",
       " 'concussion': 0.545,\n",
       " 'aching': 0.544,\n",
       " 'weakly': 0.544,\n",
       " 'discontent': 0.543,\n",
       " 'undertaker': 0.538,\n",
       " 'assailant': 0.536,\n",
       " 'deterioration': 0.536,\n",
       " 'fooled': 0.531,\n",
       " 'embarrassment': 0.531,\n",
       " 'embolism': 0.531,\n",
       " 'prisoner': 0.531,\n",
       " 'homesick': 0.531,\n",
       " 'disgust': 0.531,\n",
       " 'confined': 0.531,\n",
       " 'attacking': 0.531,\n",
       " 'requiem': 0.531,\n",
       " 'sick': 0.531,\n",
       " 'dispossessed': 0.531,\n",
       " 'inimical': 0.531,\n",
       " 'antisocial': 0.531,\n",
       " 'outburst': 0.531,\n",
       " 'neurosis': 0.531,\n",
       " 'sorely': 0.531,\n",
       " 'forfeiture': 0.531,\n",
       " 'tarnish': 0.531,\n",
       " 'theft': 0.531,\n",
       " 'penance': 0.531,\n",
       " 'epitaph': 0.531,\n",
       " 'sullen': 0.531,\n",
       " 'ache': 0.531,\n",
       " 'console': 0.531,\n",
       " 'inflict': 0.531,\n",
       " 'disparity': 0.531,\n",
       " 'unpleasant': 0.53,\n",
       " 'forgotten': 0.53,\n",
       " 'grievance': 0.53,\n",
       " 'relapse': 0.53,\n",
       " 'disable': 0.529,\n",
       " 'defenseless': 0.526,\n",
       " 'defunct': 0.518,\n",
       " 'ridicule': 0.518,\n",
       " 'sequestration': 0.516,\n",
       " 'sin': 0.516,\n",
       " 'stripped': 0.516,\n",
       " 'retard': 0.516,\n",
       " 'unfriendly': 0.516,\n",
       " 'unlucky': 0.516,\n",
       " 'sore': 0.516,\n",
       " 'wither': 0.516,\n",
       " 'vegetative': 0.516,\n",
       " 'hideous': 0.516,\n",
       " 'accident': 0.516,\n",
       " 'stifled': 0.516,\n",
       " 'blindness': 0.516,\n",
       " 'dumps': 0.516,\n",
       " 'misfortune': 0.516,\n",
       " 'weakness': 0.516,\n",
       " 'displeased': 0.516,\n",
       " 'delusion': 0.516,\n",
       " 'decayed': 0.516,\n",
       " 'elimination': 0.516,\n",
       " 'dictatorship': 0.509,\n",
       " 'complain': 0.509,\n",
       " 'regrettable': 0.509,\n",
       " 'insecure': 0.509,\n",
       " 'unrequited': 0.509,\n",
       " 'lose': 0.509,\n",
       " 'witchcraft': 0.508,\n",
       " 'rot': 0.5,\n",
       " 'nauseous': 0.5,\n",
       " 'negative': 0.5,\n",
       " 'aftermath': 0.5,\n",
       " 'aggravating': 0.5,\n",
       " 'evict': 0.5,\n",
       " 'battled': 0.5,\n",
       " 'crash': 0.5,\n",
       " 'mad': 0.5,\n",
       " 'atherosclerosis': 0.5,\n",
       " 'disturbed': 0.5,\n",
       " 'ungodly': 0.5,\n",
       " 'messedup': 0.5,\n",
       " 'offended': 0.5,\n",
       " 'injurious': 0.5,\n",
       " 'weary': 0.5,\n",
       " 'mortality': 0.5,\n",
       " 'cringe': 0.5,\n",
       " 'drugged': 0.5,\n",
       " 'dolor': 0.5,\n",
       " 'chronic': 0.5,\n",
       " 'anxiety': 0.5,\n",
       " 'cytomegalovirus': 0.5,\n",
       " 'gonorrhea': 0.5,\n",
       " 'difficulty': 0.5,\n",
       " 'frowning': 0.5,\n",
       " 'discomfort': 0.5,\n",
       " 'endemic': 0.5,\n",
       " 'bier': 0.5,\n",
       " 'intolerant': 0.5,\n",
       " 'deport': 0.5,\n",
       " 'unfair': 0.5,\n",
       " 'senile': 0.5,\n",
       " 'disrespectful': 0.5,\n",
       " 'damper': 0.5,\n",
       " 'exclusion': 0.5,\n",
       " 'explode': 0.5,\n",
       " 'impossible': 0.5,\n",
       " 'bankruptcy': 0.5,\n",
       " 'prosecute': 0.5,\n",
       " 'coma': 0.5,\n",
       " 'howl': 0.5,\n",
       " 'derogation': 0.491,\n",
       " 'rob': 0.491,\n",
       " 'recession': 0.485,\n",
       " 'shriek': 0.485,\n",
       " 'reprisal': 0.484,\n",
       " 'wrongful': 0.484,\n",
       " 'adversity': 0.484,\n",
       " 'evasion': 0.484,\n",
       " 'offense': 0.484,\n",
       " 'beg': 0.484,\n",
       " 'sabotage': 0.484,\n",
       " 'moan': 0.484,\n",
       " 'criticism': 0.484,\n",
       " 'arsenic': 0.484,\n",
       " 'coward': 0.484,\n",
       " 'nasty': 0.484,\n",
       " 'hospital': 0.484,\n",
       " 'repress': 0.484,\n",
       " 'broke': 0.484,\n",
       " 'frailty': 0.484,\n",
       " 'insult': 0.484,\n",
       " 'worn': 0.484,\n",
       " 'resign': 0.484,\n",
       " 'wince': 0.484,\n",
       " 'disturbance': 0.484,\n",
       " 'weariness': 0.484,\n",
       " 'inadequate': 0.484,\n",
       " 'infectious': 0.483,\n",
       " 'dishonest': 0.482,\n",
       " 'decay': 0.482,\n",
       " 'dissolution': 0.48,\n",
       " 'lowest': 0.478,\n",
       " 'unhealthy': 0.474,\n",
       " 'irritation': 0.47,\n",
       " 'segregate': 0.469,\n",
       " 'dashed': 0.469,\n",
       " 'uncaring': 0.469,\n",
       " 'vulnerability': 0.469,\n",
       " 'fallout': 0.469,\n",
       " 'illegal': 0.469,\n",
       " 'unfortunate': 0.469,\n",
       " 'measles': 0.469,\n",
       " 'perversion': 0.469,\n",
       " 'unfavorable': 0.469,\n",
       " 'penalty': 0.469,\n",
       " 'collapse': 0.469,\n",
       " 'pernicious': 0.469,\n",
       " 'penal': 0.469,\n",
       " 'sedition': 0.469,\n",
       " 'uneasiness': 0.469,\n",
       " 'recidivism': 0.469,\n",
       " 'conflict': 0.469,\n",
       " 'collusion': 0.469,\n",
       " 'domination': 0.469,\n",
       " 'leftout': 0.469,\n",
       " 'moody': 0.469,\n",
       " 'ail': 0.469,\n",
       " 'frustrate': 0.469,\n",
       " 'disapproval': 0.469,\n",
       " 'criticize': 0.469,\n",
       " 'convict': 0.469,\n",
       " 'expulsion': 0.469,\n",
       " 'frayed': 0.467,\n",
       " 'infamy': 0.464,\n",
       " 'plunder': 0.461,\n",
       " 'depreciated': 0.46,\n",
       " 'wane': 0.456,\n",
       " 'unlawful': 0.455,\n",
       " 'badly': 0.455,\n",
       " 'confiscate': 0.453,\n",
       " 'refugee': 0.453,\n",
       " 'inhospitable': 0.453,\n",
       " 'secluded': 0.453,\n",
       " 'disapprove': 0.453,\n",
       " 'surrender': 0.453,\n",
       " 'inability': 0.453,\n",
       " 'criticise': 0.453,\n",
       " 'doldrums': 0.453,\n",
       " 'tribulation': 0.453,\n",
       " 'frown': 0.453,\n",
       " 'gone': 0.453,\n",
       " 'wrongly': 0.453,\n",
       " 'avalanche': 0.453,\n",
       " 'memorials': 0.453,\n",
       " 'invade': 0.453,\n",
       " 'bittersweet': 0.453,\n",
       " 'vulgarity': 0.453,\n",
       " 'despotism': 0.453,\n",
       " 'worse': 0.453,\n",
       " 'foreclose': 0.453,\n",
       " 'upheaval': 0.453,\n",
       " 'scarcity': 0.453,\n",
       " 'fell': 0.453,\n",
       " 'discriminate': 0.453,\n",
       " 'revolver': 0.453,\n",
       " 'wreck': 0.453,\n",
       " 'perplexity': 0.453,\n",
       " 'lowly': 0.448,\n",
       " 'guillotine': 0.446,\n",
       " 'lone': 0.446,\n",
       " 'encumbrance': 0.441,\n",
       " 'annulment': 0.44,\n",
       " 'bummer': 0.439,\n",
       " 'confinement': 0.439,\n",
       " 'badday': 0.439,\n",
       " 'delirious': 0.439,\n",
       " 'rejects': 0.438,\n",
       " 'paucity': 0.438,\n",
       " 'futile': 0.438,\n",
       " 'dislocated': 0.438,\n",
       " 'detainee': 0.438,\n",
       " 'appendicitis': 0.438,\n",
       " 'ghetto': 0.438,\n",
       " 'blues': 0.438,\n",
       " 'problem': 0.438,\n",
       " 'insignificant': 0.438,\n",
       " 'disconnected': 0.438,\n",
       " 'departure': 0.438,\n",
       " 'cancellation': 0.438,\n",
       " 'restriction': 0.438,\n",
       " 'forbid': 0.438,\n",
       " 'lunacy': 0.438,\n",
       " 'sympathize': 0.438,\n",
       " 'abscess': 0.438,\n",
       " 'insolvency': 0.438,\n",
       " 'apathetic': 0.438,\n",
       " 'bum': 0.438,\n",
       " 'imprudent': 0.438,\n",
       " 'underpaid': 0.438,\n",
       " 'deflate': 0.438,\n",
       " 'pensive': 0.438,\n",
       " 'absent': 0.438,\n",
       " 'darkened': 0.438,\n",
       " 'farewell': 0.438,\n",
       " 'unattainable': 0.438,\n",
       " 'lethargy': 0.438,\n",
       " 'animosity': 0.438,\n",
       " 'disapproving': 0.438,\n",
       " 'stigma': 0.438,\n",
       " 'indigent': 0.438,\n",
       " 'diminish': 0.438,\n",
       " 'coercion': 0.438,\n",
       " 'emotional': 0.438,\n",
       " 'fugitive': 0.438,\n",
       " 'disapproved': 0.438,\n",
       " 'disagreement': 0.438,\n",
       " 'disqualified': 0.435,\n",
       " 'bothering': 0.435,\n",
       " 'tripping': 0.429,\n",
       " 'sunk': 0.426,\n",
       " 'draining': 0.424,\n",
       " 'varicella': 0.424,\n",
       " 'lastday': 0.424,\n",
       " 'retribution': 0.424,\n",
       " 'haunted': 0.422,\n",
       " 'unwelcome': 0.422,\n",
       " 'handicap': 0.422,\n",
       " 'unequal': 0.422,\n",
       " 'cyst': 0.422,\n",
       " 'insolvent': 0.422,\n",
       " 'thief': 0.422,\n",
       " 'deviation': 0.422,\n",
       " 'jarring': 0.422,\n",
       " 'neuralgia': 0.422,\n",
       " 'scarce': 0.422,\n",
       " 'wrongdoing': 0.422,\n",
       " 'rip': 0.422,\n",
       " 'bad': 0.422,\n",
       " 'dispassionate': 0.422,\n",
       " 'cutting': 0.422,\n",
       " 'cardiomyopathy': 0.422,\n",
       " 'falling': 0.422,\n",
       " 'beggar': 0.422,\n",
       " 'difficulties': 0.421,\n",
       " 'drab': 0.42,\n",
       " 'invader': 0.42,\n",
       " 'fall': 0.418,\n",
       " 'illegitimate': 0.416,\n",
       " 'expel': 0.414,\n",
       " 'meaningless': 0.409,\n",
       " 'darkness': 0.409,\n",
       " 'syncope': 0.407,\n",
       " 'fault': 0.406,\n",
       " 'confine': 0.406,\n",
       " 'stroke': 0.406,\n",
       " 'obnoxious': 0.406,\n",
       " 'hoax': 0.406,\n",
       " 'wildfire': 0.406,\n",
       " 'subjected': 0.406,\n",
       " 'precarious': 0.406,\n",
       " 'darken': 0.406,\n",
       " 'fury': 0.406,\n",
       " 'feudalism': 0.406,\n",
       " 'enmity': 0.406,\n",
       " 'fatigued': 0.406,\n",
       " 'bitch': 0.406,\n",
       " 'unsatisfied': 0.405,\n",
       " 'spank': 0.403,\n",
       " 'sigh': 0.402,\n",
       " 'deluge': 0.402,\n",
       " 'spinster': 0.4,\n",
       " 'blue': 0.4,\n",
       " 'needalife': 0.398,\n",
       " 'owing': 0.398,\n",
       " 'pitfall': 0.394,\n",
       " 'pointless': 0.394,\n",
       " 'seriousness': 0.394,\n",
       " 'embarrass': 0.394,\n",
       " 'trickery': 0.391,\n",
       " 'dwarfed': 0.391,\n",
       " 'ravenous': 0.391,\n",
       " 'punch': 0.391,\n",
       " 'noose': 0.391,\n",
       " 'feeble': 0.391,\n",
       " 'debt': 0.391,\n",
       " 'daemon': 0.391,\n",
       " 'reproach': 0.391,\n",
       " 'stretcher': 0.391,\n",
       " 'condescension': 0.391,\n",
       " 'coldness': 0.391,\n",
       " 'banshee': 0.391,\n",
       " 'refused': 0.391,\n",
       " 'ifonly': 0.391,\n",
       " 'insulting': 0.391,\n",
       " 'inefficient': 0.391,\n",
       " 'exhausted': 0.391,\n",
       " 'ineptitude': 0.391,\n",
       " 'rue': 0.391,\n",
       " 'monsoon': 0.391,\n",
       " 'brute': 0.391,\n",
       " 'cage': 0.391,\n",
       " 'subvert': 0.384,\n",
       " 'jealousy': 0.382,\n",
       " 'geriatric': 0.379,\n",
       " 'struggle': 0.379,\n",
       " 'miss': 0.379,\n",
       " 'inexcusable': 0.379,\n",
       " 'ashes': 0.377,\n",
       " 'descent': 0.377,\n",
       " 'entangled': 0.377,\n",
       " 'blackness': 0.375,\n",
       " 'inferior': 0.375,\n",
       " 'landslide': 0.375,\n",
       " 'sucks': 0.375,\n",
       " 'oust': 0.375,\n",
       " 'funk': 0.375,\n",
       " 'withdraw': 0.375,\n",
       " 'goodbye': 0.375,\n",
       " 'exhaustion': 0.375,\n",
       " 'overcast': 0.375,\n",
       " 'disqualify': 0.375,\n",
       " 'scar': 0.375,\n",
       " 'dispel': 0.375,\n",
       " 'prostitution': 0.375,\n",
       " 'parting': 0.375,\n",
       " 'sympathy': 0.375,\n",
       " 'rabid': 0.375,\n",
       " 'slump': 0.375,\n",
       " 'inconsiderate': 0.375,\n",
       " 'noncompliance': 0.375,\n",
       " 'wasting': 0.375,\n",
       " 'murky': 0.375,\n",
       " 'unattractive': 0.373,\n",
       " 'delay': 0.373,\n",
       " 'apologize': 0.37,\n",
       " 'crazy': 0.368,\n",
       " 'bastard': 0.366,\n",
       " 'deteriorated': 0.365,\n",
       " 'inter': 0.364,\n",
       " 'empty': 0.364,\n",
       " 'mocking': 0.363,\n",
       " 'adder': 0.361,\n",
       " 'fruitless': 0.359,\n",
       " 'plaintive': 0.359,\n",
       " 'unemployed': 0.359,\n",
       " 'fuss': 0.359,\n",
       " 'servile': 0.359,\n",
       " 'lockup': 0.359,\n",
       " 'flounder': 0.359,\n",
       " 'arraignment': 0.359,\n",
       " 'unsuccessful': 0.359,\n",
       " 'inefficiency': 0.359,\n",
       " 'consecration': 0.359,\n",
       " 'obesity': 0.359,\n",
       " 'hindering': 0.359,\n",
       " 'fainting': 0.359,\n",
       " 'unable': 0.359,\n",
       " 'misunderstanding': 0.359,\n",
       " 'disconnect': 0.359,\n",
       " 'austere': 0.359,\n",
       " 'wrangling': 0.359,\n",
       " 'taunt': 0.359,\n",
       " 'tremor': 0.359,\n",
       " 'affront': 0.359,\n",
       " 'corse': 0.359,\n",
       " 'defendant': 0.359,\n",
       " 'unpopular': 0.359,\n",
       " 'idiocy': 0.359,\n",
       " 'doubt': 0.359,\n",
       " 'perpetrator': 0.359,\n",
       " 'numbness': 0.359,\n",
       " 'hunter': 0.359,\n",
       " 'unrest': 0.351,\n",
       " 'absentee': 0.348,\n",
       " 'wimpy': 0.348,\n",
       " 'intervention': 0.348,\n",
       " 'spoiler': 0.348,\n",
       " 'waste': 0.348,\n",
       " 'flaw': 0.347,\n",
       " 'blindly': 0.344,\n",
       " 'cumbersome': 0.344,\n",
       " 'thrash': 0.344,\n",
       " 'militia': 0.344,\n",
       " 'lawsuit': 0.344,\n",
       " 'detention': 0.344,\n",
       " 'uninspired': 0.344,\n",
       " 'desert': 0.344,\n",
       " 'uninvited': 0.344,\n",
       " 'attenuation': 0.344,\n",
       " 'litigate': 0.344,\n",
       " ...}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_intensities = get_word_affect_intensity_dict(emotion)\n",
    "word_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_emo_int = PolynomialFeatures(10)\n",
    "\n",
    "def get_emo_int_vector(tweet):\n",
    "    score = 0.0\n",
    "    for word in word_intensities.keys():\n",
    "        if word in tweet:\n",
    "            score += tweet.count(word) * float(word_intensities[word])\n",
    "    return normalize(poly_emo_int.fit_transform(np.array([score]).reshape(1, -1))[0].reshape(1, -1))[0]\n",
    "    # return [score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_emo_int_vector(\"furious\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_sentiwordnet = PolynomialFeatures(5)\n",
    "\n",
    "def get_sentiwordnetscore(tweet):\n",
    "    \n",
    "    score = np.zeros(2)\n",
    "    \n",
    "    for word in tweet.split():\n",
    "        synsetlist = list(swn.senti_synsets(word))\n",
    "        \n",
    "        if synsetlist:\n",
    "            score[0] += synsetlist[0].pos_score()\n",
    "            score[1] += synsetlist[0].neg_score()\n",
    "            \n",
    "#     return tweet_score.tolist()\n",
    "    return normalize(poly_sentiwordnet.fit_transform(np.array([score]).reshape(1, -1))[0].reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.37500185e-01, 2.34375046e-01, 2.34375046e-01, 5.85937616e-02,\n",
       "       5.85937616e-02, 5.85937616e-02, 1.46484404e-02, 1.46484404e-02,\n",
       "       1.46484404e-02, 1.46484404e-02, 3.66211010e-03, 3.66211010e-03,\n",
       "       3.66211010e-03, 3.66211010e-03, 3.66211010e-03, 9.15527525e-04,\n",
       "       9.15527525e-04, 9.15527525e-04, 9.15527525e-04, 9.15527525e-04,\n",
       "       9.15527525e-04])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiwordnetscore(\"furious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentiment Emotion Presence Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_emotion_lex_file_path = ('%s%s/' %(lexicons_path, paths2[2])) + listdir('%s%s/' %(lexicons_path, paths2[2]))[0]\n",
    "\n",
    "def get_affect_presence_list(emotion):\n",
    "    word_list = list()\n",
    "    \n",
    "    with open(sentiment_emotion_lex_file_path) as sentiment_emotion_lex_file:\n",
    "        lines = sentiment_emotion_lex_file.readlines()[1:] \n",
    "        for line in lines:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "\n",
    "            if (word_array[1] == emotion and word_array[2] == '1'):\n",
    "                word_list.append(word_array[0])\n",
    "                \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abandon',\n",
       " 'abandoned',\n",
       " 'abandonment',\n",
       " 'abduction',\n",
       " 'abortion',\n",
       " 'abortive',\n",
       " 'abscess',\n",
       " 'absence',\n",
       " 'absent',\n",
       " 'absentee',\n",
       " 'abuse',\n",
       " 'abysmal',\n",
       " 'abyss',\n",
       " 'accident',\n",
       " 'accursed',\n",
       " 'ache',\n",
       " 'aching',\n",
       " 'adder',\n",
       " 'adrift',\n",
       " 'adultery',\n",
       " 'adverse',\n",
       " 'adversity',\n",
       " 'afflict',\n",
       " 'affliction',\n",
       " 'affront',\n",
       " 'aftermath',\n",
       " 'aggravating',\n",
       " 'agony',\n",
       " 'ail',\n",
       " 'ailing',\n",
       " 'alcoholism',\n",
       " 'alienated',\n",
       " 'alienation',\n",
       " 'anathema',\n",
       " 'anchorage',\n",
       " 'anguish',\n",
       " 'animosity',\n",
       " 'annihilated',\n",
       " 'annihilation',\n",
       " 'annulment',\n",
       " 'anthrax',\n",
       " 'antisocial',\n",
       " 'anxiety',\n",
       " 'apathetic',\n",
       " 'apathy',\n",
       " 'apologize',\n",
       " 'appendicitis',\n",
       " 'arid',\n",
       " 'arraignment',\n",
       " 'arsenic',\n",
       " 'art',\n",
       " 'ashamed',\n",
       " 'ashes',\n",
       " 'assailant',\n",
       " 'assassin',\n",
       " 'assassination',\n",
       " 'atherosclerosis',\n",
       " 'atrocity',\n",
       " 'atrophy',\n",
       " 'attacking',\n",
       " 'attenuation',\n",
       " 'austere',\n",
       " 'autopsy',\n",
       " 'avalanche',\n",
       " 'awful',\n",
       " 'backwater',\n",
       " 'bacteria',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'bang',\n",
       " 'banish',\n",
       " 'banished',\n",
       " 'banishment',\n",
       " 'bankrupt',\n",
       " 'bankruptcy',\n",
       " 'banshee',\n",
       " 'barren',\n",
       " 'bastard',\n",
       " 'battered',\n",
       " 'battled',\n",
       " 'beating',\n",
       " 'beg',\n",
       " 'beggar',\n",
       " 'belittle',\n",
       " 'bereaved',\n",
       " 'bereavement',\n",
       " 'betray',\n",
       " 'betrayal',\n",
       " 'bier',\n",
       " 'bigoted',\n",
       " 'bitch',\n",
       " 'bitterly',\n",
       " 'bitterness',\n",
       " 'black',\n",
       " 'blackness',\n",
       " 'bleak',\n",
       " 'bleeding',\n",
       " 'blemish',\n",
       " 'blight',\n",
       " 'blighted',\n",
       " 'blindly',\n",
       " 'blindness',\n",
       " 'blockade',\n",
       " 'bloodshed',\n",
       " 'bloody',\n",
       " 'blue',\n",
       " 'blues',\n",
       " 'blunder',\n",
       " 'bomb',\n",
       " 'bomber',\n",
       " 'bondage',\n",
       " 'boredom',\n",
       " 'bothering',\n",
       " 'bottom',\n",
       " 'breakup',\n",
       " 'broke',\n",
       " 'broken',\n",
       " 'brute',\n",
       " 'bugaboo',\n",
       " 'bum',\n",
       " 'burdensome',\n",
       " 'burial',\n",
       " 'buried',\n",
       " 'burke',\n",
       " 'bury',\n",
       " 'cadaver',\n",
       " 'cage',\n",
       " 'calamity',\n",
       " 'cancel',\n",
       " 'cancer',\n",
       " 'captive',\n",
       " 'captivity',\n",
       " 'carcass',\n",
       " 'carcinoma',\n",
       " 'cardiomyopathy',\n",
       " 'carnage',\n",
       " 'case',\n",
       " 'casket',\n",
       " 'casualty',\n",
       " 'cataract',\n",
       " 'catastrophe',\n",
       " 'cemetery',\n",
       " 'chagrin',\n",
       " 'chaos',\n",
       " 'chargeable',\n",
       " 'choke',\n",
       " 'cholera',\n",
       " 'chronic',\n",
       " 'closure',\n",
       " 'clouded',\n",
       " 'cloudy',\n",
       " 'cocaine',\n",
       " 'coercion',\n",
       " 'coffin',\n",
       " 'coldness',\n",
       " 'collapse',\n",
       " 'collusion',\n",
       " 'coma',\n",
       " 'comatose',\n",
       " 'commemorate',\n",
       " 'committal',\n",
       " 'communism',\n",
       " 'complain',\n",
       " 'conceal',\n",
       " 'concerned',\n",
       " 'concussion',\n",
       " 'condemnation',\n",
       " 'condescension',\n",
       " 'condolence',\n",
       " 'confession',\n",
       " 'confine',\n",
       " 'confined',\n",
       " 'confinement',\n",
       " 'confiscate',\n",
       " 'conflict',\n",
       " 'consecration',\n",
       " 'console',\n",
       " 'constraint',\n",
       " 'contaminated',\n",
       " 'convict',\n",
       " 'corpse',\n",
       " 'corrupting',\n",
       " 'corse',\n",
       " 'couch',\n",
       " 'coward',\n",
       " 'crash',\n",
       " 'crazy',\n",
       " 'cremation',\n",
       " 'cringe',\n",
       " 'cripple',\n",
       " 'crippled',\n",
       " 'criticism',\n",
       " 'criticize',\n",
       " 'cross',\n",
       " 'crucifixion',\n",
       " 'cruel',\n",
       " 'cruelty',\n",
       " 'crumbling',\n",
       " 'crushed',\n",
       " 'cry',\n",
       " 'crying',\n",
       " 'crypt',\n",
       " 'cumbersome',\n",
       " 'cupping',\n",
       " 'curse',\n",
       " 'cursed',\n",
       " 'cutting',\n",
       " 'cyst',\n",
       " 'cytomegalovirus',\n",
       " 'daemon',\n",
       " 'damage',\n",
       " 'damages',\n",
       " 'damnation',\n",
       " 'danger',\n",
       " 'dark',\n",
       " 'darken',\n",
       " 'darkened',\n",
       " 'darkness',\n",
       " 'dashed',\n",
       " 'deadly',\n",
       " 'death',\n",
       " 'debacle',\n",
       " 'debt',\n",
       " 'decay',\n",
       " 'decayed',\n",
       " 'deceased',\n",
       " 'deceit',\n",
       " 'deceitful',\n",
       " 'deceive',\n",
       " 'decomposed',\n",
       " 'decomposition',\n",
       " 'default',\n",
       " 'defeated',\n",
       " 'defendant',\n",
       " 'defenseless',\n",
       " 'deflate',\n",
       " 'deformed',\n",
       " 'deformity',\n",
       " 'defunct',\n",
       " 'defy',\n",
       " 'degeneracy',\n",
       " 'degrading',\n",
       " 'delay',\n",
       " 'delirious',\n",
       " 'delirium',\n",
       " 'deluge',\n",
       " 'delusion',\n",
       " 'dementia',\n",
       " 'demise',\n",
       " 'demolish',\n",
       " 'demon',\n",
       " 'demonic',\n",
       " 'demonstrative',\n",
       " 'demoralized',\n",
       " 'denied',\n",
       " 'depart',\n",
       " 'departed',\n",
       " 'departure',\n",
       " 'dependence',\n",
       " 'deplorable',\n",
       " 'deplore',\n",
       " 'deport',\n",
       " 'deportation',\n",
       " 'depraved',\n",
       " 'depreciated',\n",
       " 'depress',\n",
       " 'depressed',\n",
       " 'depressing',\n",
       " 'depression',\n",
       " 'depressive',\n",
       " 'deprivation',\n",
       " 'derogation',\n",
       " 'derogatory',\n",
       " 'descent',\n",
       " 'desecration',\n",
       " 'desert',\n",
       " 'deserted',\n",
       " 'desolation',\n",
       " 'despair',\n",
       " 'despairing',\n",
       " 'despotism',\n",
       " 'destination',\n",
       " 'destitute',\n",
       " 'destroyed',\n",
       " 'destroying',\n",
       " 'detainee',\n",
       " 'detention',\n",
       " 'deteriorate',\n",
       " 'deteriorated',\n",
       " 'deterioration',\n",
       " 'devastate',\n",
       " 'devastating',\n",
       " 'devastation',\n",
       " 'deviation',\n",
       " 'devil',\n",
       " 'dictatorship',\n",
       " 'die',\n",
       " 'difficulties',\n",
       " 'difficulty',\n",
       " 'dilapidated',\n",
       " 'diminish',\n",
       " 'dire',\n",
       " 'disability',\n",
       " 'disable',\n",
       " 'disabled',\n",
       " 'disagreeing',\n",
       " 'disagreement',\n",
       " 'disallowed',\n",
       " 'disappoint',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'disappointment',\n",
       " 'disapproval',\n",
       " 'disapprove',\n",
       " 'disapproved',\n",
       " 'disapproving',\n",
       " 'disaster',\n",
       " 'disastrous',\n",
       " 'discolored',\n",
       " 'discomfort',\n",
       " 'disconnect',\n",
       " 'disconnected',\n",
       " 'discontent',\n",
       " 'discontinuity',\n",
       " 'discourage',\n",
       " 'discriminate',\n",
       " 'discrimination',\n",
       " 'disease',\n",
       " 'diseased',\n",
       " 'disembodied',\n",
       " 'disfigured',\n",
       " 'disgrace',\n",
       " 'disgraced',\n",
       " 'disgruntled',\n",
       " 'disgust',\n",
       " 'disheartened',\n",
       " 'disheartening',\n",
       " 'dishonest',\n",
       " 'dishonor',\n",
       " 'disillusionment',\n",
       " 'disliked',\n",
       " 'dislocated',\n",
       " 'dismal',\n",
       " 'dismay',\n",
       " 'dismemberment',\n",
       " 'dismissal',\n",
       " 'disparage',\n",
       " 'disparaging',\n",
       " 'disparity',\n",
       " 'dispassionate',\n",
       " 'dispel',\n",
       " 'displaced',\n",
       " 'displeased',\n",
       " 'dispossessed',\n",
       " 'disqualified',\n",
       " 'disqualify',\n",
       " 'disrespectful',\n",
       " 'disservice',\n",
       " 'dissolution',\n",
       " 'distraught',\n",
       " 'distress',\n",
       " 'disturbance',\n",
       " 'disturbed',\n",
       " 'divorce',\n",
       " 'doldrums',\n",
       " 'dole',\n",
       " 'dolor',\n",
       " 'domination',\n",
       " 'doomed',\n",
       " 'doomsday',\n",
       " 'doubt',\n",
       " 'downfall',\n",
       " 'drab',\n",
       " 'dreadful',\n",
       " 'dreadfully',\n",
       " 'dreary',\n",
       " 'drown',\n",
       " 'drugged',\n",
       " 'dull',\n",
       " 'dumps',\n",
       " 'duress',\n",
       " 'dwarfed',\n",
       " 'dying',\n",
       " 'dysentery',\n",
       " 'earthquake',\n",
       " 'elimination',\n",
       " 'emaciated',\n",
       " 'embarrass',\n",
       " 'embarrassment',\n",
       " 'embolism',\n",
       " 'emergency',\n",
       " 'emptiness',\n",
       " 'encumbrance',\n",
       " 'endemic',\n",
       " 'endless',\n",
       " 'endocarditis',\n",
       " 'enmity',\n",
       " 'enslaved',\n",
       " 'entangled',\n",
       " 'epidemic',\n",
       " 'epitaph',\n",
       " 'error',\n",
       " 'eschew',\n",
       " 'esteem',\n",
       " 'euthanasia',\n",
       " 'evanescence',\n",
       " 'evasion',\n",
       " 'evict',\n",
       " 'eviction',\n",
       " 'evil',\n",
       " 'excluded',\n",
       " 'excluding',\n",
       " 'exclusion',\n",
       " 'execution',\n",
       " 'executioner',\n",
       " 'exhausted',\n",
       " 'exhaustion',\n",
       " 'exile',\n",
       " 'exorcism',\n",
       " 'expel',\n",
       " 'expire',\n",
       " 'explode',\n",
       " 'expulsion',\n",
       " 'exterminate',\n",
       " 'extinct',\n",
       " 'failing',\n",
       " 'failure',\n",
       " 'fainting',\n",
       " 'faithless',\n",
       " 'fall',\n",
       " 'falling',\n",
       " 'famine',\n",
       " 'fasting',\n",
       " 'fat',\n",
       " 'fatal',\n",
       " 'fatality',\n",
       " 'fatigued',\n",
       " 'fatty',\n",
       " 'fault',\n",
       " 'fearful',\n",
       " 'fearfully',\n",
       " 'feeble',\n",
       " 'feeling',\n",
       " 'fell',\n",
       " 'feudalism',\n",
       " 'flaccid',\n",
       " 'flaw',\n",
       " 'fleece',\n",
       " 'flinch',\n",
       " 'flog',\n",
       " 'flop',\n",
       " 'flounder',\n",
       " 'forbid',\n",
       " 'foreclose',\n",
       " 'forfeit',\n",
       " 'forfeiture',\n",
       " 'forgotten',\n",
       " 'forlorn',\n",
       " 'forsake',\n",
       " 'forsaken',\n",
       " 'fortress',\n",
       " 'fragile',\n",
       " 'frailty',\n",
       " 'fraught',\n",
       " 'frayed',\n",
       " 'frighten',\n",
       " 'frightful',\n",
       " 'frown',\n",
       " 'frowning',\n",
       " 'fruitless',\n",
       " 'frustrate',\n",
       " 'fugitive',\n",
       " 'funeral',\n",
       " 'funk',\n",
       " 'furrow',\n",
       " 'fury',\n",
       " 'fuss',\n",
       " 'futile',\n",
       " 'gallows',\n",
       " 'geriatric',\n",
       " 'ghetto',\n",
       " 'gloom',\n",
       " 'gloomy',\n",
       " 'glum',\n",
       " 'gonorrhea',\n",
       " 'gore',\n",
       " 'gory',\n",
       " 'grave',\n",
       " 'gray',\n",
       " 'grief',\n",
       " 'grievance',\n",
       " 'grieve',\n",
       " 'grievous',\n",
       " 'grim',\n",
       " 'groan',\n",
       " 'grounded',\n",
       " 'grumpy',\n",
       " 'guillotine',\n",
       " 'guilt',\n",
       " 'guilty',\n",
       " 'gullible',\n",
       " 'haggard',\n",
       " 'halter',\n",
       " 'halting',\n",
       " 'hamstring',\n",
       " 'handicap',\n",
       " 'hanging',\n",
       " 'hardship',\n",
       " 'harmful',\n",
       " 'harry',\n",
       " 'hate',\n",
       " 'hateful',\n",
       " 'hatred',\n",
       " 'haunted',\n",
       " 'hearse',\n",
       " 'heartache',\n",
       " 'heartfelt',\n",
       " 'heartless',\n",
       " 'hell',\n",
       " 'hellish',\n",
       " 'helpless',\n",
       " 'helplessness',\n",
       " 'hemorrhage',\n",
       " 'hermit',\n",
       " 'hideous',\n",
       " 'hindering',\n",
       " 'hoary',\n",
       " 'hoax',\n",
       " 'hobo',\n",
       " 'hollow',\n",
       " 'holocaust',\n",
       " 'homeless',\n",
       " 'homesick',\n",
       " 'homicide',\n",
       " 'honest',\n",
       " 'hopeless',\n",
       " 'hopelessness',\n",
       " 'horrid',\n",
       " 'horrific',\n",
       " 'horrifying',\n",
       " 'horror',\n",
       " 'horrors',\n",
       " 'hospice',\n",
       " 'hospital',\n",
       " 'howl',\n",
       " 'humble',\n",
       " 'humbled',\n",
       " 'humbug',\n",
       " 'humiliate',\n",
       " 'humiliation',\n",
       " 'hunter',\n",
       " 'hurt',\n",
       " 'hurtful',\n",
       " 'hurting',\n",
       " 'hut',\n",
       " 'hydrocephalus',\n",
       " 'hymn',\n",
       " 'idiocy',\n",
       " 'ill',\n",
       " 'illegal',\n",
       " 'illegitimate',\n",
       " 'illness',\n",
       " 'immoral',\n",
       " 'impossible',\n",
       " 'impotence',\n",
       " 'imprisoned',\n",
       " 'imprisonment',\n",
       " 'imprudent',\n",
       " 'inability',\n",
       " 'inadequate',\n",
       " 'inappropriate',\n",
       " 'incarceration',\n",
       " 'incase',\n",
       " 'incest',\n",
       " 'income',\n",
       " 'incompatible',\n",
       " 'incompetent',\n",
       " 'inconsequential',\n",
       " 'inconsiderate',\n",
       " 'inconvenient',\n",
       " 'incrimination',\n",
       " 'incurable',\n",
       " 'indifference',\n",
       " 'indigent',\n",
       " 'inefficiency',\n",
       " 'inefficient',\n",
       " 'ineptitude',\n",
       " 'inequality',\n",
       " 'inexcusable',\n",
       " 'infamy',\n",
       " 'infanticide',\n",
       " 'infectious',\n",
       " 'inferior',\n",
       " 'infertility',\n",
       " 'infidelity',\n",
       " 'inflict',\n",
       " 'infliction',\n",
       " 'inhibit',\n",
       " 'inhospitable',\n",
       " 'inhuman',\n",
       " 'inhumanity',\n",
       " 'inimical',\n",
       " 'injure',\n",
       " 'injured',\n",
       " 'injurious',\n",
       " 'injury',\n",
       " 'insanity',\n",
       " 'insecure',\n",
       " 'insignificant',\n",
       " 'insolvency',\n",
       " 'insolvent',\n",
       " 'insult',\n",
       " 'insulting',\n",
       " 'insurmountable',\n",
       " 'inter',\n",
       " 'intercede',\n",
       " 'interested',\n",
       " 'interment',\n",
       " 'interrupted',\n",
       " 'intervention',\n",
       " 'intolerant',\n",
       " 'invade',\n",
       " 'invader',\n",
       " 'invalid',\n",
       " 'irreconcilable',\n",
       " 'irreparable',\n",
       " 'irritation',\n",
       " 'isolate',\n",
       " 'isolated',\n",
       " 'isolation',\n",
       " 'jail',\n",
       " 'jarring',\n",
       " 'jealousy',\n",
       " 'jurisprudence',\n",
       " 'kennel',\n",
       " 'kidnap',\n",
       " 'kill',\n",
       " 'killing',\n",
       " 'knell',\n",
       " 'labored',\n",
       " 'lace',\n",
       " 'lagging',\n",
       " 'lament',\n",
       " 'lamenting',\n",
       " 'landslide',\n",
       " 'languishing',\n",
       " 'late',\n",
       " 'lawsuit',\n",
       " 'lax',\n",
       " 'leave',\n",
       " 'leprosy',\n",
       " 'lesbian',\n",
       " 'lethal',\n",
       " 'lethargy',\n",
       " 'leukemia',\n",
       " 'lie',\n",
       " 'lifeless',\n",
       " 'limited',\n",
       " 'liquor',\n",
       " 'listless',\n",
       " 'litigate',\n",
       " 'lockup',\n",
       " 'lone',\n",
       " 'loneliness',\n",
       " 'lonely',\n",
       " 'lonesome',\n",
       " 'longing',\n",
       " 'lose',\n",
       " 'losing',\n",
       " 'loss',\n",
       " 'lost',\n",
       " 'lovely',\n",
       " 'lower',\n",
       " 'lowest',\n",
       " 'lowly',\n",
       " 'lunacy',\n",
       " 'lush',\n",
       " 'lynch',\n",
       " 'mad',\n",
       " 'malaise',\n",
       " 'malaria',\n",
       " 'malevolent',\n",
       " 'malicious',\n",
       " 'malignancy',\n",
       " 'mangle',\n",
       " 'manslaughter',\n",
       " 'margin',\n",
       " 'martyr',\n",
       " 'martyrdom',\n",
       " 'massacre',\n",
       " 'mausoleum',\n",
       " 'meaningless',\n",
       " 'measles',\n",
       " 'meek',\n",
       " 'melancholic',\n",
       " 'melancholy',\n",
       " 'melodrama',\n",
       " 'meltdown',\n",
       " 'memorials',\n",
       " 'militia',\n",
       " 'miscarriage',\n",
       " 'miserable',\n",
       " 'miserably',\n",
       " 'misery',\n",
       " 'misfortune',\n",
       " 'mishap',\n",
       " 'misrepresentation',\n",
       " 'missing',\n",
       " 'mistake',\n",
       " 'misunderstanding',\n",
       " 'moan',\n",
       " 'mocking',\n",
       " 'molestation',\n",
       " 'monsoon',\n",
       " 'moody',\n",
       " 'morbid',\n",
       " 'morbidity',\n",
       " 'morgue',\n",
       " 'moribund',\n",
       " 'mortality',\n",
       " 'mortification',\n",
       " 'mortuary',\n",
       " 'mother',\n",
       " 'mourn',\n",
       " 'mournful',\n",
       " 'mourning',\n",
       " 'mug',\n",
       " 'murder',\n",
       " 'murderer',\n",
       " 'murderous',\n",
       " 'murky',\n",
       " 'music',\n",
       " 'musical',\n",
       " 'mutilation',\n",
       " 'myopia',\n",
       " 'napkin',\n",
       " 'nasty',\n",
       " 'nauseous',\n",
       " 'necessity',\n",
       " 'nefarious',\n",
       " 'negative',\n",
       " 'neglected',\n",
       " 'negro',\n",
       " 'nepotism',\n",
       " 'nether',\n",
       " 'neuralgia',\n",
       " 'neurosis',\n",
       " 'noncompliance',\n",
       " 'nonsensical',\n",
       " 'noose',\n",
       " 'nothingness',\n",
       " 'numbness',\n",
       " 'nutritious',\n",
       " 'obesity',\n",
       " 'obit',\n",
       " 'obituary',\n",
       " 'obliterate',\n",
       " 'obliteration',\n",
       " 'obnoxious',\n",
       " 'obstacle',\n",
       " 'oddity',\n",
       " 'offended',\n",
       " 'offender',\n",
       " 'offense',\n",
       " 'older',\n",
       " 'onerous',\n",
       " 'opera',\n",
       " 'opium',\n",
       " 'oppress',\n",
       " 'oppression',\n",
       " 'oppressive',\n",
       " 'oppressor',\n",
       " 'orchestra',\n",
       " 'orphan',\n",
       " 'oust',\n",
       " 'outburst',\n",
       " 'outcast',\n",
       " 'ovation',\n",
       " 'overdue',\n",
       " 'overload',\n",
       " 'overpriced',\n",
       " 'overwhelmed',\n",
       " 'owing',\n",
       " 'pain',\n",
       " 'pained',\n",
       " 'painful',\n",
       " 'painfully',\n",
       " 'palsy',\n",
       " 'pandemic',\n",
       " 'paralysis',\n",
       " 'paralyzed',\n",
       " 'pare',\n",
       " 'parting',\n",
       " 'pathetic',\n",
       " 'paucity',\n",
       " 'pauper',\n",
       " 'penal',\n",
       " 'penalty',\n",
       " 'penance',\n",
       " 'pensive',\n",
       " 'perdition',\n",
       " 'peril',\n",
       " 'perilous',\n",
       " 'perish',\n",
       " 'perished',\n",
       " 'perishing',\n",
       " 'pernicious',\n",
       " 'perpetrator',\n",
       " 'perplexity',\n",
       " 'persecution',\n",
       " 'perversion',\n",
       " 'pessimism',\n",
       " 'pessimist',\n",
       " 'pine',\n",
       " 'pious',\n",
       " 'pitfall',\n",
       " 'pity',\n",
       " 'plague',\n",
       " 'plaintive',\n",
       " 'plea',\n",
       " 'plight',\n",
       " 'plunder',\n",
       " 'poaching',\n",
       " 'pointless',\n",
       " 'poison',\n",
       " 'poisoned',\n",
       " 'poisonous',\n",
       " 'polio',\n",
       " 'posthumous',\n",
       " 'poverty',\n",
       " 'powerless',\n",
       " 'precarious',\n",
       " 'priesthood',\n",
       " 'prison',\n",
       " 'prisoner',\n",
       " 'probation',\n",
       " 'problem',\n",
       " 'procession',\n",
       " 'progression',\n",
       " 'prosecute',\n",
       " 'prostitution',\n",
       " 'psychosis',\n",
       " 'punch',\n",
       " 'punished',\n",
       " 'punishing',\n",
       " 'punitive',\n",
       " 'quiet',\n",
       " 'rabid',\n",
       " 'rack',\n",
       " 'rainy',\n",
       " 'rape',\n",
       " 'rating',\n",
       " 'ravenous',\n",
       " 'recession',\n",
       " 'recidivism',\n",
       " 'refugee',\n",
       " 'refused',\n",
       " 'regret',\n",
       " 'regrettable',\n",
       " 'regretted',\n",
       " 'regretting',\n",
       " 'reject',\n",
       " 'rejection',\n",
       " 'rejects',\n",
       " 'relapse',\n",
       " 'relics',\n",
       " 'remiss',\n",
       " 'remorse',\n",
       " 'remove',\n",
       " 'repress',\n",
       " 'reprisal',\n",
       " 'reproach',\n",
       " 'requiem',\n",
       " 'resentment',\n",
       " 'resign',\n",
       " 'resignation',\n",
       " 'resigned',\n",
       " 'resisting',\n",
       " 'restrict',\n",
       " 'restriction',\n",
       " 'retard',\n",
       " 'retirement',\n",
       " 'retribution',\n",
       " 'revoke',\n",
       " 'revolution',\n",
       " 'revolver',\n",
       " 'rheumatism',\n",
       " 'ridicule',\n",
       " 'rob',\n",
       " 'robbery',\n",
       " 'romance',\n",
       " 'rot',\n",
       " 'rubble',\n",
       " 'rue',\n",
       " 'ruin',\n",
       " 'ruined',\n",
       " 'ruinous',\n",
       " 'rumor',\n",
       " 'runaway',\n",
       " 'rupture',\n",
       " 'sabotage',\n",
       " 'sacrifices',\n",
       " 'sadly',\n",
       " 'sadness',\n",
       " 'sanctify',\n",
       " 'sap',\n",
       " 'sarcasm',\n",
       " 'sarcoma',\n",
       " 'savor',\n",
       " 'scar',\n",
       " 'scarce',\n",
       " 'scarcely',\n",
       " 'scarcity',\n",
       " 'schizophrenia',\n",
       " 'scold',\n",
       " 'scourge',\n",
       " 'scrapie',\n",
       " 'secluded',\n",
       " 'sedition',\n",
       " 'segregate',\n",
       " 'senile',\n",
       " 'senseless',\n",
       " 'sentence',\n",
       " 'sepsis',\n",
       " 'sequestration',\n",
       " 'seriousness',\n",
       " 'servile',\n",
       " 'setback',\n",
       " 'severance',\n",
       " 'shack',\n",
       " 'shackle',\n",
       " 'shame',\n",
       " 'shameful',\n",
       " 'shatter',\n",
       " 'shattered',\n",
       " 'shell',\n",
       " 'shipwreck',\n",
       " 'shiver',\n",
       " 'shortage',\n",
       " 'shot',\n",
       " 'shriek',\n",
       " 'shrink',\n",
       " 'shroud',\n",
       " 'shun',\n",
       " 'sick',\n",
       " 'sickening',\n",
       " 'sickly',\n",
       " 'sickness',\n",
       " 'sin',\n",
       " 'sinful',\n",
       " 'sing',\n",
       " 'sinner',\n",
       " 'sisterhood',\n",
       " 'skid',\n",
       " 'slaughter',\n",
       " 'slaughterhouse',\n",
       " 'slaughtering',\n",
       " 'slave',\n",
       " 'slavery',\n",
       " 'slayer',\n",
       " 'sluggish',\n",
       " 'slump',\n",
       " 'slur',\n",
       " 'smite',\n",
       " 'snort',\n",
       " 'sob',\n",
       " 'socialist',\n",
       " 'soldier',\n",
       " 'somber',\n",
       " 'sonnet',\n",
       " 'sordid',\n",
       " 'sore',\n",
       " 'sorely',\n",
       " 'soreness',\n",
       " 'sorrow',\n",
       " 'sorrowful',\n",
       " 'soulless',\n",
       " 'spank',\n",
       " 'specter',\n",
       " 'speculation',\n",
       " 'spinster',\n",
       " 'splitting',\n",
       " 'spoiler',\n",
       " 'sprain',\n",
       " 'squall',\n",
       " 'stab',\n",
       " 'stagnant',\n",
       " 'starvation',\n",
       " 'steal',\n",
       " 'sterile',\n",
       " 'stifled',\n",
       " 'stigma',\n",
       " 'stillborn',\n",
       " 'stillness',\n",
       " 'stingy',\n",
       " 'stint',\n",
       " 'strangle',\n",
       " 'stretcher',\n",
       " 'stricken',\n",
       " 'strip',\n",
       " ...]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = get_affect_presence_list(emotion)\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_emotion_feature(tweet):\n",
    "    \n",
    "    vector = np.zeros(1)\n",
    "    for word in word_list:\n",
    "        if word in tweet.split():\n",
    "            vector[0] = 1.0\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment_emotion_feature(\"furious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Hashtag Emotion Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_emotion_lex_file_path = ('%s%s/' %(lexicons_path, paths2[3])) + listdir('%s%s/' %(lexicons_path, paths2[3]))[0]\n",
    "    \n",
    "def get_hashtag_emotion_intensity(emotion):\n",
    "    hastag_intensities = dict()\n",
    "    \n",
    "    with open(hashtag_emotion_lex_file_path) as hashtag_emotion_lex_file:\n",
    "        for line in hashtag_emotion_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            \n",
    "            if (word_array[0] == emotion):\n",
    "                hastag_intensities[clean_str(word_array[1])] = float(word_array[2])\n",
    "\n",
    "    return hastag_intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_emotion_intensities = get_hashtag_emotion_intensity(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_hashtag_emotion = PolynomialFeatures(10)\n",
    "\n",
    "def get_hashtag_emotion_vector(tweet):\n",
    "    score = 0.0\n",
    "    for word in hashtag_emotion_intensities.keys():\n",
    "        if word in tweet:\n",
    "            score += tweet.count(word) * float(hashtag_emotion_intensities[word])\n",
    "            \n",
    "#     return [score]\n",
    "    return normalize(poly_hashtag_emotion.fit_transform(np.array([score]).reshape(1, -1))[0].reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.03558200e-05, 5.95833308e-05, 1.74405812e-04, 5.10501626e-04,\n",
       "       1.49428455e-03, 4.37390637e-03, 1.28028206e-02, 3.74750167e-02,\n",
       "       1.09692772e-01, 3.21080690e-01, 9.39832293e-01])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hashtag_emotion_vector(\"furious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Emoticon Sentiment Lexicon¬∂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bigrams-pmilexicon.txt', 'pairs-pmilexicon.txt', 'unigrams-pmilexicon.txt']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir('%s%s/' %(lexicons_path, paths2[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_lexicon_unigrams_file_path = ('%s%s/' %(lexicons_path, paths2[4])) + listdir('%s%s/' %(lexicons_path, paths2[4]))[2]\n",
    "emoticon_lexicon_bigrams_file_path = ('%s%s/' %(lexicons_path, paths2[4])) + listdir('%s%s/' %(lexicons_path, paths2[4]))[0]\n",
    "emoticon_lexicon_pairs_file_path = ('%s%s/' %(lexicons_path, paths2[4])) + listdir('%s%s/' %(lexicons_path, paths2[4]))[1]\n",
    "pair_split_string = \"---\"\n",
    "    \n",
    "def get_emoticon_lexicon_unigram_dict():\n",
    "    emoticon_lexicon_unigrams = dict()\n",
    "    with open(emoticon_lexicon_unigrams_file_path, encoding = 'UTF-8') as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_lexicon_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_lexicon_unigrams\n",
    "\n",
    "def get_emoticon_lexicon_bigram_dict():\n",
    "    emoticon_lexicon_bigrams = dict()\n",
    "    with open(emoticon_lexicon_bigrams_file_path, encoding = 'UTF-8') as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_lexicon_bigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_lexicon_bigrams\n",
    "\n",
    "def get_emoticon_lexicon_pairs_dict():\n",
    "    emoticon_lexicon_pairs = dict()\n",
    "    with open(emoticon_lexicon_pairs_file_path, encoding = 'UTF-8') as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            pair = word_array[0].split(pair_split_string)\n",
    "            token_1 = clean_str(pair[0])\n",
    "            token_2 = clean_str(pair[1])\n",
    "            if token_1 and token_2:\n",
    "                token_1_dict = None\n",
    "                if token_1 in emoticon_lexicon_pairs.keys():\n",
    "                    token_1_dict = emoticon_lexicon_pairs[token_1]\n",
    "                else:\n",
    "                    token_1_dict = dict()\n",
    "                    \n",
    "                token_1_dict[token_2] = np.array([float(val) for val in word_array[1:]])\n",
    "                emoticon_lexicon_pairs[token_1] = token_1_dict\n",
    "    \n",
    "    return emoticon_lexicon_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_lexicon_unigram_dict = get_emoticon_lexicon_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_lexicon_bigram_dict = get_emoticon_lexicon_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_lexicon_pairs_dict = get_emoticon_lexicon_pairs_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_emoticon_lexicon = PolynomialFeatures(5)\n",
    "\n",
    "def get_unigram_sentiment_emoticon_lexicon_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for token in tokens:\n",
    "        word = clean_str(token)\n",
    "        if word in emoticon_lexicon_unigram_dict.keys():\n",
    "            vector_list += emoticon_lexicon_unigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "        \n",
    "#     return vector_list\n",
    "    return normalize(poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].reshape(1, -1))[0]\n",
    "\n",
    "\n",
    "def get_bigram_sentiment_emoticon_lexicon_vector(tokens):\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for bi_token in bi_tokens:\n",
    "        word = clean_str(\" \".join(bi_token))\n",
    "        if word in emoticon_lexicon_bigram_dict.keys():\n",
    "            vector_list += emoticon_lexicon_bigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "        \n",
    "#     return vector_list\n",
    "    return normalize(poly_emoji_intensity.fit_transform(vector_list.reshape(1, -1))[0].reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_pair_sentiment_emoticon_lexicon_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        word_1 = clean_str(tokens[i])\n",
    "        if word_1 in emoticon_lexicon_pairs_dict.keys():\n",
    "            token_1_dict = emoticon_lexicon_pairs_dict[word_1]\n",
    "            for j in range(i, len(tokens)):\n",
    "                word_2 = clean_str(tokens[j])\n",
    "                if word_2 in token_1_dict.keys():\n",
    "                    vector_list += token_1_dict[word_2]\n",
    "                    counter += 1\n",
    "                    \n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "        \n",
    "#     return vector_list\n",
    "    return normalize(poly_emoji_intensity.fit_transform(vector_list.reshape(1, -1))[0].reshape(1, -1))[0].tolist()\n",
    "\n",
    "def get_sentiment_emoticon_lexicon_vector(tweet):\n",
    "    \n",
    "    tokens = word_tokenize(tweet)\n",
    "    final_list = np.asarray([])\n",
    "    \n",
    "    # Adding unigram features\n",
    "    final_list = np.append(\n",
    "        final_list,\n",
    "        get_unigram_sentiment_emoticon_lexicon_vector(tokens)\n",
    "    )\n",
    "    \n",
    "    final_list = np.append(\n",
    "        final_list,\n",
    "        get_bigram_sentiment_emoticon_lexicon_vector(tokens)\n",
    "    )\n",
    "    \n",
    "    final_list = np.append(\n",
    "        final_list,\n",
    "        get_pair_sentiment_emoticon_lexicon_vector(tokens)\n",
    "    )\n",
    "   \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.21931637e-10, 1.32048704e-10, 3.39255564e-08, 2.76623768e-08,\n",
       "       3.34083222e-11, 8.58316578e-09, 6.99858133e-09, 2.20516117e-06,\n",
       "       1.79805449e-06, 1.46610597e-06, 8.45230551e-12, 2.17154094e-09,\n",
       "       1.77064108e-09, 5.57905775e-07, 4.54907786e-07, 3.70924810e-07,\n",
       "       1.43335476e-04, 1.16873542e-04, 9.52968880e-05, 7.77036164e-05,\n",
       "       2.13843329e-12, 5.49399858e-10, 4.47972192e-10, 1.41150161e-07,\n",
       "       1.15091670e-07, 9.38439770e-08, 3.62638754e-05, 2.95690061e-05,\n",
       "       2.41101127e-05, 1.96590149e-05, 9.31680593e-03, 7.59678022e-03,\n",
       "       6.19429772e-03, 5.05073506e-03, 4.11829167e-03, 5.41023623e-13,\n",
       "       1.38998164e-10, 1.13336965e-10, 3.57109908e-08, 2.91181925e-08,\n",
       "       2.37425262e-08, 9.17476048e-06, 7.48095854e-06, 6.09985850e-06,\n",
       "       4.97373078e-06, 2.35715190e-03, 1.92198540e-03, 1.56715732e-03,\n",
       "       1.27783597e-03, 1.04192779e-03, 6.05592386e-01, 4.93790715e-01,\n",
       "       4.02629352e-01, 3.28297779e-01, 2.67688958e-01, 2.18269458e-01,\n",
       "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment_emoticon_lexicon_vector(\"furious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Emoticon Sentiment Aff-Neg Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Emoticon-AFFLEX-NEGLEX-bigrams.txt', 'Emoticon-AFFLEX-NEGLEX-unigrams.txt']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir('%s%s/' %(lexicons_path, paths2[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_afflex_unigrams_file_path = ('%s%s/' %(lexicons_path, paths2[5])) + listdir('%s%s/' %(lexicons_path, paths2[5]))[1]\n",
    "emoticon_afflex_bigrams_file_path = ('%s%s/' %(lexicons_path, paths2[5])) + listdir('%s%s/' %(lexicons_path, paths2[5]))[0]\n",
    "    \n",
    "def get_emoticon_afflex_unigram_dict():\n",
    "    emoticon_afflex_unigrams = dict()\n",
    "    with open(emoticon_afflex_unigrams_file_path, encoding = 'UTF-8') as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_afflex_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_afflex_unigrams\n",
    "\n",
    "def get_emoticon_afflex_bigram_dict():\n",
    "    emoticon_afflex_bigrams = dict()\n",
    "    with open(emoticon_afflex_bigrams_file_path, encoding = 'UTF-8') as emoticon_lexicon_file:\n",
    "        for line in emoticon_lexicon_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            emoticon_afflex_bigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return emoticon_afflex_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_afflex_unigram_dict = get_emoticon_afflex_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoticon_afflex_bigram_dict = get_emoticon_afflex_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_emoticon_lexicon = PolynomialFeatures(5)\n",
    "# poly_emoticon_lexicon = PolynomialFeatures(1)\n",
    "\n",
    "def get_unigram_sentiment_emoticon_afflex_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for token in tokens:\n",
    "        word = clean_str(token)\n",
    "        if word in emoticon_afflex_unigram_dict.keys():\n",
    "            vector_list += emoticon_afflex_unigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "        \n",
    "#     return vector_list\n",
    "    return normalize(poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].reshape(1, -1))[0]\n",
    "\n",
    "\n",
    "def get_bigram_sentiment_emoticon_afflex_vector(tokens):\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for bi_token in bi_tokens:\n",
    "        word = clean_str(\" \".join(bi_token))\n",
    "        if word in emoticon_afflex_bigram_dict.keys():\n",
    "            vector_list += emoticon_afflex_bigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "        \n",
    "#     return vector_list\n",
    "    return normalize(poly_emoticon_lexicon.fit_transform(vector_list.reshape(1, -1))[0].reshape(1, -1))[0]\n",
    "\n",
    "def get_sentiment_emoticon_afflex_vector(tweet):\n",
    "    final_list = np.asarray([])\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Adding unigram features\n",
    "    final_list = np.append(final_list, get_unigram_sentiment_emoticon_afflex_vector(tokens))\n",
    "    \n",
    "    # Adding bigram featunigram_list =ures\n",
    "    final_list = np.append(final_list, get_bigram_sentiment_emoticon_afflex_vector(tokens))\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.95719929e-10, 1.33357616e-10, 2.77003950e-08, 2.09731562e-08,\n",
       "       4.49415166e-11, 9.33503312e-09, 7.06795365e-09, 1.93902765e-06,\n",
       "       1.46812094e-06, 1.11157728e-06, 1.51452911e-11, 3.14590616e-09,\n",
       "       2.38190038e-09, 6.53452318e-07, 4.94756755e-07, 3.74601543e-07,\n",
       "       1.35731936e-04, 1.02768466e-04, 7.78104096e-05, 5.89135958e-05,\n",
       "       5.10396310e-12, 1.06017038e-09, 8.02700428e-10, 2.20213431e-07,\n",
       "       1.66733027e-07, 1.26240720e-07, 4.57416623e-05, 3.46329729e-05,\n",
       "       2.62221080e-05, 1.98538818e-05, 9.50123549e-03, 7.19379259e-03,\n",
       "       5.44672867e-03, 4.12395171e-03, 3.12242058e-03, 1.72003556e-12,\n",
       "       3.57277417e-10, 2.70510044e-10, 7.42119264e-08, 5.61890300e-08,\n",
       "       4.25431227e-08, 1.54149402e-05, 1.16713119e-05, 8.83685041e-06,\n",
       "       6.69075817e-06, 3.20191636e-03, 2.42430810e-03, 1.83554756e-03,\n",
       "       1.38977173e-03, 1.05225574e-03, 6.65086484e-01, 5.03565481e-01,\n",
       "       3.81271007e-01, 2.88676620e-01, 2.18569441e-01, 1.65488291e-01,\n",
       "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment_emoticon_afflex_vector(\"furious\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8) Hashtag Sentiment Aff-Neg Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HS-AFFLEX-NEGLEX-bigrams.txt', 'HS-AFFLEX-NEGLEX-unigrams.txt']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir('%s%s/' %(lexicons_path, paths2[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_affneglex_unigrams_file_path = ('%s%s/' %(lexicons_path, paths2[6])) + listdir('%s%s/' %(lexicons_path, paths2[6]))[1]\n",
    "hashtag_affneglex_bigrams_file_path = ('%s%s/' %(lexicons_path, paths2[6])) + listdir('%s%s/' %(lexicons_path, paths2[6]))[0]\n",
    "    \n",
    "def get_hashtag_affneglex_unigram_dict():\n",
    "    hashtag_affneglex_unigrams = dict()\n",
    "    with open(hashtag_affneglex_unigrams_file_path) as hashtag_sent_lex_file:\n",
    "        for line in hashtag_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            hashtag_affneglex_unigrams[clean_str(word_array[0])] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return hashtag_affneglex_unigrams\n",
    "\n",
    "def get_hashtag_affneglex_bigram_dict():\n",
    "    hashtag_affneglex_bigrams = dict()\n",
    "    with open(hashtag_affneglex_bigrams_file_path) as hashtag_sent_lex_file:\n",
    "        for line in hashtag_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            hashtag_affneglex_bigrams[clean_str(word_array[0])] = np.array([float(val) for val in word_array[1:]])\n",
    "\n",
    "    return hashtag_affneglex_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_affneglex_unigram_dict = get_hashtag_affneglex_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_affneglex_bigram_dict = get_hashtag_affneglex_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_hashtag_sent_affneglex = PolynomialFeatures(5)\n",
    "\n",
    "def get_unigram_sentiment_hashtag_affneglex_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for token in tokens:\n",
    "        word = clean_str(token)\n",
    "        if word in hashtag_affneglex_unigram_dict.keys():\n",
    "            vector_list += hashtag_affneglex_unigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "        \n",
    "#     return vector_list\n",
    "    return normalize(poly_hashtag_sent_affneglex.fit_transform(vector_list.reshape(1, -1))[0].reshape(1, -1))[0]\n",
    "\n",
    "def get_bigram_sentiment_hashtag_affneglex_vector(tokens):\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for bi_token in bi_tokens:\n",
    "        word = clean_str(\" \".join(bi_token))\n",
    "        if word in hashtag_affneglex_bigram_dict.keys():\n",
    "            vector_list += hashtag_affneglex_bigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "        \n",
    "#     return vector_list\n",
    "    return normalize(poly_hashtag_sent_affneglex.fit_transform(vector_list.reshape(1, -1))[0].reshape(1, -1))[0]\n",
    "\n",
    "def get_sentiment_hashtag_affneglex_vector(tweet):\n",
    "    final_list = np.asarray([])\n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Adding unigram features\n",
    "    final_list = np.append(final_list, get_unigram_sentiment_hashtag_affneglex_vector(tokens))\n",
    "    # Adding bigram features\n",
    "    final_list = np.append(final_list, get_bigram_sentiment_hashtag_affneglex_vector(tokens))\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.50661452e-05, -5.16768782e-06,  6.02645810e-05,  1.35595307e-04,\n",
       "        1.77251692e-06, -2.06707513e-05, -4.65091904e-05,  2.41058324e-04,\n",
       "        5.42381229e-04,  1.22035777e-03, -6.07973304e-07,  7.09006769e-06,\n",
       "        1.59526523e-05, -8.26830051e-05, -1.86036762e-04, -4.18582713e-04,\n",
       "        9.64233296e-04,  2.16952492e-03,  4.88143106e-03,  1.09832199e-02,\n",
       "        2.08534843e-07, -2.43189322e-06, -5.47175974e-06,  2.83602708e-05,\n",
       "        6.38106092e-05,  1.43573871e-04, -3.30732020e-04, -7.44147046e-04,\n",
       "       -1.67433085e-03, -3.76724442e-03,  3.85693318e-03,  8.67809966e-03,\n",
       "        1.95257242e-02,  4.39328795e-02,  9.88489790e-02, -7.15274513e-08,\n",
       "        8.34139374e-07,  1.87681359e-06, -9.72757287e-06, -2.18870390e-05,\n",
       "       -4.92458377e-05,  1.13441083e-04,  2.55242437e-04,  5.74295483e-04,\n",
       "        1.29216484e-03, -1.32292808e-03, -2.97658818e-03, -6.69732341e-03,\n",
       "       -1.50689777e-02, -3.39051998e-02,  1.54277327e-02,  3.47123987e-02,\n",
       "        7.81028970e-02,  1.75731518e-01,  3.95395916e-01,  8.89640811e-01,\n",
       "        1.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment_hashtag_affneglex_vector(\"furious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9) Hashtag Sentiment Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HS-bigrams.txt', 'HS-pairs.txt', 'HS-unigrams.txt']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listdir('%s%s/' %(lexicons_path, paths2[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_sent_lex_unigrams_file_path = ('%s%s/' %(lexicons_path, paths2[7])) + listdir('%s%s/' %(lexicons_path, paths2[7]))[2]\n",
    "hash_sent_lex_bigrams_file_path = ('%s%s/' %(lexicons_path, paths2[7])) + listdir('%s%s/' %(lexicons_path, paths2[7]))[0]\n",
    "hash_sent_lex_pairs_file_path = ('%s%s/' %(lexicons_path, paths2[7])) + listdir('%s%s/' %(lexicons_path, paths2[7]))[1]\n",
    "pair_split_string = \"---\"\n",
    "\n",
    "\n",
    "def get_hash_sent_lex_unigram_dict():\n",
    "    hash_sent_lex_unigrams = dict()\n",
    "    with open(hash_sent_lex_unigrams_file_path) as hash_sent_lex_file:\n",
    "        for line in hash_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            if clean_str(word_array[0]):\n",
    "                hash_sent_lex_unigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return hash_sent_lex_unigrams\n",
    "\n",
    "def get_hash_sent_lex_bigram_dict():\n",
    "    hash_sent_lex_bigrams = dict()\n",
    "    with open(hash_sent_lex_bigrams_file_path) as hash_sent_lex_file:\n",
    "        for line in hash_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            if clean_str(word_array[0]):\n",
    "                hash_sent_lex_bigrams[word_array[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return hash_sent_lex_bigrams\n",
    "\n",
    "def get_hash_sent_lex_pairs_dict():\n",
    "    hash_sent_lex_pairs = dict()\n",
    "    with open(hash_sent_lex_pairs_file_path) as hash_sent_lex_file:\n",
    "        for line in hash_sent_lex_file:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            pair = word_array[0].split(pair_split_string)\n",
    "            token_1 = clean_str(pair[0])\n",
    "            token_2 = clean_str(pair[1])\n",
    "            if token_1 and token_2:\n",
    "                token_1_dict = None\n",
    "                if token_1 in hash_sent_lex_pairs.keys():\n",
    "                    token_1_dict = hash_sent_lex_pairs[token_1]\n",
    "                else:\n",
    "                    token_1_dict = dict()\n",
    "                    \n",
    "                token_1_dict[token_2] = np.array([float(val) for val in word_array[1:]])\n",
    "                hash_sent_lex_pairs[token_1] = token_1_dict\n",
    "    \n",
    "    return hash_sent_lex_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_sent_lex_unigram_dict = get_hash_sent_lex_unigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_sent_lex_bigram_dict = get_hash_sent_lex_bigram_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_sent_lex_pairs_dict = get_hash_sent_lex_pairs_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_hash_sent_lex = PolynomialFeatures(5)\n",
    "\n",
    "def get_unigram_sentiment_hash_sent_lex_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        word = clean_str(token)\n",
    "        if word in hash_sent_lex_unigram_dict.keys():\n",
    "            vector_list += hash_sent_lex_unigram_dict[word]\n",
    "            counter += 1\n",
    "\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    \n",
    "#     return vector_list\n",
    "    return normalize(poly_hash_sent_lex.fit_transform(vector_list.reshape(1, -1))[0].reshape(1, -1))[0]\n",
    "    \n",
    "def get_bigram_sentiment_hash_sent_lex_vector(tokens):\n",
    "    bi_tokens = bigrams(tokens)\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    for bi_token in bi_tokens:\n",
    "        word = clean_str(\" \".join(bi_token))\n",
    "        if word in hash_sent_lex_bigram_dict.keys():\n",
    "            vector_list += hash_sent_lex_bigram_dict[word]\n",
    "            counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "    \n",
    "#     return vector_list\n",
    "    return normalize(poly_hash_sent_lex.fit_transform(vector_list.reshape(1, -1))[0].reshape(1, -1))[0]\n",
    "\n",
    "def get_pair_sentiment_hash_sent_lex_vector(tokens):\n",
    "    vector_list = np.zeros(3)\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        word_1 = clean_str(tokens[i])\n",
    "        if word_1 in hash_sent_lex_pairs_dict.keys():\n",
    "            token_1_dict = hash_sent_lex_pairs_dict[word_1]\n",
    "            for j in range(i, len(tokens)):\n",
    "                word_2 = clean_str(tokens[j])\n",
    "                if word_2 in token_1_dict.keys():\n",
    "                    vector_list += token_1_dict[word_2]\n",
    "                    counter += 1\n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "        \n",
    "#     return vector_list\n",
    "    return normalize(poly_hash_sent_lex.fit_transform(vector_list.reshape(1, -1))[0].reshape(1, -1))[0]\n",
    "    \n",
    "def get_sentiment_hash_sent_lex_vector(tweet):\n",
    "    final_list = np.asarray([])\n",
    "    \n",
    "    tokens = word_tokenize(tweet)\n",
    "    \n",
    "    # Adding unigram features\n",
    "    final_list = np.append(final_list, get_unigram_sentiment_hash_sent_lex_vector(tokens))\n",
    "    # Adding bigram features\n",
    "    final_list = np.append(final_list, get_bigram_sentiment_hash_sent_lex_vector(tokens))\n",
    "    # Adding pair features\n",
    "    final_list = np.append(final_list, get_pair_sentiment_hash_sent_lex_vector(tokens))\n",
    "\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.09887390e-09, 3.39075115e-09, 2.09053830e-07, 1.73361713e-07,\n",
       "       2.25484951e-09, 1.39020797e-07, 1.15285539e-07, 8.57120703e-06,\n",
       "       7.10783022e-06, 5.89429823e-06, 1.49947493e-09, 9.24488300e-08,\n",
       "       7.66648834e-08, 5.69985268e-06, 4.72670710e-06, 3.91970833e-06,\n",
       "       3.51419488e-04, 2.91421039e-04, 2.41666228e-04, 2.00406140e-04,\n",
       "       9.97150826e-10, 6.14784720e-08, 5.09821475e-08, 3.79040203e-06,\n",
       "       3.14326022e-06, 2.60660604e-06, 2.33693960e-04, 1.93794991e-04,\n",
       "       1.60708041e-04, 1.33270083e-04, 1.44081990e-02, 1.19482626e-02,\n",
       "       9.90831533e-03, 8.21665174e-03, 6.81380876e-03, 6.63105299e-10,\n",
       "       4.08831839e-08, 3.39031281e-08, 2.52061735e-06, 2.09026805e-06,\n",
       "       1.73339301e-06, 1.55406483e-04, 1.28873669e-04, 1.06870848e-04,\n",
       "       8.86246052e-05, 9.58145235e-03, 7.94559463e-03, 6.58902970e-03,\n",
       "       5.46407341e-03, 4.53118282e-03, 5.90736160e-01, 4.89878767e-01,\n",
       "       4.06240929e-01, 3.36882721e-01, 2.79366159e-01, 2.31669498e-01,\n",
       "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sentiment_hash_sent_lex_vector(\"furious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10) Depeche Mood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "depeche_mood_file_path = ('%s%s/' %(lexicons_path, paths2[8])) + listdir('%s%s/' %(lexicons_path, paths2[8]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depeche_vector_dict():\n",
    "    depeche_vector_dict = dict()\n",
    "    with open(depeche_mood_file_path) as depeche_mood_file:\n",
    "        lines = depeche_mood_file.readlines()[1:] \n",
    "        for line in lines:\n",
    "            word_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n",
    "            depeche_vector_dict[word_array[0].split(\"#\")[0]] = np.array([float(val) for val in word_array[1:]])\n",
    "    \n",
    "    return depeche_vector_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "depeche_vector_dict = get_depeche_vector_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(depeche_vector_dict[\"0\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_depm = PolynomialFeatures(5)\n",
    "\n",
    "def get_depeche_mood_vector(tweet):\n",
    "    vector_list = np.zeros(8)\n",
    "    tokens = word_tokenize(tweet)\n",
    "    counter = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in depeche_vector_dict.keys():\n",
    "            vector_list += np.array(depeche_vector_dict[token])\n",
    "            counter += 1\n",
    "    \n",
    "    if counter > 0:\n",
    "        vector_list /= counter\n",
    "        \n",
    "    return normalize(poly_depm.fit_transform(vector_list.reshape(1, -1))[0].reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.34469068e-01, 5.98056709e-02, 1.49434297e-01, ...,\n",
       "       1.06644824e-05, 5.70178406e-06, 3.04846875e-06])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_depeche_mood_vector(\"i am so mad about power rangers. i am incensed. i am furious.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (3)  Prepare Sentence Vectors as Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_active_vector_method(string):\n",
    "    return int(string)\n",
    "\n",
    "\n",
    "def vectorize_tweets(tweet_list, bin_string, vector_dict):\n",
    "\n",
    "    vectors = list()\n",
    "    frames = list()\n",
    "\n",
    "    '''Pre-trained Word embeddings'''\n",
    "    index = 0\n",
    "    if is_active_vector_method(bin_string[index]):\n",
    "        if index not in vector_dict.keys():\n",
    "            tmp_vector = \\\n",
    "                pd.DataFrame(list(map(lambda x: get_averaged_embeddings(x, wv_model_g, w2v_dimensions_g), tweet_list)))\n",
    "            vector_dict[index] = tmp_vector\n",
    "        frames.append(vector_dict[index])\n",
    "\n",
    "    index = 1\n",
    "    if is_active_vector_method(bin_string[index]):\n",
    "        if index not in vector_dict.keys():\n",
    "            tmp_vector = \\\n",
    "                pd.DataFrame(list(map(lambda x: get_averaged_embeddings(x, wv_model_w, w2v_dimensions_w), tweet_list)))\n",
    "            vector_dict[index] = tmp_vector\n",
    "        frames.append(vector_dict[index])\n",
    "\n",
    "\n",
    "    '''NRC Emotion Intensity Lexicon'''\n",
    "    index = 2\n",
    "    if is_active_vector_method(bin_string[index]):\n",
    "        if index not in vector_dict.keys():\n",
    "            tmp_vector = pd.DataFrame(list(map(lambda x: get_emo_int_vector(x), tweet_list)))\n",
    "            vector_dict[index] = tmp_vector\n",
    "        frames.append(vector_dict[index])\n",
    "\n",
    "    '''WordNet'''\n",
    "    index = 3\n",
    "    if is_active_vector_method(bin_string[index]):\n",
    "        if index not in vector_dict.keys():\n",
    "            tmp_vector = pd.DataFrame(list(map(lambda x: get_sentiwordnetscore(x), tweet_list)))\n",
    "            vector_dict[index] = tmp_vector\n",
    "        frames.append(vector_dict[index])\n",
    "\n",
    "    '''NRC Sentiment Lexicon'''\n",
    "    index = 4\n",
    "    if is_active_vector_method(bin_string[index]):\n",
    "        if index not in vector_dict.keys():\n",
    "            tmp_vector = pd.DataFrame(list(map(lambda x: get_sentiment_emotion_feature(x), tweet_list)))\n",
    "            vector_dict[index] = tmp_vector\n",
    "        frames.append(vector_dict[index])\n",
    "\n",
    "    index = 5\n",
    "    if is_active_vector_method(bin_string[index]):\n",
    "        if index not in vector_dict.keys():\n",
    "            tmp_vector = pd.DataFrame(list(map(lambda x: get_sentiment_emoticon_lexicon_vector(x), tweet_list)))\n",
    "            vector_dict[index] = tmp_vector\n",
    "        frames.append(vector_dict[index])\n",
    "\n",
    "    index = 6\n",
    "    if is_active_vector_method(bin_string[index]):\n",
    "        if index not in vector_dict.keys():\n",
    "            tmp_vector = pd.DataFrame(list(map(lambda x: get_sentiment_emoticon_afflex_vector(x), tweet_list)))\n",
    "            vector_dict[index] = tmp_vector\n",
    "        frames.append(vector_dict[index])\n",
    "\n",
    "    '''NRC Hashtag Lexicon'''\n",
    "    index = 7\n",
    "    if is_active_vector_method(bin_string[index]):\n",
    "        if index not in vector_dict.keys():\n",
    "            tmp_vector = pd.DataFrame(list(map(lambda x: get_hashtag_emotion_vector(x), tweet_list)))\n",
    "            vector_dict[index] = tmp_vector\n",
    "        frames.append(vector_dict[index])\n",
    "\n",
    "    index = 8\n",
    "    if is_active_vector_method(bin_string[index]):\n",
    "        if index not in vector_dict.keys():\n",
    "            tmp_vector = pd.DataFrame(list(map(lambda x: get_sentiment_hash_sent_lex_vector(x), tweet_list)))\n",
    "            vector_dict[index] = tmp_vector\n",
    "        frames.append(vector_dict[index])\n",
    "\n",
    "    index = 9\n",
    "    if is_active_vector_method(bin_string[index]):\n",
    "        if index not in vector_dict.keys():\n",
    "            tmp_vector = pd.DataFrame(list(map(lambda x: get_sentiment_hashtag_affneglex_vector(x), tweet_list)))\n",
    "            vector_dict[index] = tmp_vector\n",
    "        frames.append(vector_dict[index])\n",
    "\n",
    "   \n",
    "    index = 10\n",
    "    if is_active_vector_method(bin_string[index]):\n",
    "        if index not in vector_dict.keys():\n",
    "            tmp_vector = pd.DataFrame(list(map(lambda x: get_emoji_intensity(x), tweet_list)))\n",
    "            vector_dict[index] = tmp_vector\n",
    "        frames.append(vector_dict[index])\n",
    "        \n",
    "    index = 11\n",
    "    if is_active_vector_method(bin_string[index]):\n",
    "        if index not in vector_dict.keys():\n",
    "            tmp_vector = pd.DataFrame(list(map(lambda x: get_depeche_mood_vector(x), tweet_list)))\n",
    "            vector_dict[index] = tmp_vector\n",
    "        frames.append(vector_dict[index])\n",
    "\n",
    "    vectors = pd.concat(frames, axis=1)\n",
    "\n",
    "    return vectors.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index_mapping = \\\n",
    "    {\n",
    "        0: \"Glove[Twitter]\",\n",
    "        1: \"Word2Vec[Twitter]\",\n",
    "        2: \"NRC-Emotion Intensity Lexicon\",\n",
    "        3: \"Wordnet-Affect\",\n",
    "        4: \"NRC-Emotion-Lexicon\",\n",
    "        5: \"NRC-Emoticon-Lexicon\",\n",
    "        6: \"NRC-Emoticon-AffLexNegLex\",\n",
    "        7: \"NRC-Hashtag-Emotion\",\n",
    "        8: \"NRC-Hashtag-Sentiment-Lexicon\",\n",
    "        9: \"NRC-Hashtag-Sentiment-AffLexNegLex\",\n",
    "        10: \"Emoji Intensity\",\n",
    "        11: \"Depeche Mood\"\n",
    "    \n",
    "    }\n",
    "\n",
    "\n",
    "def get_features_from_identifier(bin_string):\n",
    "    features = list()\n",
    "    for i in range(len(bin_string)):\n",
    "        if int(bin_string[i]):\n",
    "            features.append(feature_index_mapping[i])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Glove[Twitter]',\n",
       " 'Word2Vec[Twitter]',\n",
       " 'NRC-Emotion-Lexicon',\n",
       " 'NRC-Emoticon-Lexicon',\n",
       " 'NRC-Hashtag-Sentiment-Lexicon',\n",
       " 'Emoji Intensity']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string1 = \"110011001010\"\n",
    "get_features_from_identifier(string1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vector_dict = dict()\n",
    "test_vector_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_tweets(train_tweets, string1, train_vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "860\n",
      "943\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "dimension = len(x_train[0])\n",
    "print(dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "score_train = np.array(train_intensities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = vectorize_tweets(test_tweets, string1, test_vector_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "673\n",
      "943\n"
     ]
    }
   ],
   "source": [
    "print(len(x_test))\n",
    "print(len(x_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors_path = \"files/\" + emotion + \"_vectors/train_vectors.npy\"\n",
    "test_vectors_path = \"files/\" + emotion + \"_vectors/test_vectors.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectors_dict\n",
    "import pickle\n",
    "with open(train_vectors_path, 'wb') as train_vectors_file:\n",
    "    pickle.dump(train_vector_dict, train_vectors_file)\n",
    "\n",
    "with open(test_vectors_path, 'wb') as test_vectors_file:\n",
    "    pickle.dump(test_vector_dict, test_vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore vectors_dict\n",
    "import pickle\n",
    "with open(train_vectors_path, 'rb') as train_vectors_file:\n",
    "    train_vector_dict = pickle.load(train_vectors_file)\n",
    "\n",
    "with open(test_vectors_path, 'rb') as test_vectors_file:\n",
    "    test_vector_dict = pickle.load(test_vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vectors_path = \"files/\" + emotion + \"_vectors/x_train.npy\"\n",
    "y_train_vectors_path = \"files/\" + emotion + \"_vectors/y_train.npy\"\n",
    "\n",
    "x_test_vectors_path = \"files/\" + emotion + \"_vectors/x_test.npy\"\n",
    "y_test_vectors_path = \"files/\" + emotion + \"_vectors/y_test.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train_vectors\n",
    "import pickle\n",
    "with open(x_train_vectors_path, 'wb') as x_train_vectors_file:\n",
    "    x_train = pickle.dump(x_train, x_train_vectors_file)\n",
    "with open(y_train_vectors_path, 'wb') as y_train_vectors_file:\n",
    "    score_train = pickle.dump(score_train, y_train_vectors_file)\n",
    "\n",
    "with open(x_test_vectors_path, 'wb') as x_test_vectors_file:\n",
    "    x_test = pickle.dump(x_test, x_test_vectors_file)\n",
    "with open(y_test_vectors_path, 'wb') as y_test_vectors_file:\n",
    "    test_intensities = pickle.dump(test_intensities, y_test_vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore vectors_dict\n",
    "with open(x_train_vectors_path, 'rb') as x_train_vectors_file:\n",
    "    x_train = pickle.load(x_train_vectors_file)\n",
    "with open(y_train_vectors_path, 'rb') as y_train_vectors_file:\n",
    "    score_train = pickle.load(y_train_vectors_file)\n",
    "\n",
    "with open(x_test_vectors_path, 'rb') as x_test_vectors_file:\n",
    "    x_test = pickle.load(x_test_vectors_file)\n",
    "with open(y_test_vectors_path, 'rb') as y_test_vectors_file:\n",
    "    test_intensities = pickle.load(y_test_vectors_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "score_train = np.array(score_train)\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "test_intensities = np.array(test_intensities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(860, 943) \n",
      " (860,) \n",
      " (673, 943) \n",
      " (673,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,'\\n',score_train.shape,'\\n',x_test.shape, '\\n', test_intensities.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lists(pred, gold):\n",
    "    if len(pred) == len(gold):\n",
    "        gold_scores=gold\n",
    "        pred_scores=pred         \n",
    "        \n",
    "        # lists storing gold and prediction scores where gold score >= 0.5\n",
    "        gold_scores_range_05_1=[]\n",
    "        pred_scores_range_05_1=[]\n",
    "         \n",
    "            \n",
    "        for i in range(len(gold_scores)):\n",
    "            if(gold_scores[i]>=0.5):\n",
    "                gold_scores_range_05_1.append(gold_scores[i])\n",
    "                pred_scores_range_05_1.append(pred_scores[i])\n",
    "                \n",
    "        # return zero correlation if predictions are constant\n",
    "        if np.std(pred_scores)==0 or np.std(gold_scores)==0:\n",
    "            return (0,0,0,0)\n",
    "        \n",
    "        pears_corr=scipy.stats.pearsonr(pred_scores,gold_scores)[0]                                    \n",
    "        spear_corr=scipy.stats.spearmanr(pred_scores,gold_scores)[0]   \n",
    "\n",
    "\n",
    "        pears_corr_range_05_1=scipy.stats.pearsonr(pred_scores_range_05_1,gold_scores_range_05_1)[0]                                    \n",
    "        spear_corr_range_05_1=scipy.stats.spearmanr(pred_scores_range_05_1,gold_scores_range_05_1)[0]           \n",
    "        \n",
    "      \n",
    "        return np.array([pears_corr,spear_corr,pears_corr_range_05_1,spear_corr_range_05_1])\n",
    "    else:\n",
    "        raise ValueError('Predictions and gold data have different number of lines.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "import scipy\n",
    "def pearson_score(ground_truth, predictions):\n",
    "    score = scipy.stats.pearsonr(predictions,ground_truth)[0]\n",
    "    return score\n",
    "PS = make_scorer(pearson_score, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "ml_model = XGBRegressor(objective=\"reg:squarederror\",seed=0)\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\n",
    "     \"max_depth\": range(3, 11),\n",
    "     \"n_estimators\": [100,300,500,700,900,1000,3000]\n",
    " }\n",
    "\n",
    "\n",
    "# run randomized search\n",
    "grid_search = GridSearchCV(ml_model,param_grid=param_dist, cv=5, n_jobs=-1,return_train_score=True)\n",
    "\n",
    "trainingtime = pd.DataFrame(columns = [\"Model\", \"Training Time(Seconds)\"])\n",
    "start_time_XG =time.time()\n",
    "\n",
    "grid_search.fit(x_train, score_train)\n",
    "trainingtime.loc[0] = [\"XGBoost\", round((time.time()-start_time_XG), 2)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.356518236466849"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=3,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "             objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, seed=0, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.992865410840231"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_.score(x_train, score_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.864994</td>\n",
       "      <td>0.127215</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>1.196790e-03</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 100}</td>\n",
       "      <td>-10.113587</td>\n",
       "      <td>-9.621670</td>\n",
       "      <td>-10.879781</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.356518</td>\n",
       "      <td>4.952454</td>\n",
       "      <td>1</td>\n",
       "      <td>0.995585</td>\n",
       "      <td>0.997818</td>\n",
       "      <td>0.997016</td>\n",
       "      <td>0.997080</td>\n",
       "      <td>0.996507</td>\n",
       "      <td>0.996801</td>\n",
       "      <td>0.000738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.597395</td>\n",
       "      <td>0.392581</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>4.884805e-04</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 300}</td>\n",
       "      <td>-10.063228</td>\n",
       "      <td>-9.600327</td>\n",
       "      <td>-10.922902</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.359017</td>\n",
       "      <td>4.963745</td>\n",
       "      <td>2</td>\n",
       "      <td>0.998765</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999414</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.421693</td>\n",
       "      <td>1.740514</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>6.308264e-04</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 500}</td>\n",
       "      <td>-10.063275</td>\n",
       "      <td>-9.600255</td>\n",
       "      <td>-10.922874</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.359037</td>\n",
       "      <td>4.963769</td>\n",
       "      <td>3</td>\n",
       "      <td>0.998765</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999414</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.815070</td>\n",
       "      <td>0.415415</td>\n",
       "      <td>0.010572</td>\n",
       "      <td>4.886361e-04</td>\n",
       "      <td>3</td>\n",
       "      <td>700</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 700}</td>\n",
       "      <td>-10.063307</td>\n",
       "      <td>-9.600226</td>\n",
       "      <td>-10.922861</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.359053</td>\n",
       "      <td>4.963776</td>\n",
       "      <td>4</td>\n",
       "      <td>0.998765</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999414</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.134171</td>\n",
       "      <td>0.329794</td>\n",
       "      <td>0.010572</td>\n",
       "      <td>4.887724e-04</td>\n",
       "      <td>3</td>\n",
       "      <td>900</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 900}</td>\n",
       "      <td>-10.063339</td>\n",
       "      <td>-9.600196</td>\n",
       "      <td>-10.922848</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.359068</td>\n",
       "      <td>4.963784</td>\n",
       "      <td>5</td>\n",
       "      <td>0.998765</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999414</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>48.057080</td>\n",
       "      <td>1.079289</td>\n",
       "      <td>0.011769</td>\n",
       "      <td>1.162990e-03</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 1000}</td>\n",
       "      <td>-10.063355</td>\n",
       "      <td>-9.600181</td>\n",
       "      <td>-10.922841</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.359076</td>\n",
       "      <td>4.963788</td>\n",
       "      <td>6</td>\n",
       "      <td>0.998765</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999414</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>113.346275</td>\n",
       "      <td>1.931267</td>\n",
       "      <td>0.013763</td>\n",
       "      <td>1.162361e-03</td>\n",
       "      <td>3</td>\n",
       "      <td>3000</td>\n",
       "      <td>{'max_depth': 3, 'n_estimators': 3000}</td>\n",
       "      <td>-10.063674</td>\n",
       "      <td>-9.599887</td>\n",
       "      <td>-10.922708</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.359231</td>\n",
       "      <td>4.963862</td>\n",
       "      <td>7</td>\n",
       "      <td>0.998765</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999414</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999068</td>\n",
       "      <td>0.999447</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.747509</td>\n",
       "      <td>0.222275</td>\n",
       "      <td>0.008776</td>\n",
       "      <td>3.989459e-04</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 100}</td>\n",
       "      <td>-10.451384</td>\n",
       "      <td>-9.070590</td>\n",
       "      <td>-11.674358</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.801381</td>\n",
       "      <td>5.383823</td>\n",
       "      <td>8</td>\n",
       "      <td>0.998730</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.999045</td>\n",
       "      <td>0.999422</td>\n",
       "      <td>0.000494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>21.380421</td>\n",
       "      <td>0.366891</td>\n",
       "      <td>0.009576</td>\n",
       "      <td>4.902087e-04</td>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 300}</td>\n",
       "      <td>-10.454265</td>\n",
       "      <td>-9.064789</td>\n",
       "      <td>-11.688308</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.804467</td>\n",
       "      <td>5.386391</td>\n",
       "      <td>14</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999069</td>\n",
       "      <td>0.999448</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27.833165</td>\n",
       "      <td>0.412251</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>3.992081e-04</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 500}</td>\n",
       "      <td>-10.454230</td>\n",
       "      <td>-9.064755</td>\n",
       "      <td>-11.688290</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.804442</td>\n",
       "      <td>5.386368</td>\n",
       "      <td>13</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999069</td>\n",
       "      <td>0.999448</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34.461838</td>\n",
       "      <td>0.424268</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>4.883443e-04</td>\n",
       "      <td>4</td>\n",
       "      <td>700</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 700}</td>\n",
       "      <td>-10.454193</td>\n",
       "      <td>-9.064720</td>\n",
       "      <td>-11.688277</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.804418</td>\n",
       "      <td>5.386344</td>\n",
       "      <td>12</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999069</td>\n",
       "      <td>0.999448</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42.119957</td>\n",
       "      <td>0.963256</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>6.306756e-04</td>\n",
       "      <td>4</td>\n",
       "      <td>900</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 900}</td>\n",
       "      <td>-10.454158</td>\n",
       "      <td>-9.064686</td>\n",
       "      <td>-11.688264</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.804394</td>\n",
       "      <td>5.386321</td>\n",
       "      <td>11</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999069</td>\n",
       "      <td>0.999448</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>45.846591</td>\n",
       "      <td>0.714028</td>\n",
       "      <td>0.011370</td>\n",
       "      <td>1.017271e-03</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 1000}</td>\n",
       "      <td>-10.454139</td>\n",
       "      <td>-9.064668</td>\n",
       "      <td>-11.688257</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.804382</td>\n",
       "      <td>5.386310</td>\n",
       "      <td>10</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999069</td>\n",
       "      <td>0.999448</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>111.316902</td>\n",
       "      <td>2.260121</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>4.883052e-04</td>\n",
       "      <td>4</td>\n",
       "      <td>3000</td>\n",
       "      <td>{'max_depth': 4, 'n_estimators': 3000}</td>\n",
       "      <td>-10.453779</td>\n",
       "      <td>-9.064323</td>\n",
       "      <td>-11.688128</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.804145</td>\n",
       "      <td>5.386079</td>\n",
       "      <td>9</td>\n",
       "      <td>0.998766</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999995</td>\n",
       "      <td>0.999069</td>\n",
       "      <td>0.999448</td>\n",
       "      <td>0.000492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13.222640</td>\n",
       "      <td>0.257796</td>\n",
       "      <td>0.009572</td>\n",
       "      <td>7.994871e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 100}</td>\n",
       "      <td>-10.916802</td>\n",
       "      <td>-9.500330</td>\n",
       "      <td>-11.514391</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.929578</td>\n",
       "      <td>5.391370</td>\n",
       "      <td>21</td>\n",
       "      <td>0.998768</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999070</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19.903970</td>\n",
       "      <td>0.383625</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>3.991606e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 300}</td>\n",
       "      <td>-10.916714</td>\n",
       "      <td>-9.500247</td>\n",
       "      <td>-11.514433</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.929527</td>\n",
       "      <td>5.391321</td>\n",
       "      <td>20</td>\n",
       "      <td>0.998768</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999070</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>26.164029</td>\n",
       "      <td>0.335529</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>3.990652e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 500}</td>\n",
       "      <td>-10.916675</td>\n",
       "      <td>-9.500218</td>\n",
       "      <td>-11.514444</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.929507</td>\n",
       "      <td>5.391294</td>\n",
       "      <td>19</td>\n",
       "      <td>0.998768</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999070</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33.559252</td>\n",
       "      <td>0.736204</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>3.989459e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>700</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 700}</td>\n",
       "      <td>-10.916636</td>\n",
       "      <td>-9.500191</td>\n",
       "      <td>-11.514456</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.929488</td>\n",
       "      <td>5.391266</td>\n",
       "      <td>18</td>\n",
       "      <td>0.998768</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999070</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>39.783805</td>\n",
       "      <td>0.676906</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>3.989697e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>900</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 900}</td>\n",
       "      <td>-10.916597</td>\n",
       "      <td>-9.500163</td>\n",
       "      <td>-11.514467</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.929469</td>\n",
       "      <td>5.391239</td>\n",
       "      <td>17</td>\n",
       "      <td>0.998768</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999070</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>42.428133</td>\n",
       "      <td>0.268503</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>3.988743e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 1000}</td>\n",
       "      <td>-10.916577</td>\n",
       "      <td>-9.500150</td>\n",
       "      <td>-11.514472</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.929459</td>\n",
       "      <td>5.391225</td>\n",
       "      <td>16</td>\n",
       "      <td>0.998768</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999070</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>108.893983</td>\n",
       "      <td>0.716911</td>\n",
       "      <td>0.013164</td>\n",
       "      <td>7.451449e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>3000</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 3000}</td>\n",
       "      <td>-10.916185</td>\n",
       "      <td>-9.499878</td>\n",
       "      <td>-11.514587</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.929267</td>\n",
       "      <td>5.390950</td>\n",
       "      <td>15</td>\n",
       "      <td>0.998768</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999070</td>\n",
       "      <td>0.999449</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>11.310751</td>\n",
       "      <td>0.348538</td>\n",
       "      <td>0.009575</td>\n",
       "      <td>4.886555e-04</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 100}</td>\n",
       "      <td>-10.868872</td>\n",
       "      <td>-9.128751</td>\n",
       "      <td>-12.348676</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.306303</td>\n",
       "      <td>5.765238</td>\n",
       "      <td>29</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>17.799000</td>\n",
       "      <td>0.282292</td>\n",
       "      <td>0.009574</td>\n",
       "      <td>4.888698e-04</td>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 300}</td>\n",
       "      <td>-10.868947</td>\n",
       "      <td>-9.128681</td>\n",
       "      <td>-12.348719</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.306335</td>\n",
       "      <td>5.765296</td>\n",
       "      <td>30</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.472752</td>\n",
       "      <td>0.337900</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>4.887529e-04</td>\n",
       "      <td>6</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 500}</td>\n",
       "      <td>-10.868984</td>\n",
       "      <td>-9.128652</td>\n",
       "      <td>-12.348732</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.306347</td>\n",
       "      <td>5.765329</td>\n",
       "      <td>31</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>31.557006</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>0.010571</td>\n",
       "      <td>7.976890e-04</td>\n",
       "      <td>6</td>\n",
       "      <td>700</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 700}</td>\n",
       "      <td>-10.869022</td>\n",
       "      <td>-9.128623</td>\n",
       "      <td>-12.348744</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.306359</td>\n",
       "      <td>5.765361</td>\n",
       "      <td>32</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>38.554892</td>\n",
       "      <td>0.695143</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>4.882468e-04</td>\n",
       "      <td>6</td>\n",
       "      <td>900</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 900}</td>\n",
       "      <td>-10.869060</td>\n",
       "      <td>-9.128594</td>\n",
       "      <td>-12.348757</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.306370</td>\n",
       "      <td>5.765393</td>\n",
       "      <td>33</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>40.762986</td>\n",
       "      <td>0.499613</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>7.463784e-04</td>\n",
       "      <td>6</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 1000}</td>\n",
       "      <td>-10.869078</td>\n",
       "      <td>-9.128580</td>\n",
       "      <td>-12.348763</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.306376</td>\n",
       "      <td>5.765409</td>\n",
       "      <td>34</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>106.694065</td>\n",
       "      <td>0.347784</td>\n",
       "      <td>0.012965</td>\n",
       "      <td>6.307510e-04</td>\n",
       "      <td>6</td>\n",
       "      <td>3000</td>\n",
       "      <td>{'max_depth': 6, 'n_estimators': 3000}</td>\n",
       "      <td>-10.869454</td>\n",
       "      <td>-9.128290</td>\n",
       "      <td>-12.348887</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.306494</td>\n",
       "      <td>5.765732</td>\n",
       "      <td>35</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.855369</td>\n",
       "      <td>0.222916</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>9.773889e-04</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 100}</td>\n",
       "      <td>-11.220269</td>\n",
       "      <td>-9.769610</td>\n",
       "      <td>-11.432506</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.180087</td>\n",
       "      <td>5.558429</td>\n",
       "      <td>22</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>17.715822</td>\n",
       "      <td>0.210052</td>\n",
       "      <td>0.009176</td>\n",
       "      <td>3.989458e-04</td>\n",
       "      <td>7</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 300}</td>\n",
       "      <td>-11.220199</td>\n",
       "      <td>-9.769530</td>\n",
       "      <td>-11.432559</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.180103</td>\n",
       "      <td>5.558462</td>\n",
       "      <td>23</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>23.941771</td>\n",
       "      <td>0.406897</td>\n",
       "      <td>0.009175</td>\n",
       "      <td>3.987313e-04</td>\n",
       "      <td>7</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 500}</td>\n",
       "      <td>-11.220158</td>\n",
       "      <td>-9.769501</td>\n",
       "      <td>-11.432575</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.180111</td>\n",
       "      <td>5.558473</td>\n",
       "      <td>24</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30.433610</td>\n",
       "      <td>0.512643</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>1.353028e-03</td>\n",
       "      <td>7</td>\n",
       "      <td>700</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 700}</td>\n",
       "      <td>-11.220116</td>\n",
       "      <td>-9.769471</td>\n",
       "      <td>-11.432591</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.180120</td>\n",
       "      <td>5.558483</td>\n",
       "      <td>25</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>37.391204</td>\n",
       "      <td>0.301059</td>\n",
       "      <td>0.010971</td>\n",
       "      <td>2.336015e-07</td>\n",
       "      <td>7</td>\n",
       "      <td>900</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 900}</td>\n",
       "      <td>-11.220074</td>\n",
       "      <td>-9.769442</td>\n",
       "      <td>-11.432606</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.180129</td>\n",
       "      <td>5.558493</td>\n",
       "      <td>26</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>41.206999</td>\n",
       "      <td>0.886727</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>4.887141e-04</td>\n",
       "      <td>7</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 1000}</td>\n",
       "      <td>-11.220054</td>\n",
       "      <td>-9.769427</td>\n",
       "      <td>-11.432614</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.180133</td>\n",
       "      <td>5.558499</td>\n",
       "      <td>27</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>107.558951</td>\n",
       "      <td>1.295740</td>\n",
       "      <td>0.013564</td>\n",
       "      <td>1.017074e-03</td>\n",
       "      <td>7</td>\n",
       "      <td>3000</td>\n",
       "      <td>{'max_depth': 7, 'n_estimators': 3000}</td>\n",
       "      <td>-11.219638</td>\n",
       "      <td>-9.769133</td>\n",
       "      <td>-11.432771</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.180220</td>\n",
       "      <td>5.558603</td>\n",
       "      <td>28</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999416</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999071</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>11.376974</td>\n",
       "      <td>0.796355</td>\n",
       "      <td>0.010572</td>\n",
       "      <td>4.212531e-03</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 100}</td>\n",
       "      <td>-11.981037</td>\n",
       "      <td>-10.348775</td>\n",
       "      <td>-10.982988</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.496729</td>\n",
       "      <td>5.765739</td>\n",
       "      <td>36</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999072</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>17.356982</td>\n",
       "      <td>0.178005</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>9.536743e-08</td>\n",
       "      <td>8</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 300}</td>\n",
       "      <td>-11.980948</td>\n",
       "      <td>-10.348858</td>\n",
       "      <td>-10.983031</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.496775</td>\n",
       "      <td>5.765772</td>\n",
       "      <td>37</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999072</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>24.311184</td>\n",
       "      <td>0.406182</td>\n",
       "      <td>0.009974</td>\n",
       "      <td>1.092928e-03</td>\n",
       "      <td>8</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 500}</td>\n",
       "      <td>-11.980910</td>\n",
       "      <td>-10.348892</td>\n",
       "      <td>-10.983044</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.496795</td>\n",
       "      <td>5.765780</td>\n",
       "      <td>38</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999072</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>31.514719</td>\n",
       "      <td>0.314280</td>\n",
       "      <td>0.009774</td>\n",
       "      <td>3.991843e-04</td>\n",
       "      <td>8</td>\n",
       "      <td>700</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 700}</td>\n",
       "      <td>-11.980872</td>\n",
       "      <td>-10.348926</td>\n",
       "      <td>-10.983057</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.496816</td>\n",
       "      <td>5.765788</td>\n",
       "      <td>39</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999072</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>37.047323</td>\n",
       "      <td>0.440655</td>\n",
       "      <td>0.010173</td>\n",
       "      <td>3.989221e-04</td>\n",
       "      <td>8</td>\n",
       "      <td>900</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 900}</td>\n",
       "      <td>-11.980834</td>\n",
       "      <td>-10.348961</td>\n",
       "      <td>-10.983070</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.496837</td>\n",
       "      <td>5.765797</td>\n",
       "      <td>40</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999072</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40.480542</td>\n",
       "      <td>0.272943</td>\n",
       "      <td>0.010572</td>\n",
       "      <td>4.885972e-04</td>\n",
       "      <td>8</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 1000}</td>\n",
       "      <td>-11.980815</td>\n",
       "      <td>-10.348978</td>\n",
       "      <td>-10.983076</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.496847</td>\n",
       "      <td>5.765801</td>\n",
       "      <td>41</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999072</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>105.418876</td>\n",
       "      <td>0.647961</td>\n",
       "      <td>0.012964</td>\n",
       "      <td>6.321843e-04</td>\n",
       "      <td>8</td>\n",
       "      <td>3000</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 3000}</td>\n",
       "      <td>-11.980432</td>\n",
       "      <td>-10.349321</td>\n",
       "      <td>-10.983205</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.497056</td>\n",
       "      <td>5.765886</td>\n",
       "      <td>42</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999072</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>11.586813</td>\n",
       "      <td>0.282941</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>2.431402e-07</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 100}</td>\n",
       "      <td>-11.592462</td>\n",
       "      <td>-10.930299</td>\n",
       "      <td>-11.986656</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.972212</td>\n",
       "      <td>6.149046</td>\n",
       "      <td>50</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>18.096803</td>\n",
       "      <td>0.352454</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>4.885387e-04</td>\n",
       "      <td>9</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 300}</td>\n",
       "      <td>-11.592541</td>\n",
       "      <td>-10.930375</td>\n",
       "      <td>-11.986613</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.972214</td>\n",
       "      <td>6.148998</td>\n",
       "      <td>51</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>24.624745</td>\n",
       "      <td>0.332334</td>\n",
       "      <td>0.009375</td>\n",
       "      <td>4.888308e-04</td>\n",
       "      <td>9</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 500}</td>\n",
       "      <td>-11.592578</td>\n",
       "      <td>-10.930405</td>\n",
       "      <td>-11.986599</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.972218</td>\n",
       "      <td>6.148970</td>\n",
       "      <td>52</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>31.405611</td>\n",
       "      <td>0.424133</td>\n",
       "      <td>0.009974</td>\n",
       "      <td>2.611745e-07</td>\n",
       "      <td>9</td>\n",
       "      <td>700</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 700}</td>\n",
       "      <td>-11.592615</td>\n",
       "      <td>-10.930435</td>\n",
       "      <td>-11.986584</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.972221</td>\n",
       "      <td>6.148943</td>\n",
       "      <td>53</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>37.716533</td>\n",
       "      <td>0.350809</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>5.917394e-07</td>\n",
       "      <td>9</td>\n",
       "      <td>900</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 900}</td>\n",
       "      <td>-11.592652</td>\n",
       "      <td>-10.930465</td>\n",
       "      <td>-11.986570</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.972225</td>\n",
       "      <td>6.148915</td>\n",
       "      <td>54</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>40.787920</td>\n",
       "      <td>0.454218</td>\n",
       "      <td>0.010174</td>\n",
       "      <td>3.987810e-04</td>\n",
       "      <td>9</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 1000}</td>\n",
       "      <td>-11.592670</td>\n",
       "      <td>-10.930480</td>\n",
       "      <td>-11.986563</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.972227</td>\n",
       "      <td>6.148901</td>\n",
       "      <td>55</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>106.513947</td>\n",
       "      <td>0.468144</td>\n",
       "      <td>0.015359</td>\n",
       "      <td>3.816267e-03</td>\n",
       "      <td>9</td>\n",
       "      <td>3000</td>\n",
       "      <td>{'max_depth': 9, 'n_estimators': 3000}</td>\n",
       "      <td>-11.593042</td>\n",
       "      <td>-10.930782</td>\n",
       "      <td>-11.986420</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.972264</td>\n",
       "      <td>6.148626</td>\n",
       "      <td>56</td>\n",
       "      <td>0.998769</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>12.129962</td>\n",
       "      <td>0.228564</td>\n",
       "      <td>0.008775</td>\n",
       "      <td>3.983313e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>-11.872853</td>\n",
       "      <td>-10.291972</td>\n",
       "      <td>-12.837837</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.908319</td>\n",
       "      <td>5.905335</td>\n",
       "      <td>43</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>18.639951</td>\n",
       "      <td>0.205053</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>6.308264e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 300}</td>\n",
       "      <td>-11.872927</td>\n",
       "      <td>-10.292053</td>\n",
       "      <td>-12.837884</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.908397</td>\n",
       "      <td>5.905372</td>\n",
       "      <td>44</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>24.988772</td>\n",
       "      <td>0.218472</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>2.631180e-03</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 500}</td>\n",
       "      <td>-11.872965</td>\n",
       "      <td>-10.292085</td>\n",
       "      <td>-12.837899</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.908433</td>\n",
       "      <td>5.905381</td>\n",
       "      <td>45</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>31.922431</td>\n",
       "      <td>0.128023</td>\n",
       "      <td>0.010769</td>\n",
       "      <td>1.597003e-03</td>\n",
       "      <td>10</td>\n",
       "      <td>700</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 700}</td>\n",
       "      <td>-11.873002</td>\n",
       "      <td>-10.292117</td>\n",
       "      <td>-12.837915</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.908468</td>\n",
       "      <td>5.905389</td>\n",
       "      <td>46</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>40.136861</td>\n",
       "      <td>0.289191</td>\n",
       "      <td>0.010572</td>\n",
       "      <td>4.886361e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>900</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 900}</td>\n",
       "      <td>-11.873040</td>\n",
       "      <td>-10.292149</td>\n",
       "      <td>-12.837930</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.908503</td>\n",
       "      <td>5.905397</td>\n",
       "      <td>47</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>40.563918</td>\n",
       "      <td>1.009600</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>1.492648e-03</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 1000}</td>\n",
       "      <td>-11.873058</td>\n",
       "      <td>-10.292165</td>\n",
       "      <td>-12.837938</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.908521</td>\n",
       "      <td>5.905402</td>\n",
       "      <td>48</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>67.048889</td>\n",
       "      <td>3.053438</td>\n",
       "      <td>0.007779</td>\n",
       "      <td>3.989459e-04</td>\n",
       "      <td>10</td>\n",
       "      <td>3000</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 3000}</td>\n",
       "      <td>-11.873433</td>\n",
       "      <td>-10.292484</td>\n",
       "      <td>-12.838093</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.908874</td>\n",
       "      <td>5.905485</td>\n",
       "      <td>49</td>\n",
       "      <td>0.998770</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>0.999073</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       12.864994      0.127215         0.011370    1.196790e-03   \n",
       "1       27.597395      0.392581         0.010372    4.884805e-04   \n",
       "2       32.421693      1.740514         0.009973    6.308264e-04   \n",
       "3       37.815070      0.415415         0.010572    4.886361e-04   \n",
       "4       44.134171      0.329794         0.010572    4.887724e-04   \n",
       "5       48.057080      1.079289         0.011769    1.162990e-03   \n",
       "6      113.346275      1.931267         0.013763    1.162361e-03   \n",
       "7       12.747509      0.222275         0.008776    3.989459e-04   \n",
       "8       21.380421      0.366891         0.009576    4.902087e-04   \n",
       "9       27.833165      0.412251         0.009774    3.992081e-04   \n",
       "10      34.461838      0.424268         0.010372    4.883443e-04   \n",
       "11      42.119957      0.963256         0.010971    6.306756e-04   \n",
       "12      45.846591      0.714028         0.011370    1.017271e-03   \n",
       "13     111.316902      2.260121         0.013564    4.883052e-04   \n",
       "14      13.222640      0.257796         0.009572    7.994871e-04   \n",
       "15      19.903970      0.383625         0.009175    3.991606e-04   \n",
       "16      26.164029      0.335529         0.010173    3.990652e-04   \n",
       "17      33.559252      0.736204         0.010173    3.989459e-04   \n",
       "18      39.783805      0.676906         0.010173    3.989697e-04   \n",
       "19      42.428133      0.268503         0.010771    3.988743e-04   \n",
       "20     108.893983      0.716911         0.013164    7.451449e-04   \n",
       "21      11.310751      0.348538         0.009575    4.886555e-04   \n",
       "22      17.799000      0.282292         0.009574    4.888698e-04   \n",
       "23      24.472752      0.337900         0.009375    4.887529e-04   \n",
       "24      31.557006      0.339300         0.010571    7.976890e-04   \n",
       "25      38.554892      0.695143         0.010372    4.882468e-04   \n",
       "26      40.762986      0.499613         0.010771    7.463784e-04   \n",
       "27     106.694065      0.347784         0.012965    6.307510e-04   \n",
       "28      10.855369      0.222916         0.009175    9.773889e-04   \n",
       "29      17.715822      0.210052         0.009176    3.989458e-04   \n",
       "30      23.941771      0.406897         0.009175    3.987313e-04   \n",
       "31      30.433610      0.512643         0.010372    1.353028e-03   \n",
       "32      37.391204      0.301059         0.010971    2.336015e-07   \n",
       "33      41.206999      0.886727         0.010372    4.887141e-04   \n",
       "34     107.558951      1.295740         0.013564    1.017074e-03   \n",
       "35      11.376974      0.796355         0.010572    4.212531e-03   \n",
       "36      17.356982      0.178005         0.008976    9.536743e-08   \n",
       "37      24.311184      0.406182         0.009974    1.092928e-03   \n",
       "38      31.514719      0.314280         0.009774    3.991843e-04   \n",
       "39      37.047323      0.440655         0.010173    3.989221e-04   \n",
       "40      40.480542      0.272943         0.010572    4.885972e-04   \n",
       "41     105.418876      0.647961         0.012964    6.321843e-04   \n",
       "42      11.586813      0.282941         0.008976    2.431402e-07   \n",
       "43      18.096803      0.352454         0.009375    4.885387e-04   \n",
       "44      24.624745      0.332334         0.009375    4.888308e-04   \n",
       "45      31.405611      0.424133         0.009974    2.611745e-07   \n",
       "46      37.716533      0.350809         0.009973    5.917394e-07   \n",
       "47      40.787920      0.454218         0.010174    3.987810e-04   \n",
       "48     106.513947      0.468144         0.015359    3.816267e-03   \n",
       "49      12.129962      0.228564         0.008775    3.983313e-04   \n",
       "50      18.639951      0.205053         0.009973    6.308264e-04   \n",
       "51      24.988772      0.218472         0.010771    2.631180e-03   \n",
       "52      31.922431      0.128023         0.010769    1.597003e-03   \n",
       "53      40.136861      0.289191         0.010572    4.886361e-04   \n",
       "54      40.563918      1.009600         0.007380    1.492648e-03   \n",
       "55      67.048889      3.053438         0.007779    3.989459e-04   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "0                3                100   \n",
       "1                3                300   \n",
       "2                3                500   \n",
       "3                3                700   \n",
       "4                3                900   \n",
       "5                3               1000   \n",
       "6                3               3000   \n",
       "7                4                100   \n",
       "8                4                300   \n",
       "9                4                500   \n",
       "10               4                700   \n",
       "11               4                900   \n",
       "12               4               1000   \n",
       "13               4               3000   \n",
       "14               5                100   \n",
       "15               5                300   \n",
       "16               5                500   \n",
       "17               5                700   \n",
       "18               5                900   \n",
       "19               5               1000   \n",
       "20               5               3000   \n",
       "21               6                100   \n",
       "22               6                300   \n",
       "23               6                500   \n",
       "24               6                700   \n",
       "25               6                900   \n",
       "26               6               1000   \n",
       "27               6               3000   \n",
       "28               7                100   \n",
       "29               7                300   \n",
       "30               7                500   \n",
       "31               7                700   \n",
       "32               7                900   \n",
       "33               7               1000   \n",
       "34               7               3000   \n",
       "35               8                100   \n",
       "36               8                300   \n",
       "37               8                500   \n",
       "38               8                700   \n",
       "39               8                900   \n",
       "40               8               1000   \n",
       "41               8               3000   \n",
       "42               9                100   \n",
       "43               9                300   \n",
       "44               9                500   \n",
       "45               9                700   \n",
       "46               9                900   \n",
       "47               9               1000   \n",
       "48               9               3000   \n",
       "49              10                100   \n",
       "50              10                300   \n",
       "51              10                500   \n",
       "52              10                700   \n",
       "53              10                900   \n",
       "54              10               1000   \n",
       "55              10               3000   \n",
       "\n",
       "                                     params  split0_test_score  \\\n",
       "0     {'max_depth': 3, 'n_estimators': 100}         -10.113587   \n",
       "1     {'max_depth': 3, 'n_estimators': 300}         -10.063228   \n",
       "2     {'max_depth': 3, 'n_estimators': 500}         -10.063275   \n",
       "3     {'max_depth': 3, 'n_estimators': 700}         -10.063307   \n",
       "4     {'max_depth': 3, 'n_estimators': 900}         -10.063339   \n",
       "5    {'max_depth': 3, 'n_estimators': 1000}         -10.063355   \n",
       "6    {'max_depth': 3, 'n_estimators': 3000}         -10.063674   \n",
       "7     {'max_depth': 4, 'n_estimators': 100}         -10.451384   \n",
       "8     {'max_depth': 4, 'n_estimators': 300}         -10.454265   \n",
       "9     {'max_depth': 4, 'n_estimators': 500}         -10.454230   \n",
       "10    {'max_depth': 4, 'n_estimators': 700}         -10.454193   \n",
       "11    {'max_depth': 4, 'n_estimators': 900}         -10.454158   \n",
       "12   {'max_depth': 4, 'n_estimators': 1000}         -10.454139   \n",
       "13   {'max_depth': 4, 'n_estimators': 3000}         -10.453779   \n",
       "14    {'max_depth': 5, 'n_estimators': 100}         -10.916802   \n",
       "15    {'max_depth': 5, 'n_estimators': 300}         -10.916714   \n",
       "16    {'max_depth': 5, 'n_estimators': 500}         -10.916675   \n",
       "17    {'max_depth': 5, 'n_estimators': 700}         -10.916636   \n",
       "18    {'max_depth': 5, 'n_estimators': 900}         -10.916597   \n",
       "19   {'max_depth': 5, 'n_estimators': 1000}         -10.916577   \n",
       "20   {'max_depth': 5, 'n_estimators': 3000}         -10.916185   \n",
       "21    {'max_depth': 6, 'n_estimators': 100}         -10.868872   \n",
       "22    {'max_depth': 6, 'n_estimators': 300}         -10.868947   \n",
       "23    {'max_depth': 6, 'n_estimators': 500}         -10.868984   \n",
       "24    {'max_depth': 6, 'n_estimators': 700}         -10.869022   \n",
       "25    {'max_depth': 6, 'n_estimators': 900}         -10.869060   \n",
       "26   {'max_depth': 6, 'n_estimators': 1000}         -10.869078   \n",
       "27   {'max_depth': 6, 'n_estimators': 3000}         -10.869454   \n",
       "28    {'max_depth': 7, 'n_estimators': 100}         -11.220269   \n",
       "29    {'max_depth': 7, 'n_estimators': 300}         -11.220199   \n",
       "30    {'max_depth': 7, 'n_estimators': 500}         -11.220158   \n",
       "31    {'max_depth': 7, 'n_estimators': 700}         -11.220116   \n",
       "32    {'max_depth': 7, 'n_estimators': 900}         -11.220074   \n",
       "33   {'max_depth': 7, 'n_estimators': 1000}         -11.220054   \n",
       "34   {'max_depth': 7, 'n_estimators': 3000}         -11.219638   \n",
       "35    {'max_depth': 8, 'n_estimators': 100}         -11.981037   \n",
       "36    {'max_depth': 8, 'n_estimators': 300}         -11.980948   \n",
       "37    {'max_depth': 8, 'n_estimators': 500}         -11.980910   \n",
       "38    {'max_depth': 8, 'n_estimators': 700}         -11.980872   \n",
       "39    {'max_depth': 8, 'n_estimators': 900}         -11.980834   \n",
       "40   {'max_depth': 8, 'n_estimators': 1000}         -11.980815   \n",
       "41   {'max_depth': 8, 'n_estimators': 3000}         -11.980432   \n",
       "42    {'max_depth': 9, 'n_estimators': 100}         -11.592462   \n",
       "43    {'max_depth': 9, 'n_estimators': 300}         -11.592541   \n",
       "44    {'max_depth': 9, 'n_estimators': 500}         -11.592578   \n",
       "45    {'max_depth': 9, 'n_estimators': 700}         -11.592615   \n",
       "46    {'max_depth': 9, 'n_estimators': 900}         -11.592652   \n",
       "47   {'max_depth': 9, 'n_estimators': 1000}         -11.592670   \n",
       "48   {'max_depth': 9, 'n_estimators': 3000}         -11.593042   \n",
       "49   {'max_depth': 10, 'n_estimators': 100}         -11.872853   \n",
       "50   {'max_depth': 10, 'n_estimators': 300}         -11.872927   \n",
       "51   {'max_depth': 10, 'n_estimators': 500}         -11.872965   \n",
       "52   {'max_depth': 10, 'n_estimators': 700}         -11.873002   \n",
       "53   {'max_depth': 10, 'n_estimators': 900}         -11.873040   \n",
       "54  {'max_depth': 10, 'n_estimators': 1000}         -11.873058   \n",
       "55  {'max_depth': 10, 'n_estimators': 3000}         -11.873433   \n",
       "\n",
       "    split1_test_score  split2_test_score  ...  mean_test_score  \\\n",
       "0           -9.621670         -10.879781  ...        -9.356518   \n",
       "1           -9.600327         -10.922902  ...        -9.359017   \n",
       "2           -9.600255         -10.922874  ...        -9.359037   \n",
       "3           -9.600226         -10.922861  ...        -9.359053   \n",
       "4           -9.600196         -10.922848  ...        -9.359068   \n",
       "5           -9.600181         -10.922841  ...        -9.359076   \n",
       "6           -9.599887         -10.922708  ...        -9.359231   \n",
       "7           -9.070590         -11.674358  ...        -9.801381   \n",
       "8           -9.064789         -11.688308  ...        -9.804467   \n",
       "9           -9.064755         -11.688290  ...        -9.804442   \n",
       "10          -9.064720         -11.688277  ...        -9.804418   \n",
       "11          -9.064686         -11.688264  ...        -9.804394   \n",
       "12          -9.064668         -11.688257  ...        -9.804382   \n",
       "13          -9.064323         -11.688128  ...        -9.804145   \n",
       "14          -9.500330         -11.514391  ...        -9.929578   \n",
       "15          -9.500247         -11.514433  ...        -9.929527   \n",
       "16          -9.500218         -11.514444  ...        -9.929507   \n",
       "17          -9.500191         -11.514456  ...        -9.929488   \n",
       "18          -9.500163         -11.514467  ...        -9.929469   \n",
       "19          -9.500150         -11.514472  ...        -9.929459   \n",
       "20          -9.499878         -11.514587  ...        -9.929267   \n",
       "21          -9.128751         -12.348676  ...       -10.306303   \n",
       "22          -9.128681         -12.348719  ...       -10.306335   \n",
       "23          -9.128652         -12.348732  ...       -10.306347   \n",
       "24          -9.128623         -12.348744  ...       -10.306359   \n",
       "25          -9.128594         -12.348757  ...       -10.306370   \n",
       "26          -9.128580         -12.348763  ...       -10.306376   \n",
       "27          -9.128290         -12.348887  ...       -10.306494   \n",
       "28          -9.769610         -11.432506  ...       -10.180087   \n",
       "29          -9.769530         -11.432559  ...       -10.180103   \n",
       "30          -9.769501         -11.432575  ...       -10.180111   \n",
       "31          -9.769471         -11.432591  ...       -10.180120   \n",
       "32          -9.769442         -11.432606  ...       -10.180129   \n",
       "33          -9.769427         -11.432614  ...       -10.180133   \n",
       "34          -9.769133         -11.432771  ...       -10.180220   \n",
       "35         -10.348775         -10.982988  ...       -10.496729   \n",
       "36         -10.348858         -10.983031  ...       -10.496775   \n",
       "37         -10.348892         -10.983044  ...       -10.496795   \n",
       "38         -10.348926         -10.983057  ...       -10.496816   \n",
       "39         -10.348961         -10.983070  ...       -10.496837   \n",
       "40         -10.348978         -10.983076  ...       -10.496847   \n",
       "41         -10.349321         -10.983205  ...       -10.497056   \n",
       "42         -10.930299         -11.986656  ...       -10.972212   \n",
       "43         -10.930375         -11.986613  ...       -10.972214   \n",
       "44         -10.930405         -11.986599  ...       -10.972218   \n",
       "45         -10.930435         -11.986584  ...       -10.972221   \n",
       "46         -10.930465         -11.986570  ...       -10.972225   \n",
       "47         -10.930480         -11.986563  ...       -10.972227   \n",
       "48         -10.930782         -11.986420  ...       -10.972264   \n",
       "49         -10.291972         -12.837837  ...       -10.908319   \n",
       "50         -10.292053         -12.837884  ...       -10.908397   \n",
       "51         -10.292085         -12.837899  ...       -10.908433   \n",
       "52         -10.292117         -12.837915  ...       -10.908468   \n",
       "53         -10.292149         -12.837930  ...       -10.908503   \n",
       "54         -10.292165         -12.837938  ...       -10.908521   \n",
       "55         -10.292484         -12.838093  ...       -10.908874   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0         4.952454                1            0.995585            0.997818   \n",
       "1         4.963745                2            0.998765            0.999995   \n",
       "2         4.963769                3            0.998765            0.999995   \n",
       "3         4.963776                4            0.998765            0.999995   \n",
       "4         4.963784                5            0.998765            0.999995   \n",
       "5         4.963788                6            0.998765            0.999995   \n",
       "6         4.963862                7            0.998765            0.999995   \n",
       "7         5.383823                8            0.998730            0.999976   \n",
       "8         5.386391               14            0.998766            0.999996   \n",
       "9         5.386368               13            0.998766            0.999996   \n",
       "10        5.386344               12            0.998766            0.999996   \n",
       "11        5.386321               11            0.998766            0.999996   \n",
       "12        5.386310               10            0.998766            0.999996   \n",
       "13        5.386079                9            0.998766            0.999996   \n",
       "14        5.391370               21            0.998768            0.999996   \n",
       "15        5.391321               20            0.998768            0.999996   \n",
       "16        5.391294               19            0.998768            0.999996   \n",
       "17        5.391266               18            0.998768            0.999996   \n",
       "18        5.391239               17            0.998768            0.999996   \n",
       "19        5.391225               16            0.998768            0.999996   \n",
       "20        5.390950               15            0.998768            0.999996   \n",
       "21        5.765238               29            0.998769            0.999997   \n",
       "22        5.765296               30            0.998769            0.999997   \n",
       "23        5.765329               31            0.998769            0.999997   \n",
       "24        5.765361               32            0.998769            0.999997   \n",
       "25        5.765393               33            0.998769            0.999997   \n",
       "26        5.765409               34            0.998769            0.999997   \n",
       "27        5.765732               35            0.998769            0.999997   \n",
       "28        5.558429               22            0.998769            0.999997   \n",
       "29        5.558462               23            0.998769            0.999997   \n",
       "30        5.558473               24            0.998769            0.999997   \n",
       "31        5.558483               25            0.998769            0.999997   \n",
       "32        5.558493               26            0.998769            0.999997   \n",
       "33        5.558499               27            0.998769            0.999997   \n",
       "34        5.558603               28            0.998769            0.999997   \n",
       "35        5.765739               36            0.998769            0.999998   \n",
       "36        5.765772               37            0.998769            0.999998   \n",
       "37        5.765780               38            0.998769            0.999998   \n",
       "38        5.765788               39            0.998769            0.999998   \n",
       "39        5.765797               40            0.998769            0.999998   \n",
       "40        5.765801               41            0.998769            0.999998   \n",
       "41        5.765886               42            0.998769            0.999998   \n",
       "42        6.149046               50            0.998769            0.999998   \n",
       "43        6.148998               51            0.998769            0.999998   \n",
       "44        6.148970               52            0.998769            0.999998   \n",
       "45        6.148943               53            0.998769            0.999998   \n",
       "46        6.148915               54            0.998769            0.999998   \n",
       "47        6.148901               55            0.998769            0.999998   \n",
       "48        6.148626               56            0.998769            0.999998   \n",
       "49        5.905335               43            0.998770            0.999998   \n",
       "50        5.905372               44            0.998770            0.999998   \n",
       "51        5.905381               45            0.998770            0.999998   \n",
       "52        5.905389               46            0.998770            0.999998   \n",
       "53        5.905397               47            0.998770            0.999998   \n",
       "54        5.905402               48            0.998770            0.999998   \n",
       "55        5.905485               49            0.998770            0.999998   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "0             0.997016            0.997080            0.996507   \n",
       "1             0.999414            0.999995            0.999068   \n",
       "2             0.999414            0.999995            0.999068   \n",
       "3             0.999414            0.999995            0.999068   \n",
       "4             0.999414            0.999995            0.999068   \n",
       "5             0.999414            0.999995            0.999068   \n",
       "6             0.999414            0.999995            0.999068   \n",
       "7             0.999394            0.999963            0.999045   \n",
       "8             0.999416            0.999995            0.999069   \n",
       "9             0.999416            0.999995            0.999069   \n",
       "10            0.999416            0.999995            0.999069   \n",
       "11            0.999416            0.999995            0.999069   \n",
       "12            0.999416            0.999995            0.999069   \n",
       "13            0.999416            0.999995            0.999069   \n",
       "14            0.999416            0.999996            0.999070   \n",
       "15            0.999416            0.999996            0.999070   \n",
       "16            0.999416            0.999996            0.999070   \n",
       "17            0.999416            0.999996            0.999070   \n",
       "18            0.999416            0.999996            0.999070   \n",
       "19            0.999416            0.999996            0.999070   \n",
       "20            0.999416            0.999996            0.999070   \n",
       "21            0.999416            0.999996            0.999071   \n",
       "22            0.999416            0.999996            0.999071   \n",
       "23            0.999416            0.999996            0.999071   \n",
       "24            0.999416            0.999996            0.999071   \n",
       "25            0.999416            0.999996            0.999071   \n",
       "26            0.999416            0.999996            0.999071   \n",
       "27            0.999416            0.999996            0.999071   \n",
       "28            0.999416            0.999997            0.999071   \n",
       "29            0.999416            0.999997            0.999071   \n",
       "30            0.999416            0.999997            0.999071   \n",
       "31            0.999416            0.999997            0.999071   \n",
       "32            0.999416            0.999997            0.999071   \n",
       "33            0.999416            0.999997            0.999071   \n",
       "34            0.999416            0.999997            0.999071   \n",
       "35            0.999417            0.999997            0.999072   \n",
       "36            0.999417            0.999997            0.999072   \n",
       "37            0.999417            0.999997            0.999072   \n",
       "38            0.999417            0.999997            0.999072   \n",
       "39            0.999417            0.999997            0.999072   \n",
       "40            0.999417            0.999997            0.999072   \n",
       "41            0.999417            0.999997            0.999072   \n",
       "42            0.999417            0.999997            0.999073   \n",
       "43            0.999417            0.999997            0.999073   \n",
       "44            0.999417            0.999997            0.999073   \n",
       "45            0.999417            0.999997            0.999073   \n",
       "46            0.999417            0.999997            0.999073   \n",
       "47            0.999417            0.999997            0.999073   \n",
       "48            0.999417            0.999997            0.999073   \n",
       "49            0.999417            0.999997            0.999073   \n",
       "50            0.999417            0.999997            0.999073   \n",
       "51            0.999417            0.999997            0.999073   \n",
       "52            0.999417            0.999997            0.999073   \n",
       "53            0.999417            0.999997            0.999073   \n",
       "54            0.999417            0.999997            0.999073   \n",
       "55            0.999417            0.999997            0.999073   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "0           0.996801         0.000738  \n",
       "1           0.999447         0.000492  \n",
       "2           0.999447         0.000492  \n",
       "3           0.999447         0.000492  \n",
       "4           0.999447         0.000492  \n",
       "5           0.999447         0.000492  \n",
       "6           0.999447         0.000492  \n",
       "7           0.999422         0.000494  \n",
       "8           0.999448         0.000492  \n",
       "9           0.999448         0.000492  \n",
       "10          0.999448         0.000492  \n",
       "11          0.999448         0.000492  \n",
       "12          0.999448         0.000492  \n",
       "13          0.999448         0.000492  \n",
       "14          0.999449         0.000491  \n",
       "15          0.999449         0.000491  \n",
       "16          0.999449         0.000491  \n",
       "17          0.999449         0.000491  \n",
       "18          0.999449         0.000491  \n",
       "19          0.999449         0.000491  \n",
       "20          0.999449         0.000491  \n",
       "21          0.999450         0.000491  \n",
       "22          0.999450         0.000491  \n",
       "23          0.999450         0.000491  \n",
       "24          0.999450         0.000491  \n",
       "25          0.999450         0.000491  \n",
       "26          0.999450         0.000491  \n",
       "27          0.999450         0.000491  \n",
       "28          0.999450         0.000491  \n",
       "29          0.999450         0.000491  \n",
       "30          0.999450         0.000491  \n",
       "31          0.999450         0.000491  \n",
       "32          0.999450         0.000491  \n",
       "33          0.999450         0.000491  \n",
       "34          0.999450         0.000491  \n",
       "35          0.999450         0.000491  \n",
       "36          0.999450         0.000491  \n",
       "37          0.999450         0.000491  \n",
       "38          0.999450         0.000491  \n",
       "39          0.999450         0.000491  \n",
       "40          0.999450         0.000491  \n",
       "41          0.999450         0.000491  \n",
       "42          0.999451         0.000491  \n",
       "43          0.999451         0.000491  \n",
       "44          0.999451         0.000491  \n",
       "45          0.999451         0.000491  \n",
       "46          0.999451         0.000491  \n",
       "47          0.999451         0.000491  \n",
       "48          0.999451         0.000491  \n",
       "49          0.999451         0.000491  \n",
       "50          0.999451         0.000491  \n",
       "51          0.999451         0.000491  \n",
       "52          0.999451         0.000491  \n",
       "53          0.999451         0.000491  \n",
       "54          0.999451         0.000491  \n",
       "55          0.999451         0.000491  \n",
       "\n",
       "[56 rows x 22 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 3, 'n_estimators': 100}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.300000012, max_delta_step=0, max_depth=3,\n",
       "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n",
       "             objective='reg:squarederror', random_state=0, reg_alpha=0,\n",
       "             reg_lambda=1, scale_pos_weight=1, seed=0, subsample=1,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_model_best = grid_search.best_estimator_\n",
    "\n",
    "ml_model_best.fit(x_train, score_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "def plot_learningcurve(classifier, X, y, plt_titile):\n",
    "    # check whether there is overfitting or underfitting by learning_curve\n",
    "    # choose five kinds of fraction of the maximum size of the training set: np.linspace(0.1,1.0,5)\n",
    "    train_size, train_score, test_score = learning_curve(classifier, X, y, cv=5, train_sizes=np.linspace(0.1,1.0,5))\n",
    "    train_scores_mean = np.mean(train_score, axis=1)\n",
    "    train_scores_std = np.std(train_score, axis=1)\n",
    "    test_scores_mean = np.mean(test_score, axis=1)\n",
    "    test_scores_std = np.std(test_score, axis=1)\n",
    "    plt.fill_between(train_size, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_size, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_size, train_scores_mean,'o--', color=\"r\",label=\"Training score\")\n",
    "    plt.plot(train_size, test_scores_mean,'o-', color=\"g\",label=\"Testing score\")\n",
    "    plt.grid()\n",
    "    plt.title(plt_titile)\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learningcurve(grid_search.best_estimator_, x_train, score_train, 'Learning Curve (XGBoost)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65206723, 0.64606403, 0.50504014, 0.49214106])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ml_model_best.predict(x_test)\n",
    "\n",
    "score1 = evaluate_lists(y_pred, test_intensities)\n",
    "score1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_path = \"files/final_models/\" + \"xgboost_\"+ emotion + \".dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(xgboost_path, 'wb') as xgboost_file:\n",
    "    pickle.dump(grid_search.best_estimator_, xgboost_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = pickle.load(open(xgboost_path,'rb'))\n",
    "#model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Feedfoward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self,n_feature,n_hidden,n_output):\n",
    "        super(LinearModel,self).__init__() \n",
    "        self.hidden = torch.nn.Linear(n_feature,n_hidden)\n",
    "        self.hidden.weight = torch.nn.init.xavier_normal(self.hidden.weight)\n",
    "        self.dropout = torch.nn.Dropout(p=0.25)\n",
    "        self.predict = torch.nn.Linear(n_hidden,n_output)\n",
    "    def forward(self,x):\n",
    "        out=F.relu(self.hidden(x))\n",
    "        out=self.dropout(out)\n",
    "        out=F.sigmoid(self.predict(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0574,  0.2209, -0.0529,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1568,  0.2660,  0.3558,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1148,  0.0724, -0.0264,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.1228, -0.1261, -0.0813,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.1156,  0.0796, -0.0345,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1523, -0.2883, -0.1184,  ...,  0.0000,  0.0000,  0.0000]]) \n",
      " tensor([0.6250, 0.9580, 0.2080, 0.2710, 0.4170, 0.4380, 0.6250, 0.6250, 0.7710,\n",
      "        0.8060, 0.4170, 0.9170, 0.3000, 0.4580, 0.5740, 0.2290, 0.7080, 0.6670,\n",
      "        0.6460, 0.2710, 0.3120, 0.4580, 0.2710, 0.6880, 0.6250, 0.5830, 0.4380,\n",
      "        0.6250, 0.2920, 0.5000, 0.3120, 0.3810, 0.3330, 0.6250, 0.3330, 0.3330,\n",
      "        0.5210, 0.5830, 0.4580, 0.6890, 0.4380, 0.6880, 0.8750, 0.4170, 0.3540,\n",
      "        0.2710, 0.4600, 0.2500, 0.3960, 0.6250, 0.8100, 0.7500, 0.3330, 0.2710,\n",
      "        0.5210, 0.6250, 0.4380, 0.8920, 0.3120, 0.4790, 0.5830, 0.6670, 0.8540,\n",
      "        0.5090, 0.5420, 0.8330, 0.2060, 0.3330, 0.8960, 0.5420, 0.6460, 0.4790,\n",
      "        0.3540, 0.3540, 0.4260, 0.3540, 0.1250, 0.2080, 0.7670, 0.2710, 0.4170,\n",
      "        0.6670, 0.3330, 0.4200, 0.6040, 0.3970, 0.5420, 0.5420, 0.3750, 0.2920,\n",
      "        0.4580, 0.2710, 0.4170, 0.7080, 0.2920, 0.3120, 0.3540, 0.2080, 0.5420,\n",
      "        0.5620, 0.5830, 0.3120, 0.6880, 0.6670, 0.4580, 0.0830, 0.5210, 0.3960,\n",
      "        0.4580, 0.4790, 0.3750, 0.7400, 0.3540, 0.2920, 0.5630, 0.4580, 0.5830,\n",
      "        0.2500, 0.7080, 0.2080, 0.2920, 0.7920, 0.2500, 0.6250, 0.4380, 0.4170,\n",
      "        0.5830, 0.4380])\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as Data\n",
    "batch_size = 128\n",
    "dataset = Data.TensorDataset(torch.tensor(x_train.astype(np.float32)), torch.tensor(score_train.astype(np.float32)))\n",
    "data_iter = Data.DataLoader(dataset = dataset,batch_size = batch_size, shuffle = True, num_workers = 2)\n",
    "\n",
    "for X,y in data_iter:\n",
    "    print(X, '\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearModel(\n",
      "  (hidden): Linear(in_features=943, out_features=10000, bias=True)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (predict): Linear(in_features=10000, out_features=1, bias=True)\n",
      ")\n",
      "<generator object Module.parameters at 0x000001B0635F48E0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\envs\\tf2\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# architecure: 1‚Üí10000‚Üí1\n",
    "net = LinearModel(x_train.shape[1],10000,1)\n",
    "print(net)\n",
    "print(net.parameters())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.0199,  0.0074, -0.0088,  ..., -0.0037, -0.0104, -0.0051],\n",
      "        [ 0.0055,  0.0107, -0.0044,  ..., -0.0032, -0.0141,  0.0091],\n",
      "        [ 0.0195, -0.0118,  0.0171,  ...,  0.0145,  0.0029,  0.0131],\n",
      "        ...,\n",
      "        [ 0.0175,  0.0028, -0.0234,  ...,  0.0232,  0.0033,  0.0017],\n",
      "        [ 0.0213,  0.0145,  0.0102,  ...,  0.0108, -0.0103, -0.0062],\n",
      "        [-0.0109,  0.0151,  0.0202,  ...,  0.0229, -0.0019, -0.0063]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0243,  0.0111,  0.0054,  ..., -0.0050, -0.0147, -0.0044],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([[-0.0091, -0.0032, -0.0019,  ..., -0.0063,  0.0055,  0.0035]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([0.0061], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "para = list(net.parameters())\n",
    "print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "print(optimizer)\n",
    "\n",
    "loss_func = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = Data.TensorDataset(torch.tensor(x_test.astype(np.float32)), torch.tensor(test_intensities.astype(np.float32)))\n",
    "data_iter_test = Data.DataLoader(dataset = dataset_test,batch_size = batch_size, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\envs\\tf2\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch10,train_loss0.010493269190192223\n",
      "epoch10,test_loss0.019386406987905502\n",
      "epoch20,train_loss0.002621123567223549\n",
      "epoch20,test_loss0.023570388555526733\n",
      "epoch30,train_loss0.001864963909611106\n",
      "epoch30,test_loss0.028130466118454933\n",
      "epoch40,train_loss0.0008500852272845805\n",
      "epoch40,test_loss0.027465978637337685\n",
      "epoch50,train_loss0.0008925755973905325\n",
      "epoch50,test_loss0.025597240775823593\n",
      "epoch60,train_loss0.0005398434004746377\n",
      "epoch60,test_loss0.0243566632270813\n",
      "epoch70,train_loss0.0006715902709402144\n",
      "epoch70,test_loss0.023758187890052795\n",
      "epoch80,train_loss0.00033751074806787074\n",
      "epoch80,test_loss0.024120919406414032\n",
      "epoch90,train_loss0.00032289623050019145\n",
      "epoch90,test_loss0.02601834386587143\n",
      "epoch100,train_loss0.00033925421303138137\n",
      "epoch100,test_loss0.024159787222743034\n",
      "epoch110,train_loss0.00038308906368911266\n",
      "epoch110,test_loss0.02484787628054619\n",
      "epoch120,train_loss0.00025087615358643234\n",
      "epoch120,test_loss0.024002835154533386\n",
      "epoch130,train_loss0.00035635026870295405\n",
      "epoch130,test_loss0.023477202281355858\n",
      "epoch140,train_loss0.00038620654959231615\n",
      "epoch140,test_loss0.02367052249610424\n",
      "epoch150,train_loss0.0011829915456473827\n",
      "epoch150,test_loss0.02399727888405323\n",
      "epoch160,train_loss0.0002208295773016289\n",
      "epoch160,test_loss0.024420587345957756\n",
      "epoch170,train_loss0.0009405647870153189\n",
      "epoch170,test_loss0.02403721772134304\n",
      "epoch180,train_loss0.0004148837469983846\n",
      "epoch180,test_loss0.0221998393535614\n",
      "epoch190,train_loss0.0006621192442253232\n",
      "epoch190,test_loss0.024200567975640297\n",
      "epoch200,train_loss0.00022424457711167634\n",
      "epoch200,test_loss0.02289101853966713\n",
      "epoch210,train_loss0.0003244329709559679\n",
      "epoch210,test_loss0.023778149858117104\n",
      "epoch220,train_loss0.00016831494576763362\n",
      "epoch220,test_loss0.024245280772447586\n",
      "epoch230,train_loss0.00018506238120608032\n",
      "epoch230,test_loss0.02352370321750641\n",
      "epoch240,train_loss0.00021639092301484197\n",
      "epoch240,test_loss0.023565853014588356\n",
      "epoch250,train_loss0.00034192067687399685\n",
      "epoch250,test_loss0.023492326959967613\n",
      "epoch260,train_loss0.0003990119439549744\n",
      "epoch260,test_loss0.02354857325553894\n",
      "epoch270,train_loss0.0003469997027423233\n",
      "epoch270,test_loss0.022620046511292458\n",
      "epoch280,train_loss0.00025525438832119107\n",
      "epoch280,test_loss0.02357407845556736\n",
      "epoch290,train_loss0.0001922924566315487\n",
      "epoch290,test_loss0.02256944589316845\n",
      "epoch300,train_loss0.00026835789321921766\n",
      "epoch300,test_loss0.023213542997837067\n",
      "epoch310,train_loss0.0006456093396991491\n",
      "epoch310,test_loss0.02303103543817997\n",
      "epoch320,train_loss0.0002971359936054796\n",
      "epoch320,test_loss0.02216389589011669\n",
      "epoch330,train_loss0.00020101961854379624\n",
      "epoch330,test_loss0.024103879928588867\n",
      "epoch340,train_loss0.00017303861386608332\n",
      "epoch340,test_loss0.022638758644461632\n",
      "epoch350,train_loss0.00022349963546730578\n",
      "epoch350,test_loss0.022569896653294563\n",
      "epoch360,train_loss0.00020849937573075294\n",
      "epoch360,test_loss0.02346130460500717\n",
      "epoch370,train_loss0.000190026214113459\n",
      "epoch370,test_loss0.023529743775725365\n",
      "epoch380,train_loss0.0004640354309231043\n",
      "epoch380,test_loss0.023537667468190193\n",
      "epoch390,train_loss0.00025976900360547006\n",
      "epoch390,test_loss0.02319776639342308\n",
      "epoch400,train_loss0.00022342272859532386\n",
      "epoch400,test_loss0.022963738068938255\n",
      "epoch410,train_loss0.0006269585574045777\n",
      "epoch410,test_loss0.022451655939221382\n",
      "epoch420,train_loss0.0003616043250076473\n",
      "epoch420,test_loss0.021861713379621506\n",
      "epoch430,train_loss0.00036407014704309404\n",
      "epoch430,test_loss0.02218436636030674\n",
      "epoch440,train_loss0.00021537808061111718\n",
      "epoch440,test_loss0.024053825065493584\n",
      "epoch450,train_loss0.00020753899298142642\n",
      "epoch450,test_loss0.02150733768939972\n",
      "epoch460,train_loss0.00025015001301653683\n",
      "epoch460,test_loss0.022246381267905235\n",
      "epoch470,train_loss0.00017016353376675397\n",
      "epoch470,test_loss0.023008031770586967\n",
      "epoch480,train_loss0.00030171091202646494\n",
      "epoch480,test_loss0.022750861942768097\n",
      "epoch490,train_loss0.0002534288796596229\n",
      "epoch490,test_loss0.02362264320254326\n",
      "epoch500,train_loss0.00016934039012994617\n",
      "epoch500,test_loss0.021794408559799194\n",
      "epoch510,train_loss0.0003381658170837909\n",
      "epoch510,test_loss0.0240899920463562\n",
      "epoch520,train_loss0.00016202122787944973\n",
      "epoch520,test_loss0.022505391389131546\n",
      "epoch530,train_loss0.0002090677007799968\n",
      "epoch530,test_loss0.021760931238532066\n",
      "epoch540,train_loss0.00015081590390764177\n",
      "epoch540,test_loss0.0229099802672863\n",
      "epoch550,train_loss0.00040054204873740673\n",
      "epoch550,test_loss0.021881692111492157\n",
      "epoch560,train_loss0.00036058874684385955\n",
      "epoch560,test_loss0.02318943850696087\n",
      "epoch570,train_loss0.000337935023708269\n",
      "epoch570,test_loss0.02304280735552311\n",
      "epoch580,train_loss0.00021356485376600176\n",
      "epoch580,test_loss0.0217985138297081\n",
      "epoch590,train_loss0.0009507134673185647\n",
      "epoch590,test_loss0.023889897391200066\n",
      "epoch600,train_loss0.00011381592776160687\n",
      "epoch600,test_loss0.022404728457331657\n",
      "epoch610,train_loss0.00023190223146229982\n",
      "epoch610,test_loss0.022719666361808777\n",
      "epoch620,train_loss0.0003982485504820943\n",
      "epoch620,test_loss0.023194124922156334\n",
      "epoch630,train_loss0.0002094960364047438\n",
      "epoch630,test_loss0.022719383239746094\n",
      "epoch640,train_loss0.0001969969307538122\n",
      "epoch640,test_loss0.022883698344230652\n",
      "epoch650,train_loss0.00022216937213670462\n",
      "epoch650,test_loss0.022107159718871117\n",
      "epoch660,train_loss0.00025638940860517323\n",
      "epoch660,test_loss0.022105759009718895\n",
      "epoch670,train_loss0.0004854060534853488\n",
      "epoch670,test_loss0.022955190390348434\n",
      "epoch680,train_loss0.00018913800886366516\n",
      "epoch680,test_loss0.022416863590478897\n",
      "epoch690,train_loss0.0001306425838265568\n",
      "epoch690,test_loss0.022364307194948196\n",
      "epoch700,train_loss0.00015235241153277457\n",
      "epoch700,test_loss0.022888610139489174\n",
      "epoch710,train_loss0.00019382857135497034\n",
      "epoch710,test_loss0.021823067218065262\n",
      "epoch720,train_loss0.00022556977637577802\n",
      "epoch720,test_loss0.022184275090694427\n",
      "epoch730,train_loss0.00026475609047338367\n",
      "epoch730,test_loss0.02280193194746971\n",
      "epoch740,train_loss0.00027688962290994823\n",
      "epoch740,test_loss0.02335244230926037\n",
      "epoch750,train_loss0.0002776163164526224\n",
      "epoch750,test_loss0.02113169990479946\n",
      "epoch760,train_loss0.00015727186109870672\n",
      "epoch760,test_loss0.022246908396482468\n",
      "epoch770,train_loss0.0001946381526067853\n",
      "epoch770,test_loss0.02324773371219635\n",
      "epoch780,train_loss0.00030897511169314384\n",
      "epoch780,test_loss0.0229401383548975\n",
      "epoch790,train_loss0.000311660289298743\n",
      "epoch790,test_loss0.021820293739438057\n",
      "epoch800,train_loss0.000138590854476206\n",
      "epoch800,test_loss0.02214656211435795\n",
      "epoch810,train_loss0.0001770880917320028\n",
      "epoch810,test_loss0.02210870198905468\n",
      "epoch820,train_loss0.00013662995479535311\n",
      "epoch820,test_loss0.02241174690425396\n",
      "epoch830,train_loss0.00010045482486020774\n",
      "epoch830,test_loss0.022338928654789925\n",
      "epoch840,train_loss0.00015614375297445804\n",
      "epoch840,test_loss0.02174980752170086\n",
      "epoch850,train_loss0.0002769276325125247\n",
      "epoch850,test_loss0.022553732618689537\n",
      "epoch860,train_loss0.00027341931127011776\n",
      "epoch860,test_loss0.022727832198143005\n",
      "epoch870,train_loss0.0001083935858332552\n",
      "epoch870,test_loss0.023236826062202454\n",
      "epoch880,train_loss0.0001664457522565499\n",
      "epoch880,test_loss0.02201920561492443\n",
      "epoch890,train_loss0.0001626619923627004\n",
      "epoch890,test_loss0.0219759289175272\n",
      "epoch900,train_loss0.0002051568153547123\n",
      "epoch900,test_loss0.02220258116722107\n",
      "epoch910,train_loss0.00017213969840668142\n",
      "epoch910,test_loss0.022105298936367035\n",
      "epoch920,train_loss0.00010355746053392068\n",
      "epoch920,test_loss0.02239268273115158\n",
      "epoch930,train_loss0.00023656676057726145\n",
      "epoch930,test_loss0.0221985075622797\n",
      "epoch940,train_loss0.0011221586028113961\n",
      "epoch940,test_loss0.022360464558005333\n",
      "epoch950,train_loss0.0001787360815797001\n",
      "epoch950,test_loss0.022178230807185173\n",
      "epoch960,train_loss0.00019760942086577415\n",
      "epoch960,test_loss0.02183137647807598\n",
      "epoch970,train_loss0.0001507425622548908\n",
      "epoch970,test_loss0.022278908640146255\n",
      "epoch980,train_loss0.0012584241339936852\n",
      "epoch980,test_loss0.02104969508945942\n",
      "epoch990,train_loss0.00017371391004417092\n",
      "epoch990,test_loss0.022459762170910835\n",
      "epoch1000,train_loss0.00018178838945459574\n",
      "epoch1000,test_loss0.022539813071489334\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "\n",
    "start_time_NN =time.time()\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "test_losses = []\n",
    "\n",
    "num_epochs = 1000\n",
    "train_interval = 10\n",
    "test_interval = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    net.train()\n",
    "    for X,y in data_iter:\n",
    "        prediction = net(X)\n",
    "        loss = loss_func(prediction,y.view(-1,1))\n",
    "    \n",
    "    # reset gradient, equal to net.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "\n",
    "    if((epoch+1)%train_interval==0):\n",
    "        print(\"epoch{},train_loss{}\".format(epoch+1,loss.data))\n",
    "        train_losses.append(loss.item())\n",
    "   \n",
    "\n",
    "      \n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for X_test, y_test in data_iter_test:\n",
    "            \n",
    "            prediction1 = net(X_test)\n",
    "            loss1 = loss_func(prediction1, y_test.view(-1,1))\n",
    "            \n",
    "    if ((epoch+1) % test_interval == 0):       \n",
    "        print(\"epoch{},test_loss{}\".format(epoch+1,loss1.data))\n",
    "        #test_loss += float(loss1.item())\n",
    "        test_losses.append(loss1.item())\n",
    "        \n",
    "trainingtime.loc[1] = [\"Simple Neural Network\", round((time.time()-start_time_NN), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = range(len(train_losses))\n",
    "train_y = train_losses\n",
    "\n",
    "train_iters = len(data_iter)\n",
    "#test_x = np.arange(1, len(test_losses)+1) * train_iters*test_interval \n",
    "test_x = range(len(test_losses))\n",
    "test_y = test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(train_x, train_y, label='Train')\n",
    "plt.plot(test_x, test_y, label='Test')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(673,)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_tensor = net(torch.tensor(x_test.astype(np.float32)))\n",
    "# len(y_pred)\n",
    "y_pred_array = y_pred_tensor.detach().numpy()\n",
    "y_pred_a = np.concatenate((y_pred_array), axis=None)\n",
    "y_pred_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6737093 , 0.67496191, 0.52557922, 0.52246362])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score2 = evaluate_lists(y_pred_a, test_intensities)\n",
    "score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'files/final_models/simpleNN_sadness.pkl.tar'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_neural_network_path = \"files/final_models/\" + \"simpleNN_\"+ emotion + \".pkl.tar\"\n",
    "simple_neural_network_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net, simple_neural_network_path) \n",
    "torch.save({'state_dict': net.state_dict()}, simple_neural_network_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare the Performance and Training Time Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Training Time(Seconds)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>953.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Simple Neural Network</td>\n",
       "      <td>2363.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Training Time(Seconds)\n",
       "0                XGBoost                  953.81\n",
       "1  Simple Neural Network                 2363.85"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingtime.to_csv(\"training_time_\"+emotion+\".csv\",mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pears-corr</th>\n",
       "      <th>spear-corr</th>\n",
       "      <th>pears-corr-range-05-1</th>\n",
       "      <th>spear-corr-range-05-1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xgboost</th>\n",
       "      <td>0.652067</td>\n",
       "      <td>0.646064</td>\n",
       "      <td>0.505040</td>\n",
       "      <td>0.492141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simpleNN</th>\n",
       "      <td>0.673709</td>\n",
       "      <td>0.674962</td>\n",
       "      <td>0.525579</td>\n",
       "      <td>0.522464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pears-corr  spear-corr  pears-corr-range-05-1  spear-corr-range-05-1\n",
       "xgboost     0.652067    0.646064               0.505040               0.492141\n",
       "simpleNN    0.673709    0.674962               0.525579               0.522464"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_score = pd.DataFrame(data = [score1,score2], columns = ['pears-corr','spear-corr','pears-corr-range-05-1','spear-corr-range-05-1'],\\\n",
    "             index = ['xgboost','simpleNN'])\n",
    "all_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_score.to_csv('score_'+emotion+'.csv',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
